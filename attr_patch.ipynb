{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dictionary_learning import AutoEncoder, ActivationBuffer, GatedAutoEncoder, JumpReluAutoEncoder\n",
    "from nnsight import LanguageModel\n",
    "from dictionary_learning.interp import examine_dimension\n",
    "from dictionary_learning.utils import zst_to_generator\n",
    "from huggingface_hub import hf_hub_download, list_repo_files\n",
    "\n",
    "import torch as t\n",
    "import numpy as np\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nnsight.models.UnifiedTransformer import UnifiedTransformer\n",
    "# from sae_lens import SparseAutoencoder\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "DEVICE = \"cuda:0\"\n",
    "model = LanguageModel(\"google/gemma-2-2b\", attn_implementation=\"eager\",\n",
    "                      torch_dtype=t.bfloat16, device_map=DEVICE)\n",
    "\n",
    "# loading dictionaries\n",
    "layer = 19\n",
    "\n",
    "# dictionary hyperparameters\n",
    "# dict_id = 10\n",
    "# expansion_factor = 64\n",
    "# dictionary_size = expansion_factor * activation_dim\n",
    "\n",
    "submodules = []\n",
    "dictionaries = {}\n",
    "use_inputs = {}\n",
    "\n",
    "# For Gemma 2\n",
    "import numpy as np\n",
    "\n",
    "def _params_path_to_sae(params_path, torch_dtype=t.bfloat16):\n",
    "    params = np.load(path_to_sae_params)\n",
    "    pt_params = {k: t.from_numpy(v).cuda() for k, v in params.items()}  \n",
    "    sae = JumpReluAutoEncoder(params['W_enc'].shape[0], params['W_enc'].shape[1]).to(\"cuda\").to(torch_dtype)\n",
    "    sae.load_state_dict(pt_params)\n",
    "    sae = sae.to(DEVICE)\n",
    "    return sae\n",
    "\n",
    "def _get_l0_nearest_100_filename(repo_id, layer):\n",
    "    repo_files = list_repo_files(repo_id=repo_id)\n",
    "    attn_files = [f for f in repo_files if f.startswith(f\"layer_{layer}/width_16k\")]\n",
    "    attn_files = [f for f in attn_files if \"canonical\" not in f]\n",
    "    submod_l0s = [int(f.split(\"average_\")[1].split(\"/\")[0].split(\"_\")[-1]) for f in attn_files]\n",
    "    distances_to_100 = [abs(x - 100) for x in submod_l0s]\n",
    "    idx_mindistance = np.argmin(distances_to_100)\n",
    "    submod_file_nearest_100 = attn_files[idx_mindistance]\n",
    "    return submod_file_nearest_100\n",
    "\n",
    "repo_id = \"google/gemma-scope-2b-pt-{submod_name}\"\n",
    "# filename = \"layer_{layer_idx}/width_16k/canonical/params.npz\"\n",
    "\n",
    "# submodules.append(model.model.embed_tokens)\n",
    "# path_to_sae_params = hf_hub_download(\n",
    "#     repo_id=repo_id.format(submod_name=\"res\"),\n",
    "#     filename=\"embedding/width_4k/average_l0_44/params.npz\",\n",
    "#     force_download=False\n",
    "# )\n",
    "# sae = _params_path_to_sae(path_to_sae_params)\n",
    "# dictionaries[model.model.embed_tokens] = sae\n",
    "# use_inputs[model.model.embed_tokens] = False\n",
    "\n",
    "for layer in range(layer + 1):\n",
    "    submodules.append(model.model.layers[layer].self_attn.o_proj)\n",
    "    # get attn filename (no canonical file provided)\n",
    "    # median_l0_filename = _get_median_l0_filename(repo_id.format(submod_name=\"att\"), layer)\n",
    "    l0_filename = _get_l0_nearest_100_filename(repo_id.format(submod_name=\"att\"), layer)\n",
    "    path_to_sae_params = hf_hub_download(\n",
    "        repo_id=repo_id.format(submod_name=\"att\"),\n",
    "        filename=l0_filename,\n",
    "        force_download=False\n",
    "    )\n",
    "    sae = _params_path_to_sae(path_to_sae_params)\n",
    "    dictionaries[model.model.layers[layer].self_attn.o_proj] = sae\n",
    "    use_inputs[model.model.layers[layer].self_attn.o_proj] = True\n",
    "\n",
    "    # median_l0_filename = _get_median_l0_filename(repo_id.format(submod_name=\"mlp\"), layer)\n",
    "    l0_filename = _get_l0_nearest_100_filename(repo_id.format(submod_name=\"mlp\"), layer)\n",
    "    submodules.append(model.model.layers[layer].post_feedforward_layernorm)\n",
    "    path_to_sae_params = hf_hub_download(\n",
    "        repo_id=repo_id.format(submod_name=\"mlp\"),\n",
    "        filename=l0_filename,\n",
    "        force_download=False\n",
    "    )\n",
    "    sae = _params_path_to_sae(path_to_sae_params)\n",
    "    dictionaries[model.model.layers[layer].post_feedforward_layernorm] = sae\n",
    "    use_inputs[model.model.layers[layer].post_feedforward_layernorm] = False\n",
    "\n",
    "    # median_l0_filename = _get_median_l0_filename(repo_id.format(submod_name=\"res\"), layer)\n",
    "    l0_filename = _get_l0_nearest_100_filename(repo_id.format(submod_name=\"res\"), layer)\n",
    "    submodules.append(model.model.layers[layer])\n",
    "    path_to_sae_params = hf_hub_download(\n",
    "        repo_id=repo_id.format(submod_name=\"res\"),\n",
    "        filename=l0_filename,\n",
    "        force_download=False\n",
    "    )\n",
    "    sae = _params_path_to_sae(path_to_sae_params)\n",
    "    dictionaries[model.model.layers[layer]] = sae\n",
    "    use_inputs[model.model.layers[layer]] = False\n",
    "\n",
    "# if component == 'resid':\n",
    "#     submodule = resids[layer]\n",
    "\n",
    "# activation_dim=4096\n",
    "activation_dim=2304\n",
    "\n",
    "\n",
    "# the GPT-2 SAEs expect a BOS token at start of sequence. nnsight doesn't do this,\n",
    "# so we need to tell the tokenizer to always do this\n",
    "# model.tokenizer._tokenizer.post_processor = TemplateProcessing(\n",
    "#     single=model.tokenizer.bos_token + \" $A\",\n",
    "#     special_tokens=[(model.tokenizer.bos_token, model.tokenizer.bos_token_id)]\n",
    "# )\n",
    "\n",
    "# dictionaries = {}\n",
    "# for i in (16,):\n",
    "#     ae = GatedAutoEncoder(4096, 32768).to(\"cuda\")\n",
    "#     ae.load_state_dict(t.load(f'llama_saes/layer{i}/ae_81920.pt'))\n",
    "#     ae = ae.half()\n",
    "#     dictionaries[resids[i]] = ae\n",
    "#     break\n",
    "#     # obj = t.load(f'llama_saes/layer{i}/ae_81920.pt')\n",
    "# #     # print(obj)\n",
    "# for i in (13,):\n",
    "#     path_to_params = hf_hub_download(\n",
    "#         repo_id=\"google/gemma-scope-2b-pt-res\",\n",
    "#         filename=f\"layer_{i}/width_16k/canonical/params.npz\",\n",
    "#         force_download=False,\n",
    "#     )\n",
    "#     params = np.load(path_to_params)\n",
    "#     pt_params = {k: t.from_numpy(v).cuda() for k, v in params.items()}\n",
    "#     ae = JumpReLUSAE(params[\"W_enc\"].shape[0], params[\"W_enc\"].shape[1]).to(\"cuda\")\n",
    "#     ae.load_state_dict(pt_params)\n",
    "#     dictionaries[resids[i]] = ae\n",
    "\n",
    "# resids = list(dictionaries.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "def load_examples_prefix_len(dataset, num_examples, model, seed=12, pad_to_length=None, length=None,\n",
    "                  ignore_patch=False):\n",
    "    examples = []\n",
    "    dataset_items = open(dataset).readlines()\n",
    "    random.seed(seed)\n",
    "    random.shuffle(dataset_items)\n",
    "    for line in dataset_items:\n",
    "        data = json.loads(line)\n",
    "        clean_prefix = model.tokenizer(data[\"clean_prefix\"], return_tensors=\"pt\",\n",
    "                                        padding=False).input_ids\n",
    "        patch_prefix = model.tokenizer(data[\"patch_prefix\"], return_tensors=\"pt\",\n",
    "                                        padding=False).input_ids\n",
    "        clean_answer = model.tokenizer(data[\"clean_answer\"], return_tensors=\"pt\",\n",
    "                                        padding=False).input_ids\n",
    "        patch_answer = model.tokenizer(data[\"patch_answer\"], return_tensors=\"pt\",\n",
    "                                        padding=False).input_ids\n",
    "\n",
    "        clean_prefix_firstsent = data[\"clean_prefix\"].split(\".\")[0]\n",
    "        clean_prefix_firstsent_tok = model.tokenizer(clean_prefix_firstsent, return_tensors=\"pt\",\n",
    "                                                     padding=False).input_ids\n",
    "        \n",
    "        # remove BOS tokens from answers\n",
    "        clean_answer = clean_answer[clean_answer != model.tokenizer.bos_token_id].unsqueeze(0)\n",
    "        patch_answer = patch_answer[patch_answer != model.tokenizer.bos_token_id].unsqueeze(0)\n",
    "        # only keep examples where answers are single tokens\n",
    "        if not ignore_patch:\n",
    "            if clean_prefix.shape[1] != patch_prefix.shape[1]:\n",
    "                continue\n",
    "        # only keep examples where clean and patch answers are the same length\n",
    "        if clean_answer.shape[1] != 1 or patch_answer.shape[1] != 1:\n",
    "            continue\n",
    "        # if we specify a `length`, filter examples if they don't match\n",
    "        if length and clean_prefix_firstsent_tok.shape[1] != length:\n",
    "            continue\n",
    "        # if we specify `pad_to_length`, left-pad all inputs to a max length\n",
    "        prefix_length_wo_pad = clean_prefix.shape[1]\n",
    "        if pad_to_length:\n",
    "            model.tokenizer.padding_side = 'right'\n",
    "            pad_length = pad_to_length - prefix_length_wo_pad\n",
    "            if pad_length < 0:  # example too long\n",
    "                continue\n",
    "            # left padding: reverse, right-pad, reverse\n",
    "            clean_prefix = t.flip(F.pad(t.flip(clean_prefix, (1,)), (0, pad_length), value=model.tokenizer.pad_token_id), (1,))\n",
    "            patch_prefix = t.flip(F.pad(t.flip(patch_prefix, (1,)), (0, pad_length), value=model.tokenizer.pad_token_id), (1,))\n",
    "        \n",
    "        example_dict = {\"clean_prefix\": clean_prefix,\n",
    "                        \"patch_prefix\": patch_prefix,\n",
    "                        \"clean_answer\": clean_answer.item(),\n",
    "                        \"patch_answer\": patch_answer.item(),\n",
    "                        # \"annotations\": get_annotation(dataset, model, data),\n",
    "                        \"prefix_length_wo_pad\": prefix_length_wo_pad,}\n",
    "        examples.append(example_dict)\n",
    "        if len(examples) >= num_examples:\n",
    "            break\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "from loading_utils import load_examples\n",
    "\n",
    "data_path = \"data/NPS_gp_post_readingcomp_samelen.json\"\n",
    "# data_path = \"data/MVRR_ambiguous_samelen.json\"\n",
    "ignore_patch = True\n",
    "num_examples = 100\n",
    "length = 9\n",
    "pad_length = 32\n",
    "\n",
    "examples = load_examples_prefix_len(data_path, num_examples, model, length=length, #pad_to_length=pad_length\n",
    "                                     ignore_patch=False)\n",
    "# examples = load_examples(data_path, num_examples, model, length=length, # pad_to_length=pad_length,\n",
    "#                                      ignore_patch=True)\n",
    "print(len(examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from activation_utils import SparseAct\n",
    "\n",
    "tracer_kwargs = {'validate' : False, 'scan' : False}\n",
    "\n",
    "def _pe_ig(\n",
    "        clean,\n",
    "        patch,\n",
    "        model,\n",
    "        submodules,\n",
    "        dictionaries,\n",
    "        metric_fn,\n",
    "        use_inputs=None,\n",
    "        steps=10,\n",
    "        metric_kwargs=dict(),\n",
    "):\n",
    "    if use_inputs is None:\n",
    "        for submodule in submodules:\n",
    "            use_inputs[submodule] = False\n",
    "\n",
    "    # first run through a test input to figure out which hidden states are tuples\n",
    "    is_tuple = {}\n",
    "    with model.trace(\"_\"):\n",
    "        for submodule in submodules:\n",
    "            is_tuple[submodule] = type(submodule.output.shape) == tuple\n",
    "\n",
    "    hidden_states_clean = {}\n",
    "    with model.trace(clean, **tracer_kwargs), t.no_grad():\n",
    "        for submodule in submodules:\n",
    "            dictionary = dictionaries[submodule]\n",
    "            x = submodule.output if not use_inputs[submodule] else submodule.input\n",
    "            if use_inputs[submodule]:\n",
    "                x = x[0][0]\n",
    "            elif is_tuple[submodule]:\n",
    "                x = x[0]\n",
    "            f = dictionary.encode(x)\n",
    "            x_hat = dictionary.decode(f)\n",
    "            residual = x - x_hat\n",
    "            hidden_states_clean[submodule] = SparseAct(act=f.save(), res=residual.save())\n",
    "        metric_clean = metric_fn(model, **metric_kwargs).save()\n",
    "    hidden_states_clean = {k : v.value for k, v in hidden_states_clean.items()}\n",
    "\n",
    "    if patch is None:\n",
    "        hidden_states_patch = {\n",
    "            k : SparseAct(act=t.zeros_like(v.act), res=t.zeros_like(v.res)) for k, v in hidden_states_clean.items()\n",
    "        }\n",
    "        total_effect = None\n",
    "    else:\n",
    "        hidden_states_patch = {}\n",
    "        with model.trace(patch, **tracer_kwargs), t.no_grad():\n",
    "            for submodule in submodules:\n",
    "                dictionary = dictionaries[submodule]\n",
    "                x = submodule.output if not use_inputs[submodule] else submodule.input\n",
    "                if use_inputs[submodule]:\n",
    "                    x = x[0][0]\n",
    "                elif is_tuple[submodule]:\n",
    "                    x = x[0]\n",
    "                f = dictionary.encode(x)\n",
    "                x_hat = dictionary.decode(f)\n",
    "                residual = x - x_hat\n",
    "                hidden_states_patch[submodule] = SparseAct(act=f.save(), res=residual.save())\n",
    "            metric_patch = metric_fn(model, **metric_kwargs).save()\n",
    "        total_effect = (metric_patch.value - metric_clean.value).detach()\n",
    "        hidden_states_patch = {k : v.value for k, v in hidden_states_patch.items()}\n",
    "\n",
    "    effects = {}\n",
    "    deltas = {}\n",
    "    grads = {}\n",
    "    for submodule in submodules:\n",
    "        dictionary = dictionaries[submodule]\n",
    "        clean_state = hidden_states_clean[submodule]\n",
    "        patch_state = hidden_states_patch[submodule]\n",
    "        with model.trace(**tracer_kwargs) as tracer:\n",
    "            metrics = []\n",
    "            fs = []\n",
    "            for step in range(steps):\n",
    "                alpha = step / steps\n",
    "                f = (1 - alpha) * clean_state + alpha * patch_state\n",
    "                f.act.retain_grad()\n",
    "                f.res.retain_grad()\n",
    "                fs.append(f)\n",
    "                with tracer.invoke(clean, scan=tracer_kwargs['scan']):\n",
    "                    if use_inputs[submodule]:\n",
    "                        submodule.input[0][0][:] = dictionary.decode(f.act) + f.res\n",
    "                    elif is_tuple[submodule]:\n",
    "                        submodule.output[0][:] = dictionary.decode(f.act) + f.res\n",
    "                    else:\n",
    "                        submodule.output = dictionary.decode(f.act) + f.res\n",
    "                    metrics.append(metric_fn(model, **metric_kwargs))\n",
    "            metric = sum([m for m in metrics])\n",
    "            metric.sum().backward(retain_graph=True) # TODO : why is this necessary? Probably shouldn't be, contact jaden\n",
    "\n",
    "        mean_grad = sum([f.act.grad for f in fs]) / steps\n",
    "        mean_residual_grad = sum([f.res.grad for f in fs]) / steps\n",
    "        grad = SparseAct(act=mean_grad, res=mean_residual_grad)\n",
    "        delta = (patch_state - clean_state).detach() if patch_state is not None else -clean_state.detach()\n",
    "        effect = grad @ delta\n",
    "\n",
    "        effects[submodule] = effect\n",
    "        deltas[submodule] = delta\n",
    "        grads[submodule] = grad\n",
    "\n",
    "    return (effects, deltas, grads, total_effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e67060db81a41bbbab4cf6c36b44292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GemmaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [04:27<00:00, 11.14s/it]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 1\n",
    "num_examples = 100\n",
    "device = \"cuda\"\n",
    "num_examples = min([num_examples, len(examples)])\n",
    "n_batches = math.ceil(len(examples) / batch_size)\n",
    "batches = [\n",
    "    examples[batch*batch_size:(batch+1)*batch_size] for batch in range(n_batches)\n",
    "]\n",
    "\n",
    "running_total = 0\n",
    "nodes = None\n",
    "\n",
    "for batch in tqdm(batches):\n",
    "    clean_answer_idxs = t.tensor([e['clean_answer'] for e in batch], dtype=t.long, device=device)\n",
    "    clean_inputs = t.cat([e['clean_prefix'] for e in batch], dim=0).to(device)\n",
    "\n",
    "    patch_answer_idxs = t.tensor([e['patch_answer'] for e in batch], dtype=t.long, device=device)\n",
    "    patch_inputs = t.cat([e['patch_prefix'] for e in batch], dim=0).to(device)\n",
    "    def metric_fn(model):\n",
    "        return (\n",
    "            t.gather(model.lm_head.output[:,-1,:], dim=-1, index=patch_answer_idxs.view(-1, 1)).squeeze(-1) - \\\n",
    "            t.gather(model.lm_head.output[:,-1,:], dim=-1, index=clean_answer_idxs.view(-1, 1)).squeeze(-1)\n",
    "        )\n",
    "\n",
    "    # for example in examples[:1]:\n",
    "    effects, _, _, _ = _pe_ig(\n",
    "        clean_inputs,\n",
    "        # patch_inputs,\n",
    "        None,\n",
    "        model,\n",
    "        submodules,\n",
    "        dictionaries,\n",
    "        metric_fn,\n",
    "        use_inputs=use_inputs\n",
    "    )\n",
    "    with t.no_grad():\n",
    "        if nodes is None:\n",
    "            # nodes = {k : len(clean_inputs) * v.sum(dim=1).mean(dim=0) for k, v in effects.items()}\n",
    "            nodes = {k : len(clean_inputs) * v.mean(dim=0) for k, v in effects.items()}\n",
    "        else:\n",
    "            for k, v in effects.items():\n",
    "                # nodes[k] += len(clean_inputs) * v.sum(dim=1).mean(dim=0)\n",
    "                nodes[k] += len(clean_inputs) * v.mean(dim=0)\n",
    "        running_total += len(clean_inputs)\n",
    "    del effects, _\n",
    "    gc.collect()\n",
    "\n",
    "nodes = {k : v / running_total for k, v in nodes.items()}\n",
    "\n",
    "for i, node in enumerate(nodes):\n",
    "    t.save(nodes[node], open(f\"node_effects/effects_NPS_readingcomp/node_{i}.pt\", \"wb\"))\n",
    "\n",
    "# print(\"negative effects\")\n",
    "# for idx, submodule in enumerate(resids):\n",
    "#     print(f\"resid_{idx}\")\n",
    "#     v, i = t.topk(sum_effects[submodule].flatten(), 10, largest=False)\n",
    "#     print(np.array(np.unravel_index(i.cpu().numpy(), sum_effects[submodule].shape)).T)\n",
    "#     print(v)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = {}\n",
    "for i, submodule in enumerate(submodules):\n",
    "    nodes[submodule] = t.load(f\"node_effects/effects_NPS_readingcomp/node_{i}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0737, device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[submodules[-1]].act.shape\n",
    "idxs = (nodes[submodules[-1]].act[:length, :] > .05).nonzero()\n",
    "idx = idxs[0]\n",
    "nodes[submodules[-1]].act[idx[0], idx[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive and negative effects\n",
      "Component 0:\n",
      "Component 1:\n",
      "Component 2:\n",
      "Component 3:\n",
      "Component 4:\n",
      "Component 5:\n",
      "Component 6:\n",
      "Component 7:\n",
      "Component 8:\n",
      "tensor([    0, 15089], device='cuda:0') 38.5\n",
      "Component 9:\n",
      "Component 10:\n",
      "Component 11:\n",
      "tensor([ 0, 58], device='cuda:0') 7.21875\n",
      "tensor([   0, 9134], device='cuda:0') -8.5\n",
      "tensor([    0, 14238], device='cuda:0') 8.8125\n",
      "Component 12:\n",
      "Component 13:\n",
      "Component 14:\n",
      "Component 15:\n",
      "Component 16:\n",
      "Component 17:\n",
      "tensor([   0, 1059], device='cuda:0') -6.5\n",
      "tensor([   0, 8392], device='cuda:0') 12.4375\n",
      "tensor([   1, 3235], device='cuda:0') -5.46875\n",
      "Component 18:\n",
      "Component 19:\n",
      "Component 20:\n",
      "tensor([   0, 4478], device='cuda:0') -6.09375\n",
      "tensor([    0, 14077], device='cuda:0') -6.15625\n",
      "Component 21:\n",
      "Component 22:\n",
      "Component 23:\n",
      "tensor([  0, 110], device='cuda:0') -5.25\n",
      "tensor([   0, 3928], device='cuda:0') -7.125\n",
      "tensor([   0, 4105], device='cuda:0') 6.84375\n",
      "tensor([   0, 4137], device='cuda:0') 5.34375\n",
      "tensor([   0, 4411], device='cuda:0') 5.21875\n",
      "tensor([   0, 5060], device='cuda:0') 5.875\n",
      "tensor([   0, 5899], device='cuda:0') 6.0\n",
      "tensor([   0, 9270], device='cuda:0') 8.5625\n",
      "tensor([    0, 10015], device='cuda:0') 6.75\n",
      "tensor([    0, 10931], device='cuda:0') 6.40625\n",
      "tensor([    0, 12109], device='cuda:0') 7.375\n",
      "tensor([    0, 12287], device='cuda:0') 13.8125\n",
      "tensor([    0, 13027], device='cuda:0') -15.3125\n",
      "tensor([    0, 13236], device='cuda:0') -11.0\n",
      "tensor([    0, 14537], device='cuda:0') -8.3125\n",
      "Component 24:\n",
      "Component 25:\n",
      "Component 26:\n",
      "tensor([   0, 6069], device='cuda:0') -5.28125\n",
      "Component 27:\n",
      "Component 28:\n",
      "Component 29:\n",
      "Component 30:\n",
      "Component 31:\n",
      "Component 32:\n",
      "tensor([   0, 2843], device='cuda:0') 10.0625\n",
      "tensor([   0, 4392], device='cuda:0') 12.5625\n",
      "Component 33:\n",
      "Component 34:\n",
      "Component 35:\n",
      "tensor([    0, 12945], device='cuda:0') -9.0\n",
      "Component 36:\n",
      "Component 37:\n",
      "Component 38:\n",
      "Component 39:\n",
      "Component 40:\n",
      "Component 41:\n",
      "Component 42:\n",
      "Component 43:\n",
      "Component 44:\n",
      "Component 45:\n",
      "Component 46:\n",
      "Component 47:\n",
      "Component 48:\n",
      "Component 49:\n",
      "Component 50:\n",
      "Component 51:\n",
      "Component 52:\n",
      "Component 53:\n",
      "Component 54:\n",
      "Component 55:\n",
      "Component 56:\n",
      "Component 57:\n",
      "Component 58:\n",
      "Component 59:\n",
      "total features: 28\n"
     ]
    }
   ],
   "source": [
    "filter_bos = True\n",
    "if filter_bos:\n",
    "    start_idx = 1\n",
    "else:\n",
    "    start_idx = 0\n",
    "\n",
    "print(\"positive and negative effects\")\n",
    "n_features = 0\n",
    "for component_idx, effect in enumerate(nodes.values()):\n",
    "    print(f\"Component {component_idx}:\")\n",
    "    for idx in (effect.act[start_idx:length, :].abs() > 5.0).nonzero():\n",
    "        print(idx, effect[idx[0], idx[1]].item())\n",
    "        n_features += 1\n",
    "print(f\"total features: {n_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"400\"\n",
       "            src=\"https://neuronpedia.org/gemma-2-2b/0-gemmascope-att-16k/1608?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f6ec004f760>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# interpret features with Neuronpedia API\n",
    "\n",
    "def _submodule_idx_to_name(submodule_idx):\n",
    "    if submodule_idx % 3 == 0:\n",
    "        layer = submodule_idx // 3\n",
    "        return f\"attn_{layer}\"\n",
    "    elif submodule_idx % 3 == 1:\n",
    "        layer = submodule_idx // 3\n",
    "        return f\"mlp_{layer}\"\n",
    "    elif submodule_idx % 3 == 2:\n",
    "        layer = submodule_idx // 3\n",
    "        return f\"resid_{layer}\"\n",
    "    return f\"{submodule_name}_{layer}\"\n",
    "\n",
    "submodule_idx = 0\n",
    "feature_idx = 1608\n",
    "submodule_name = _submodule_idx_to_name(submodule_idx)\n",
    "\n",
    "def _format_activations(activations, top_n=10):\n",
    "    formatted_outputs = []\n",
    "\n",
    "    # Process the top N activations\n",
    "    for activation in activations[:top_n]:\n",
    "        tokens = activation[\"tokens\"]\n",
    "        values = activation[\"values\"]\n",
    "\n",
    "        # Find the index of the maximum activation value\n",
    "        max_value_index = values.index(max(values))\n",
    "\n",
    "        # Determine the range of tokens to include (10 before and 10 after the max token)\n",
    "        start_index = max(0, max_value_index - 10)\n",
    "        end_index = min(len(tokens), max_value_index + 11)  # +11 because the range is inclusive of the max token\n",
    "\n",
    "        # Slice the tokens and values accordingly\n",
    "        tokens_slice = tokens[start_index:end_index]\n",
    "\n",
    "        # Create the formatted string\n",
    "        formatted_string = \"\"\n",
    "        for i, token in enumerate(tokens_slice):\n",
    "            # Replace special characters ▁ with spaces and underscores with spaces\n",
    "            token = token.replace(\"▁\", \" \").replace(\"_\", \" \")\n",
    "\n",
    "            if start_index + i == max_value_index:\n",
    "                formatted_string += f\"<<{token}>>\"\n",
    "            else:\n",
    "                formatted_string += token\n",
    "\n",
    "        formatted_outputs.append(formatted_string)\n",
    "\n",
    "    return formatted_outputs\n",
    "\n",
    "from IPython.display import IFrame\n",
    "html_template = \"https://neuronpedia.org/{}/{}/{}?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
    "\n",
    "def get_dashboard_html(sae_release=\"gemma-2-2b\", sae_id=None, feature_idx=feature_idx):\n",
    "    return html_template.format(sae_release, sae_id, feature_idx)\n",
    "\n",
    "# Extract the type and number from submodule_name\n",
    "submodule_type, submodule_number = submodule_name.split('_')\n",
    "\n",
    "# Construct the sae_id based on the type\n",
    "if submodule_type == \"resid\":\n",
    "    sae_id = f\"{submodule_number}-gemmascope-res-16k\"\n",
    "elif submodule_type == \"attn\":\n",
    "    sae_id = f\"{submodule_number}-gemmascope-att-16k\"\n",
    "elif submodule_type == \"mlp\":\n",
    "    sae_id = f\"{submodule_number}-gemmascope-mlp-16k\"\n",
    "else:\n",
    "    raise ValueError(\"Unknown submodule type\")\n",
    "\n",
    "html = get_dashboard_html(sae_release=\"gemma-2-2b\", sae_id=sae_id, feature_idx=feature_idx)\n",
    "IFrame(html, width=800, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load circuits, analyze\n",
    "circuit_path = \"circuits/NPS_ambiguous_samelen_dict10_node0.5_edge0.05_n24_aggnone_gpt2.pt\"\n",
    "circuit = t.load(open(circuit_path, 'rb'))\n",
    "\n",
    "for submod in circuit[\"nodes\"]:\n",
    "    effects = circuit[\"nodes\"][submod]\n",
    "    top_effects = t.topk(effects.act, 10)\n",
    "    print(submod, top_effects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 24576])\n"
     ]
    }
   ],
   "source": [
    "with model.trace(\"testing 1\"):\n",
    "    out_save = resids[4].output\n",
    "    f = dictionaries[resids[4]].encode(out_save).save()\n",
    "print(f.value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
