{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dictionary_learning import AutoEncoder, ActivationBuffer, GatedAutoEncoder\n",
    "from nnsight import LanguageModel\n",
    "from dictionary_learning.interp import examine_dimension\n",
    "from dictionary_learning.utils import zst_to_generator\n",
    "import torch as t\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from nnsight.models.UnifiedTransformer import UnifiedTransformer\n",
    "from sae_lens import SparseAutoencoder\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "model = LanguageModel(\"meta-llama/Meta-Llama-3-8B\", torch_dtype=t.float16,\n",
    "                      device_map=\"cuda\")\n",
    "resids = [layer for layer in model.model.layers]\n",
    "component = 'resid'\n",
    "\n",
    "# if component == 'resid':\n",
    "#     submodule = resids[layer]\n",
    "\n",
    "activation_dim=4096\n",
    "\n",
    "\n",
    "\n",
    "# the GPT-2 SAEs expect a BOS token at start of sequence. nnsight doesn't do this,\n",
    "# so we need to tell the tokenizer to always do this\n",
    "# model.tokenizer._tokenizer.post_processor = TemplateProcessing(\n",
    "#     single=model.tokenizer.bos_token + \" $A\",\n",
    "#     special_tokens=[(model.tokenizer.bos_token, model.tokenizer.bos_token_id)]\n",
    "# )\n",
    "\n",
    "dictionaries = {}\n",
    "for i in (16,):\n",
    "    ae = GatedAutoEncoder(4096, 32768).to(\"cuda\")\n",
    "    ae.load_state_dict(t.load(f'llama_saes/layer{i}/ae_81920.pt'))\n",
    "    ae = ae.half()\n",
    "    dictionaries[resids[i]] = ae\n",
    "    break\n",
    "    # obj = t.load(f'llama_saes/layer{i}/ae_81920.pt')\n",
    "    # print(obj)\n",
    "resids = list(dictionaries.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "def load_examples_prefix_len(dataset, num_examples, model, seed=12, pad_to_length=None, length=None,\n",
    "                  ignore_patch=False):\n",
    "    examples = []\n",
    "    dataset_items = open(dataset).readlines()\n",
    "    random.seed(seed)\n",
    "    random.shuffle(dataset_items)\n",
    "    for line in dataset_items:\n",
    "        data = json.loads(line)\n",
    "        clean_prefix = model.tokenizer(data[\"clean_prefix\"], return_tensors=\"pt\",\n",
    "                                        padding=False).input_ids\n",
    "        patch_prefix = model.tokenizer(data[\"patch_prefix\"], return_tensors=\"pt\",\n",
    "                                        padding=False).input_ids\n",
    "        clean_answer = model.tokenizer(data[\"clean_answer\"], return_tensors=\"pt\",\n",
    "                                        padding=False).input_ids\n",
    "        patch_answer = model.tokenizer(data[\"patch_answer\"], return_tensors=\"pt\",\n",
    "                                        padding=False).input_ids\n",
    "\n",
    "        clean_prefix_firstsent = data[\"clean_prefix\"].split(\".\")[0]\n",
    "        clean_prefix_firstsent_tok = model.tokenizer(clean_prefix_firstsent, return_tensors=\"pt\",\n",
    "                                                     padding=False).input_ids\n",
    "        \n",
    "        # remove BOS tokens from answers\n",
    "        clean_answer = clean_answer[clean_answer != model.tokenizer.bos_token_id].unsqueeze(0)\n",
    "        patch_answer = patch_answer[patch_answer != model.tokenizer.bos_token_id].unsqueeze(0)\n",
    "        # only keep examples where answers are single tokens\n",
    "        if not ignore_patch:\n",
    "            if clean_prefix.shape[1] != patch_prefix.shape[1]:\n",
    "                continue\n",
    "        # only keep examples where clean and patch answers are the same length\n",
    "        if clean_answer.shape[1] != 1 or patch_answer.shape[1] != 1:\n",
    "            continue\n",
    "        # if we specify a `length`, filter examples if they don't match\n",
    "        if length and clean_prefix_firstsent_tok.shape[1] != length:\n",
    "            continue\n",
    "        # if we specify `pad_to_length`, left-pad all inputs to a max length\n",
    "        prefix_length_wo_pad = clean_prefix.shape[1]\n",
    "        if pad_to_length:\n",
    "            model.tokenizer.padding_side = 'right'\n",
    "            pad_length = pad_to_length - prefix_length_wo_pad\n",
    "            if pad_length < 0:  # example too long\n",
    "                continue\n",
    "            # left padding: reverse, right-pad, reverse\n",
    "            clean_prefix = t.flip(F.pad(t.flip(clean_prefix, (1,)), (0, pad_length), value=model.tokenizer.pad_token_id), (1,))\n",
    "            patch_prefix = t.flip(F.pad(t.flip(patch_prefix, (1,)), (0, pad_length), value=model.tokenizer.pad_token_id), (1,))\n",
    "        \n",
    "        example_dict = {\"clean_prefix\": clean_prefix,\n",
    "                        \"patch_prefix\": patch_prefix,\n",
    "                        \"clean_answer\": clean_answer.item(),\n",
    "                        \"patch_answer\": patch_answer.item(),\n",
    "                        # \"annotations\": get_annotation(dataset, model, data),\n",
    "                        \"prefix_length_wo_pad\": prefix_length_wo_pad,}\n",
    "        examples.append(example_dict)\n",
    "        if len(examples) >= num_examples:\n",
    "            break\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "from loading_utils import load_examples\n",
    "\n",
    "data_path = \"data/NPZ_gp_post_readingcomp_samelen.json\"\n",
    "# data_path = \"data/NPZ_ambiguous_samelen.json\"\n",
    "ignore_patch = True\n",
    "num_examples = 100\n",
    "length = 11\n",
    "pad_length = 32\n",
    "\n",
    "examples = load_examples_prefix_len(data_path, num_examples, model, length=length, #pad_to_length=pad_length\n",
    "                                     ignore_patch=False)\n",
    "# examples = load_examples(data_path, num_examples, model, length=length, # pad_to_length=pad_length,\n",
    "#                                      ignore_patch=True)\n",
    "print(len(examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from activation_utils import SparseAct\n",
    "\n",
    "tracer_kwargs = {'validate' : False, 'scan' : False}\n",
    "\n",
    "def _pe_ig(\n",
    "        clean,\n",
    "        patch,\n",
    "        model,\n",
    "        submodules,\n",
    "        dictionaries,\n",
    "        metric_fn,\n",
    "        steps=10,\n",
    "        metric_kwargs=dict(),\n",
    "):\n",
    "    \n",
    "    # first run through a test input to figure out which hidden states are tuples\n",
    "    is_tuple = {}\n",
    "    with model.trace(\"_\"):\n",
    "        for submodule in submodules:\n",
    "            is_tuple[submodule] = type(submodule.output.shape) == tuple\n",
    "\n",
    "    hidden_states_clean = {}\n",
    "    with model.trace(clean, **tracer_kwargs), t.no_grad():\n",
    "        for submodule in submodules:\n",
    "            dictionary = dictionaries[submodule]\n",
    "            x = submodule.output\n",
    "            if is_tuple[submodule]:\n",
    "                x = x[0]\n",
    "            f = dictionary.encode(x)\n",
    "            x_hat = dictionary.decode(f)\n",
    "            residual = x - x_hat\n",
    "            hidden_states_clean[submodule] = SparseAct(act=f.save(), res=residual.save())\n",
    "        metric_clean = metric_fn(model, **metric_kwargs).save()\n",
    "    hidden_states_clean = {k : v.value for k, v in hidden_states_clean.items()}\n",
    "\n",
    "    if patch is None:\n",
    "        hidden_states_patch = {\n",
    "            k : SparseAct(act=t.zeros_like(v.act), res=t.zeros_like(v.res)) for k, v in hidden_states_clean.items()\n",
    "        }\n",
    "        total_effect = None\n",
    "    else:\n",
    "        hidden_states_patch = {}\n",
    "        with model.trace(patch, **tracer_kwargs), t.no_grad():\n",
    "            for submodule in submodules:\n",
    "                dictionary = dictionaries[submodule]\n",
    "                x = submodule.output\n",
    "                if is_tuple[submodule]:\n",
    "                    x = x[0]\n",
    "                f = dictionary.encode(x)\n",
    "                x_hat = dictionary.decode(f)\n",
    "                residual = x - x_hat\n",
    "                hidden_states_patch[submodule] = SparseAct(act=f.save(), res=residual.save())\n",
    "            metric_patch = metric_fn(model, **metric_kwargs).save()\n",
    "        total_effect = (metric_patch.value - metric_clean.value).detach()\n",
    "        hidden_states_patch = {k : v.value for k, v in hidden_states_patch.items()}\n",
    "\n",
    "    effects = {}\n",
    "    deltas = {}\n",
    "    grads = {}\n",
    "    for submodule in submodules:\n",
    "        dictionary = dictionaries[submodule]\n",
    "        clean_state = hidden_states_clean[submodule]\n",
    "        patch_state = hidden_states_patch[submodule]\n",
    "        with model.trace(**tracer_kwargs) as tracer:\n",
    "            metrics = []\n",
    "            fs = []\n",
    "            for step in range(steps):\n",
    "                alpha = step / steps\n",
    "                f = (1 - alpha) * clean_state + alpha * patch_state\n",
    "                f.act.retain_grad()\n",
    "                f.res.retain_grad()\n",
    "                fs.append(f)\n",
    "                with tracer.invoke(clean, scan=tracer_kwargs['scan']):\n",
    "                    if is_tuple[submodule]:\n",
    "                        submodule.output[0][:] = dictionary.decode(f.act) + f.res\n",
    "                    else:\n",
    "                        submodule.output = dictionary.decode(f.act) + f.res\n",
    "                    metrics.append(metric_fn(model, **metric_kwargs))\n",
    "            metric = sum([m for m in metrics])\n",
    "            metric.sum().backward(retain_graph=True) # TODO : why is this necessary? Probably shouldn't be, contact jaden\n",
    "\n",
    "        mean_grad = sum([f.act.grad for f in fs]) / steps\n",
    "        mean_residual_grad = sum([f.res.grad for f in fs]) / steps\n",
    "        grad = SparseAct(act=mean_grad, res=mean_residual_grad)\n",
    "        delta = (patch_state - clean_state).detach() if patch_state is not None else -clean_state.detach()\n",
    "        effect = grad @ delta\n",
    "\n",
    "        effects[submodule] = effect\n",
    "        deltas[submodule] = delta\n",
    "        grads[submodule] = grad\n",
    "\n",
    "    return (effects, deltas, grads, total_effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:26<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive effects\n",
      "torch.Size([9, 32768])\n",
      "resid_0\n",
      "[[    6  8245]\n",
      " [    6  8349]\n",
      " [    6   656]\n",
      " [    8 27982]\n",
      " [    6 21196]\n",
      " [    7 21196]\n",
      " [    6  3639]\n",
      " [    6  9408]\n",
      " [    7 27982]\n",
      " [    7 17244]]\n",
      "tensor([0.0104, 0.0042, 0.0041, 0.0039, 0.0033, 0.0033, 0.0028, 0.0026, 0.0026,\n",
      "        0.0025], device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "negative effects\n",
      "resid_0\n",
      "[[    7  7196]\n",
      " [    8 26787]\n",
      " [    8 20619]\n",
      " [    7 23975]\n",
      " [    6 19969]\n",
      " [    6 15156]\n",
      " [    8  7196]\n",
      " [    6  2155]\n",
      " [    8 19387]\n",
      " [    8 12974]]\n",
      "tensor([-0.0052, -0.0020, -0.0015, -0.0012, -0.0011, -0.0011, -0.0010, -0.0010,\n",
      "        -0.0010, -0.0008], device='cuda:0', dtype=torch.float16)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 1\n",
    "num_examples = 100\n",
    "device = \"cuda\"\n",
    "num_examples = min([num_examples, len(examples)])\n",
    "n_batches = math.ceil(len(examples) / batch_size)\n",
    "batches = [\n",
    "    examples[batch*batch_size:(batch+1)*batch_size] for batch in range(n_batches)\n",
    "]\n",
    "sum_effects = {}\n",
    "\n",
    "for batch in tqdm(batches[1:]):\n",
    "    clean_answer_idxs = t.tensor([e['clean_answer'] for e in batch], dtype=t.long, device=device)\n",
    "    clean_inputs = t.cat([e['clean_prefix'] for e in batch], dim=0).to(device)\n",
    "\n",
    "    patch_answer_idxs = t.tensor([e['patch_answer'] for e in batch], dtype=t.long, device=device)\n",
    "    patch_inputs = t.cat([e['patch_prefix'] for e in batch], dim=0).to(device)\n",
    "    def metric_fn(model):\n",
    "        return (\n",
    "            t.gather(model.lm_head.output[:,-1,:], dim=-1, index=patch_answer_idxs.view(-1, 1)).squeeze(-1) - \\\n",
    "            t.gather(model.lm_head.output[:,-1,:], dim=-1, index=clean_answer_idxs.view(-1, 1)).squeeze(-1)\n",
    "        )\n",
    "\n",
    "    # for example in examples[:1]:\n",
    "    effects, _, _, _ = _pe_ig(\n",
    "        clean_inputs,\n",
    "        patch_inputs,\n",
    "        # None,\n",
    "        model,\n",
    "        resids,\n",
    "        dictionaries,\n",
    "        metric_fn\n",
    "    )\n",
    "    for submodule in resids:\n",
    "        if submodule not in sum_effects:\n",
    "            sum_effects[submodule] = effects[submodule].sum(dim=0)\n",
    "        else:\n",
    "            sum_effects[submodule] += effects[submodule].sum(dim=0)\n",
    "\n",
    "print(\"positive effects\")\n",
    "for idx, submodule in enumerate(resids):\n",
    "    sum_effects[submodule] /= num_examples\n",
    "    sum_effects[submodule] = sum_effects[submodule].act[:length, :]\n",
    "    print(sum_effects[submodule].shape)\n",
    "    print(f\"resid_{idx}\")\n",
    "    v, i = t.topk(sum_effects[submodule].flatten(), 10)\n",
    "    print(np.array(np.unravel_index(i.cpu().numpy(), sum_effects[submodule].shape)).T)\n",
    "    print(v)\n",
    "    print()\n",
    "\n",
    "print(\"negative effects\")\n",
    "for idx, submodule in enumerate(resids):\n",
    "    print(f\"resid_{idx}\")\n",
    "    v, i = t.topk(sum_effects[submodule].flatten(), 10, largest=False)\n",
    "    print(np.array(np.unravel_index(i.cpu().numpy(), sum_effects[submodule].shape)).T)\n",
    "    print(v)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resid_0\n",
      "torch.return_types.topk(\n",
      "values=tensor([2.3105, 0.6948, 0.5229, 0.4763, 0.4680, 0.4661, 0.3953, 0.3704, 0.3479,\n",
      "        0.2825], device='cuda:0', dtype=torch.float16),\n",
      "indices=tensor([17662, 19323,  8419, 17361,  1691, 23791,  4086, 15980, 25569, 20896],\n",
      "       device='cuda:0'))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, submodule in enumerate(resids):\n",
    "    print(f\"resid_{idx}\")\n",
    "    print(t.topk(-1 * sum_effects[submodule].act, 10))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load circuits, analyze\n",
    "circuit_path = \"circuits/NPS_ambiguous_samelen_dict10_node0.5_edge0.05_n24_aggnone_gpt2.pt\"\n",
    "circuit = t.load(open(circuit_path, 'rb'))\n",
    "\n",
    "for submod in circuit[\"nodes\"]:\n",
    "    effects = circuit[\"nodes\"][submod]\n",
    "    top_effects = t.topk(effects.act, 10)\n",
    "    print(submod, top_effects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 24576])\n"
     ]
    }
   ],
   "source": [
    "with model.trace(\"testing 1\"):\n",
    "    out_save = resids[4].output\n",
    "    f = dictionaries[resids[4]].encode(out_save).save()\n",
    "print(f.value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
