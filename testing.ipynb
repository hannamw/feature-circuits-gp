{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zstandard as zstd\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "from nnsight import LanguageModel\n",
    "from dictionary_learning.buffer import ActivationBuffer\n",
    "from dictionary_learning.dictionary import AutoEncoder\n",
    "from dictionary_learning.training import trainSAE\n",
    "import torch as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up model\n",
    "model = LanguageModel('EleutherAI/pythia-70m-deduped', device_map='cuda:0')\n",
    "submodule = model.gpt_neox.layers[2].mlp.dense_4h_to_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up data as a generator\n",
    "data_path = '/share/data/datasets/pile/the-eye.eu/public/AI/pile/train/00.jsonl.zst' # this dataset is not available anymore on the-eye.eu\n",
    "compressed_file = open(data_path, 'rb')\n",
    "dctx = zstd.ZstdDecompressor()\n",
    "reader = dctx.stream_reader(compressed_file)\n",
    "text_stream = io.TextIOWrapper(reader, encoding='utf-8')\n",
    "def generator():\n",
    "    for line in text_stream:\n",
    "        yield json.loads(line)['text']\n",
    "data = generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = ActivationBuffer(\n",
    "    data,\n",
    "    model,\n",
    "    submodule,\n",
    "    in_batch_size=64,\n",
    "    out_batch_size=4096,\n",
    "    n_ctxs=5e4,\n",
    "    device='cuda:0'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = trainSAE(\n",
    "    buffer,\n",
    "    activation_dim=512,\n",
    "    dictionary_size = 8 * 512,\n",
    "    steps=1000,\n",
    "    lr = 1e-3,\n",
    "    sparsity_penalty = 3e-4,\n",
    "    entropy=False,\n",
    "    resample_steps = 1000,\n",
    "    log_steps = None,\n",
    "    device='cuda:0'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = AutoEncoder(512, 4*512)#.cuda()\n",
    "ae.load_state_dict(t.load('autoencoders/reg0.0001_entFalse.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts = next(buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dict_acts = ae.encode(acts.cuda())\n",
    "freqs = (dict_acts !=0).sum(dim=0) / dict_acts.shape[0]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(freqs.cpu(), bins=np.logspace(np.log10(1e-4), np.log10(4096), 100))\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.nonzero(1 - (dict_acts == 0).all(dim=0).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae.encode(acts.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "import torch as t\n",
    "\n",
    "inputs = buffer.tokenized_batch()\n",
    "with model.generate(max_new_tokens=1, pad_token_id=model.tokenizer.pad_token_id) as generator:\n",
    "    with generator.invoke(inputs['input_ids'], scan=False) as invoker:\n",
    "        hidden_states = submodule.output.save()\n",
    "dictionary_activations = ae.encode(hidden_states.value)\n",
    "flattened_acts = rearrange(dictionary_activations, 'b n d -> (b n) d')\n",
    "freqs = (flattened_acts !=0).sum(dim=0) / flattened_acts.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, freq in enumerate(freqs):\n",
    "    if 3e-3 < freq and freq < 1e-2:\n",
    "        print(f\"feat {idx} freq: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_decode(x):\n",
    "    if isinstance(x, int):\n",
    "        return model.tokenizer.decode(x)\n",
    "    else:\n",
    "        return [list_decode(y) for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 30\n",
    "feat = 400\n",
    "acts = dictionary_activations[:, :, feat].cpu()\n",
    "flattened_acts = rearrange(acts, 'b l -> (b l)')\n",
    "topk_indices = t.argsort(flattened_acts, dim=0, descending=True)[:k]\n",
    "batch_indices = topk_indices // acts.shape[1]\n",
    "token_indices = topk_indices % acts.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuitsvis.activations import text_neuron_activations\n",
    "\n",
    "tokens = [\n",
    "    inputs['input_ids'][batch_idx, :token_idx+1].tolist() for batch_idx, token_idx in zip(batch_indices, token_indices)\n",
    "]\n",
    "tokens = list_decode(tokens)\n",
    "activations = [\n",
    "    acts[batch_idx, :token_id+1, None, None] for batch_idx, token_id in zip(batch_indices, token_indices)\n",
    "]\n",
    "text_neuron_activations(tokens, activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.tokenizer.decode(inputs[40].ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "k = 10\n",
    "found_indices = torch.argsort(dictionary_activations, descending=True)[:k]\n",
    "num_datapoints = int(dictionary_activations.shape[0]/128)\n",
    "datapoint_indices =[np.unravel_index(i, (64, 128)) for i in found_indices]\n",
    "text_list = []\n",
    "full_text = []\n",
    "token_list = []\n",
    "full_token_list = []\n",
    "for md, s_ind in datapoint_indices:\n",
    "    md = int(md)\n",
    "    s_ind = int(s_ind)\n",
    "    full_tok = torch.tensor(dataset[md][\"input_ids\"])\n",
    "    full_text.append(tokenizer.decode(full_tok))\n",
    "    tok = dataset[md][\"input_ids\"][:s_ind+1]\n",
    "    text = tokenizer.decode(tok)\n",
    "    text_list.append(text)\n",
    "    token_list.append(tok)\n",
    "    full_token_list.append(full_tok)\n",
    "text_list, full_text, token_list, full_token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can use the model to get the activations\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "import torch \n",
    "# num_features, d_model = autoencoder.encoder.shape # Fix this for shape purposes\n",
    "texts = buffer.text_batch()\n",
    "datapoints = len(texts)\n",
    "batch_size = 64\n",
    "neuron_activations = torch.zeros((datapoints*max_length, d_model))\n",
    "dictionary_activations = torch.zeros((datapoints*max_length))\n",
    "\n",
    "with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "    dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "    for i, batch in enumerate(tqdm(dl)):\n",
    "        # Replace this with your residual stream stuff\n",
    "        # _, cache = model.run_with_cache(batch.to(device))\n",
    "        # batched_neuron_activations = rearrange(cache[cache_name], \"b s n -> (b s) n\" )\n",
    "\n",
    "        # Replace with your projection to probe direction\n",
    "        # batched_dictionary_activations = smaller_auto_encoder.encode(batched_neuron_activations)\n",
    "        dictionary_activations[i*batch_size*max_length:(i+1)*batch_size*max_length] = batched_dictionary_activations.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "def entropy(p):\n",
    "    p = p/p.sum(dim=-1, keepdim=True)\n",
    "    log_p = p.log().nan_to_num()\n",
    "    entropies = -(p * log_p).sum(dim=-1)\n",
    "    out = entropies.nan_to_num().mean()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = t.Tensor([[0, 0,0], [1, 4, 2]])\n",
    "entropy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x * x.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0 * float(\"-inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "\n",
    "def entropy(p):\n",
    "    eps = 1e-8\n",
    "    # Calculate the sum along the last dimension (i.e., sum of each vector in the batch)\n",
    "    p_sum = p.sum(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Avoid in-place operations that can interfere with autograd\n",
    "    p_normed = p / (p_sum + eps)  # Add eps to prevent division by zero\n",
    "    \n",
    "    # Compute the log safely, adding eps inside the log to prevent log(0)\n",
    "    p_log = t.log(p_normed + eps)  # Add eps to prevent log(0)\n",
    "\n",
    "    # Compute the entropy, this will give zero for elements where p_normed is zero\n",
    "    ent = -(p_normed * p_log)\n",
    "    \n",
    "    # Zero out the entropy where the sum of p is zero (i.e., for all-zero vectors)\n",
    "    ent = t.where(p_sum > 0, ent, t.zeros_like(ent))\n",
    "\n",
    "    # Sum the entropy across the features and then take the mean across the batch\n",
    "    return ent.sum(dim=-1).mean()\n",
    "\n",
    "# Example usage:\n",
    "batch_size = 3\n",
    "vector_length = 5\n",
    "p = t.tensor([[0.1, 0.2, 0.7, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0],  # All-zero vector\n",
    "              [0.3, 0.3, 0.4, 0.0, 0.0]], requires_grad=True)\n",
    "\n",
    "entropy_value = entropy(p)\n",
    "entropy_value.backward()\n",
    "\n",
    "print(\"Entropy:\", entropy_value.item())\n",
    "print(\"Gradients:\", p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
