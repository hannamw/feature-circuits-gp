{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/u/smarks/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import zstandard as zstd\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "from nnsight import LanguageModel\n",
    "from dictionary_learning.buffer import ActivationBuffer\n",
    "from dictionary_learning.dictionary import AutoEncoder\n",
    "from dictionary_learning.training import trainSAE\n",
    "import torch as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel('EleutherAI/pythia-70m-deduped', device_map='cuda:0')\n",
    "submodule = model.gpt_neox.layers[2].mlp.dense_4h_to_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up data as a generator\n",
    "data_path = '/share/data/datasets/pile/the-eye.eu/public/AI/pile/train/00.jsonl.zst'\n",
    "compressed_file = open(data_path, 'rb')\n",
    "dctx = zstd.ZstdDecompressor()\n",
    "reader = dctx.stream_reader(compressed_file)\n",
    "text_stream = io.TextIOWrapper(reader, encoding='utf-8')\n",
    "def generator():\n",
    "    for line in text_stream:\n",
    "        yield json.loads(line)['text']\n",
    "data = generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = ActivationBuffer(\n",
    "    data,\n",
    "    model,\n",
    "    submodule,\n",
    "    in_batch_size=64,\n",
    "    out_batch_size=4096,\n",
    "    n_ctxs=5e4,\n",
    "    device='cuda:0'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refreshing buffer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/u/smarks/.local/lib/python3.8/site-packages/transformers/generation/utils.py:2507: UserWarning: Specified kernel cache directory is not writable! This disables kernel caching. Specified directory is /share/u/smarks/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1460.)\n",
      "  next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buffer refreshed...\n",
      "step 0 autoencoder loss: 0.7744720578193665\n",
      "step 0 % inactive: 0.0\n",
      "step 0 reconstruction loss: 3.4654366970062256, 4.7864532470703125, 4.347995281219482\n",
      "step 30 autoencoder loss: 0.09351757168769836\n",
      "step 30 % inactive: 0.0\n",
      "step 30 reconstruction loss: 3.540593147277832, 3.9492151737213135, 4.3314032554626465\n",
      "step 60 autoencoder loss: 0.0742626041173935\n",
      "step 60 % inactive: 0.0\n",
      "step 60 reconstruction loss: 3.4906044006347656, 3.6950414180755615, 4.261730194091797\n",
      "refreshing buffer...\n",
      "buffer refreshed...\n",
      "step 90 autoencoder loss: 0.06487508118152618\n",
      "step 90 % inactive: 0.0\n",
      "step 90 reconstruction loss: 3.4171013832092285, 3.610360622406006, 4.20655632019043\n",
      "step 120 autoencoder loss: 0.0584874227643013\n",
      "step 120 % inactive: 0.001708984375\n",
      "step 120 reconstruction loss: 3.3146800994873047, 3.4892454147338867, 4.09421968460083\n",
      "step 150 autoencoder loss: 0.054265595972537994\n",
      "step 150 % inactive: 0.01318359375\n",
      "step 150 reconstruction loss: 3.357200860977173, 3.5048091411590576, 4.183021545410156\n",
      "refreshing buffer...\n",
      "buffer refreshed...\n",
      "step 180 autoencoder loss: 0.05122814327478409\n",
      "step 180 % inactive: 0.046142578125\n",
      "step 180 reconstruction loss: 3.5326201915740967, 3.6785314083099365, 4.345293045043945\n",
      "step 210 autoencoder loss: 0.048912759870290756\n",
      "step 210 % inactive: 0.183349609375\n",
      "step 210 reconstruction loss: 3.5029966831207275, 3.622303009033203, 4.2562127113342285\n",
      "refreshing buffer...\n",
      "buffer refreshed...\n",
      "step 240 autoencoder loss: 0.047018684446811676\n",
      "step 240 % inactive: 0.078857421875\n",
      "step 240 reconstruction loss: 3.234924077987671, 3.3503406047821045, 4.04849100112915\n",
      "step 270 autoencoder loss: 0.045315805822610855\n",
      "step 270 % inactive: 0.164306640625\n",
      "step 270 reconstruction loss: 3.431135416030884, 3.5279459953308105, 4.196701526641846\n",
      "resampling 60 dead neurons\n",
      "step 300 autoencoder loss: 0.679692268371582\n",
      "step 300 % inactive: 0.4521484375\n",
      "step 300 reconstruction loss: 3.280283212661743, 3.9212992191314697, 4.079108715057373\n",
      "refreshing buffer...\n",
      "buffer refreshed...\n",
      "step 330 autoencoder loss: 0.055500373244285583\n",
      "step 330 % inactive: 0.122802734375\n",
      "step 330 reconstruction loss: 3.5458431243896484, 3.7082362174987793, 4.290998458862305\n",
      "step 360 autoencoder loss: 0.04878583922982216\n",
      "step 360 % inactive: 0.09033203125\n",
      "step 360 reconstruction loss: 3.4436936378479004, 3.5735065937042236, 4.222159385681152\n",
      "step 390 autoencoder loss: 0.04627137631177902\n",
      "step 390 % inactive: 0.44384765625\n",
      "step 390 reconstruction loss: 3.5259060859680176, 3.645925521850586, 4.300594329833984\n",
      "resampling 61 dead neurons\n",
      "refreshing buffer...\n",
      "buffer refreshed...\n",
      "step 420 autoencoder loss: 0.10649731755256653\n",
      "step 420 % inactive: 0.23779296875\n",
      "step 420 reconstruction loss: 3.28666090965271, 3.6271800994873047, 4.150146961212158\n",
      "step 450 autoencoder loss: 0.050789233297109604\n",
      "step 450 % inactive: 0.3232421875\n",
      "step 450 reconstruction loss: 3.4914019107818604, 3.661933660507202, 4.272274971008301\n",
      "refreshing buffer...\n",
      "buffer refreshed...\n",
      "step 480 autoencoder loss: 0.04729077219963074\n",
      "step 480 % inactive: 0.270263671875\n",
      "step 480 reconstruction loss: 3.4631192684173584, 3.5957388877868652, 4.2546610832214355\n",
      "resampling 72 dead neurons\n",
      "step 510 autoencoder loss: 0.10180880129337311\n",
      "step 510 % inactive: 0.52197265625\n",
      "step 510 reconstruction loss: 3.499508857727051, 3.868467330932617, 4.513940811157227\n",
      "step 540 autoencoder loss: 0.05450707674026489\n",
      "step 540 % inactive: 0.37451171875\n",
      "step 540 reconstruction loss: 3.419348955154419, 3.6007845401763916, 4.219581604003906\n",
      "refreshing buffer...\n",
      "buffer refreshed...\n",
      "step 570 autoencoder loss: 0.04759889841079712\n",
      "step 570 % inactive: 0.159912109375\n",
      "step 570 reconstruction loss: 3.4797470569610596, 3.6352100372314453, 4.2432332038879395\n",
      "resampling 68 dead neurons\n",
      "step 600 autoencoder loss: 0.6234667897224426\n",
      "step 600 % inactive: 0.513671875\n",
      "step 600 reconstruction loss: 3.4454360008239746, 4.101861476898193, 4.26701021194458\n",
      "step 630 autoencoder loss: 0.06156277284026146\n",
      "step 630 % inactive: 0.4287109375\n",
      "step 630 reconstruction loss: 3.5918192863464355, 3.8262672424316406, 4.325906276702881\n",
      "refreshing buffer...\n",
      "buffer refreshed...\n",
      "step 660 autoencoder loss: 0.048730116337537766\n",
      "step 660 % inactive: 0.37841796875\n",
      "step 660 reconstruction loss: 3.353856325149536, 3.5086185932159424, 4.112893104553223\n",
      "step 690 autoencoder loss: 0.04581856355071068\n",
      "step 690 % inactive: 0.432373046875\n",
      "step 690 reconstruction loss: 3.4098334312438965, 3.5674712657928467, 4.251517295837402\n",
      "resampling 71 dead neurons\n",
      "refreshing buffer...\n",
      "buffer refreshed...\n",
      "step 720 autoencoder loss: 0.06664754450321198\n",
      "step 720 % inactive: 0.474609375\n",
      "step 720 reconstruction loss: 3.436742067337036, 3.6777424812316895, 4.224893569946289\n",
      "step 750 autoencoder loss: 0.05268462002277374\n",
      "step 750 % inactive: 0.444580078125\n",
      "step 750 reconstruction loss: 3.3469977378845215, 3.5351150035858154, 4.210716247558594\n",
      "step 780 autoencoder loss: 0.045080289244651794\n",
      "step 780 % inactive: 0.421142578125\n",
      "step 780 reconstruction loss: 3.281423807144165, 3.4633023738861084, 4.472424030303955\n",
      "refreshing buffer...\n",
      "buffer refreshed...\n",
      "resampling 149 dead neurons\n",
      "step 810 autoencoder loss: 0.08383414149284363\n",
      "step 810 % inactive: 0.55810546875\n",
      "step 810 reconstruction loss: 3.507305860519409, 3.86366868019104, 4.227848052978516\n",
      "step 840 autoencoder loss: 0.05342121794819832\n",
      "step 840 % inactive: 0.47216796875\n",
      "step 840 reconstruction loss: 3.3339483737945557, 3.548475503921509, 4.2321271896362305\n",
      "step 870 autoencoder loss: 0.045919790863990784\n",
      "step 870 % inactive: 0.43017578125\n",
      "step 870 reconstruction loss: 3.4160125255584717, 3.5986876487731934, 4.429792404174805\n",
      "refreshing buffer...\n",
      "buffer refreshed...\n",
      "resampling 198 dead neurons\n",
      "step 900 autoencoder loss: 0.7355830669403076\n",
      "step 900 % inactive: 0.572509765625\n",
      "step 900 reconstruction loss: 3.3851375579833984, 4.095330238342285, 4.261585712432861\n",
      "step 930 autoencoder loss: 0.054900895804166794\n",
      "step 930 % inactive: 0.4658203125\n",
      "step 930 reconstruction loss: 3.4564175605773926, 3.716606378555298, 4.392613887786865\n",
      "refreshing buffer...\n",
      "buffer refreshed...\n",
      "step 960 autoencoder loss: 0.049480944871902466\n",
      "step 960 % inactive: 0.201171875\n",
      "step 960 reconstruction loss: 3.4296765327453613, 3.6100053787231445, 4.218535900115967\n",
      "step 990 autoencoder loss: 0.04462360590696335\n",
      "step 990 % inactive: 0.502197265625\n",
      "step 990 reconstruction loss: 3.4168930053710938, 3.578477382659912, 4.297198295593262\n",
      "resampling 139 dead neurons\n"
     ]
    }
   ],
   "source": [
    "ae = trainSAE(\n",
    "    buffer,\n",
    "    activation_dim=512,\n",
    "    dictionary_size = 8 * 512,\n",
    "    steps=1000,\n",
    "    lr = 1e-3,\n",
    "    sparsity_penalty = 3e-4,\n",
    "    entropy=False,\n",
    "    resample_steps = 1000,\n",
    "    log_steps = None,\n",
    "    device='cuda:0'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae = AutoEncoder(512, 4*512).cuda()\n",
    "ae.load_state_dict(t.load('autoencoders/reg0.0001_entFalse.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts = next(buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /tmp/matplotlib-iqmhgtg5 because the default path (/share/u/smarks/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "dict_acts = ae.encode(acts.cuda())\n",
    "freqs = (dict_acts !=0).sum(dim=0) / dict_acts.shape[0]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(freqs.cpu(), bins=np.logspace(np.log10(1e-4), np.log10(4096), 100))\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1],\n",
       "        [   2],\n",
       "        [   4],\n",
       "        ...,\n",
       "        [4083],\n",
       "        [4089],\n",
       "        [4093]], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.nonzero(1 - (dict_acts == 0).all(dim=0).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.1049e-01, 7.3272e-02, 2.1830e-01,  ..., 2.0369e-01, 4.2124e-02,\n",
       "         2.5463e-01],\n",
       "        [2.1436e-01, 3.4365e-02, 1.2208e-01,  ..., 3.8042e-02, 0.0000e+00,\n",
       "         3.0747e-02],\n",
       "        [2.8846e-01, 2.4418e-01, 2.6713e-01,  ..., 3.2065e-01, 5.4556e-03,\n",
       "         3.1329e-01],\n",
       "        ...,\n",
       "        [4.7642e-01, 2.5845e-01, 2.6834e-01,  ..., 0.0000e+00, 1.3256e-01,\n",
       "         3.3008e-01],\n",
       "        [7.5465e-01, 0.0000e+00, 0.0000e+00,  ..., 6.1018e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [1.4123e-01, 3.7879e-01, 1.3614e-01,  ..., 1.5555e-01, 2.5206e-01,\n",
       "         2.3779e-01]], device='cuda:0', grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae.encode(acts.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "import torch as t\n",
    "\n",
    "inputs = buffer.tokenized_batch()\n",
    "with model.generate(max_new_tokens=1, pad_token_id=model.tokenizer.pad_token_id) as generator:\n",
    "    with generator.invoke(inputs['input_ids'], scan=False) as invoker:\n",
    "        hidden_states = submodule.output.save()\n",
    "dictionary_activations = ae.encode(hidden_states.value)\n",
    "flattened_acts = rearrange(dictionary_activations, 'b n d -> (b n) d')\n",
    "freqs = (flattened_acts !=0).sum(dim=0) / flattened_acts.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat 5 freq: 0.007568359375\n",
      "feat 47 freq: 0.00732421875\n",
      "feat 48 freq: 0.0054931640625\n",
      "feat 74 freq: 0.0074462890625\n",
      "feat 118 freq: 0.0076904296875\n",
      "feat 126 freq: 0.0072021484375\n",
      "feat 128 freq: 0.0074462890625\n",
      "feat 146 freq: 0.0057373046875\n",
      "feat 173 freq: 0.0072021484375\n",
      "feat 179 freq: 0.009765625\n",
      "feat 189 freq: 0.00732421875\n",
      "feat 233 freq: 0.0072021484375\n",
      "feat 263 freq: 0.0072021484375\n",
      "feat 269 freq: 0.0098876953125\n",
      "feat 280 freq: 0.008056640625\n",
      "feat 343 freq: 0.0072021484375\n",
      "feat 346 freq: 0.0072021484375\n",
      "feat 375 freq: 0.007568359375\n",
      "feat 381 freq: 0.0078125\n",
      "feat 383 freq: 0.007568359375\n",
      "feat 386 freq: 0.0089111328125\n",
      "feat 390 freq: 0.0081787109375\n",
      "feat 400 freq: 0.005126953125\n",
      "feat 403 freq: 0.0087890625\n",
      "feat 410 freq: 0.007568359375\n",
      "feat 422 freq: 0.00732421875\n",
      "feat 446 freq: 0.007568359375\n",
      "feat 447 freq: 0.0074462890625\n",
      "feat 452 freq: 0.007568359375\n",
      "feat 456 freq: 0.00732421875\n",
      "feat 473 freq: 0.0072021484375\n",
      "feat 474 freq: 0.00732421875\n",
      "feat 489 freq: 0.0084228515625\n",
      "feat 499 freq: 0.0045166015625\n",
      "feat 514 freq: 0.0064697265625\n",
      "feat 521 freq: 0.0067138671875\n",
      "feat 541 freq: 0.0081787109375\n",
      "feat 563 freq: 0.007568359375\n",
      "feat 581 freq: 0.0079345703125\n",
      "feat 582 freq: 0.00732421875\n",
      "feat 593 freq: 0.0076904296875\n",
      "feat 607 freq: 0.00732421875\n",
      "feat 637 freq: 0.0074462890625\n",
      "feat 640 freq: 0.0040283203125\n",
      "feat 647 freq: 0.00732421875\n",
      "feat 653 freq: 0.00732421875\n",
      "feat 659 freq: 0.0072021484375\n",
      "feat 673 freq: 0.004150390625\n",
      "feat 677 freq: 0.0089111328125\n",
      "feat 678 freq: 0.005615234375\n",
      "feat 683 freq: 0.007080078125\n",
      "feat 693 freq: 0.0079345703125\n",
      "feat 702 freq: 0.0037841796875\n",
      "feat 726 freq: 0.00732421875\n",
      "feat 748 freq: 0.0074462890625\n",
      "feat 753 freq: 0.009521484375\n",
      "feat 769 freq: 0.007080078125\n",
      "feat 775 freq: 0.00732421875\n",
      "feat 782 freq: 0.005126953125\n",
      "feat 805 freq: 0.0048828125\n",
      "feat 844 freq: 0.005126953125\n",
      "feat 887 freq: 0.0072021484375\n",
      "feat 889 freq: 0.007080078125\n",
      "feat 900 freq: 0.0074462890625\n",
      "feat 926 freq: 0.00732421875\n",
      "feat 969 freq: 0.0098876953125\n",
      "feat 982 freq: 0.0072021484375\n",
      "feat 1022 freq: 0.0074462890625\n",
      "feat 1043 freq: 0.0072021484375\n",
      "feat 1065 freq: 0.0074462890625\n",
      "feat 1087 freq: 0.0078125\n",
      "feat 1105 freq: 0.007080078125\n",
      "feat 1119 freq: 0.0072021484375\n",
      "feat 1124 freq: 0.00341796875\n",
      "feat 1130 freq: 0.0042724609375\n",
      "feat 1134 freq: 0.00439453125\n",
      "feat 1138 freq: 0.00732421875\n",
      "feat 1144 freq: 0.009033203125\n",
      "feat 1164 freq: 0.00732421875\n",
      "feat 1237 freq: 0.0076904296875\n",
      "feat 1245 freq: 0.0072021484375\n",
      "feat 1299 freq: 0.00830078125\n",
      "feat 1303 freq: 0.00732421875\n",
      "feat 1311 freq: 0.0072021484375\n",
      "feat 1315 freq: 0.0079345703125\n",
      "feat 1334 freq: 0.0084228515625\n",
      "feat 1358 freq: 0.00732421875\n",
      "feat 1377 freq: 0.0093994140625\n",
      "feat 1384 freq: 0.0072021484375\n",
      "feat 1402 freq: 0.0068359375\n",
      "feat 1411 freq: 0.00732421875\n",
      "feat 1415 freq: 0.003173828125\n",
      "feat 1425 freq: 0.0072021484375\n",
      "feat 1473 freq: 0.00732421875\n",
      "feat 1502 freq: 0.003662109375\n",
      "feat 1549 freq: 0.007568359375\n",
      "feat 1552 freq: 0.0052490234375\n",
      "feat 1569 freq: 0.00830078125\n",
      "feat 1591 freq: 0.008056640625\n",
      "feat 1608 freq: 0.00732421875\n",
      "feat 1641 freq: 0.0069580078125\n",
      "feat 1684 freq: 0.008056640625\n",
      "feat 1685 freq: 0.004638671875\n",
      "feat 1695 freq: 0.0074462890625\n",
      "feat 1720 freq: 0.00732421875\n",
      "feat 1737 freq: 0.0072021484375\n",
      "feat 1754 freq: 0.0079345703125\n",
      "feat 1759 freq: 0.0074462890625\n",
      "feat 1787 freq: 0.00830078125\n",
      "feat 1807 freq: 0.00732421875\n",
      "feat 1819 freq: 0.0064697265625\n",
      "feat 1831 freq: 0.00732421875\n",
      "feat 1882 freq: 0.00732421875\n",
      "feat 1905 freq: 0.0072021484375\n",
      "feat 1906 freq: 0.00732421875\n",
      "feat 1931 freq: 0.0074462890625\n",
      "feat 1955 freq: 0.0069580078125\n",
      "feat 2018 freq: 0.0072021484375\n",
      "feat 2023 freq: 0.00390625\n",
      "feat 2024 freq: 0.0076904296875\n",
      "feat 2025 freq: 0.0045166015625\n",
      "feat 2035 freq: 0.0047607421875\n",
      "feat 2037 freq: 0.0074462890625\n",
      "feat 2057 freq: 0.0081787109375\n",
      "feat 2064 freq: 0.00732421875\n",
      "feat 2081 freq: 0.00732421875\n",
      "feat 2087 freq: 0.00732421875\n",
      "feat 2105 freq: 0.0045166015625\n",
      "feat 2119 freq: 0.0078125\n",
      "feat 2138 freq: 0.0037841796875\n",
      "feat 2175 freq: 0.0047607421875\n",
      "feat 2178 freq: 0.0072021484375\n",
      "feat 2185 freq: 0.0072021484375\n",
      "feat 2202 freq: 0.0072021484375\n",
      "feat 2222 freq: 0.005126953125\n",
      "feat 2231 freq: 0.0079345703125\n",
      "feat 2247 freq: 0.0084228515625\n",
      "feat 2255 freq: 0.0072021484375\n",
      "feat 2269 freq: 0.0072021484375\n",
      "feat 2271 freq: 0.0069580078125\n",
      "feat 2306 freq: 0.006103515625\n",
      "feat 2316 freq: 0.00927734375\n",
      "feat 2325 freq: 0.00830078125\n",
      "feat 2331 freq: 0.005615234375\n",
      "feat 2343 freq: 0.0076904296875\n",
      "feat 2344 freq: 0.00732421875\n",
      "feat 2347 freq: 0.0072021484375\n",
      "feat 2399 freq: 0.00732421875\n",
      "feat 2425 freq: 0.007568359375\n",
      "feat 2433 freq: 0.0074462890625\n",
      "feat 2437 freq: 0.00732421875\n",
      "feat 2443 freq: 0.0072021484375\n",
      "feat 2444 freq: 0.0074462890625\n",
      "feat 2454 freq: 0.0074462890625\n",
      "feat 2483 freq: 0.00732421875\n",
      "feat 2520 freq: 0.0084228515625\n",
      "feat 2532 freq: 0.0072021484375\n",
      "feat 2558 freq: 0.00830078125\n",
      "feat 2563 freq: 0.0074462890625\n",
      "feat 2596 freq: 0.0078125\n",
      "feat 2598 freq: 0.0081787109375\n",
      "feat 2628 freq: 0.0078125\n",
      "feat 2640 freq: 0.007568359375\n",
      "feat 2668 freq: 0.00830078125\n",
      "feat 2697 freq: 0.0074462890625\n",
      "feat 2719 freq: 0.0072021484375\n",
      "feat 2758 freq: 0.0030517578125\n",
      "feat 2770 freq: 0.0074462890625\n",
      "feat 2771 freq: 0.0037841796875\n",
      "feat 2798 freq: 0.0081787109375\n",
      "feat 2808 freq: 0.0074462890625\n",
      "feat 2817 freq: 0.0072021484375\n",
      "feat 2829 freq: 0.0074462890625\n",
      "feat 2856 freq: 0.0079345703125\n",
      "feat 2887 freq: 0.0079345703125\n",
      "feat 2889 freq: 0.0081787109375\n",
      "feat 2897 freq: 0.00732421875\n",
      "feat 2915 freq: 0.0076904296875\n",
      "feat 2927 freq: 0.00732421875\n",
      "feat 2928 freq: 0.0091552734375\n",
      "feat 2936 freq: 0.0079345703125\n",
      "feat 2954 freq: 0.00732421875\n",
      "feat 2956 freq: 0.0076904296875\n",
      "feat 2957 freq: 0.005859375\n",
      "feat 2959 freq: 0.008056640625\n",
      "feat 2965 freq: 0.0037841796875\n",
      "feat 2974 freq: 0.0045166015625\n",
      "feat 2989 freq: 0.0074462890625\n",
      "feat 3012 freq: 0.00732421875\n",
      "feat 3015 freq: 0.0072021484375\n",
      "feat 3023 freq: 0.00634765625\n",
      "feat 3029 freq: 0.0074462890625\n",
      "feat 3048 freq: 0.0067138671875\n",
      "feat 3080 freq: 0.00830078125\n",
      "feat 3100 freq: 0.00732421875\n",
      "feat 3101 freq: 0.0076904296875\n",
      "feat 3102 freq: 0.00732421875\n",
      "feat 3109 freq: 0.0076904296875\n",
      "feat 3127 freq: 0.0072021484375\n",
      "feat 3143 freq: 0.0078125\n",
      "feat 3144 freq: 0.0048828125\n",
      "feat 3148 freq: 0.00732421875\n",
      "feat 3177 freq: 0.0074462890625\n",
      "feat 3216 freq: 0.008056640625\n",
      "feat 3225 freq: 0.00732421875\n",
      "feat 3227 freq: 0.0040283203125\n",
      "feat 3259 freq: 0.0078125\n",
      "feat 3260 freq: 0.007568359375\n",
      "feat 3262 freq: 0.0045166015625\n",
      "feat 3283 freq: 0.00537109375\n",
      "feat 3288 freq: 0.007080078125\n",
      "feat 3314 freq: 0.0048828125\n",
      "feat 3319 freq: 0.0074462890625\n",
      "feat 3351 freq: 0.0072021484375\n",
      "feat 3359 freq: 0.007568359375\n",
      "feat 3370 freq: 0.008056640625\n",
      "feat 3371 freq: 0.00927734375\n",
      "feat 3376 freq: 0.00732421875\n",
      "feat 3420 freq: 0.0074462890625\n",
      "feat 3429 freq: 0.003173828125\n",
      "feat 3437 freq: 0.00390625\n",
      "feat 3438 freq: 0.0081787109375\n",
      "feat 3439 freq: 0.0040283203125\n",
      "feat 3448 freq: 0.0032958984375\n",
      "feat 3492 freq: 0.008056640625\n",
      "feat 3515 freq: 0.006591796875\n",
      "feat 3523 freq: 0.0078125\n",
      "feat 3536 freq: 0.0072021484375\n",
      "feat 3543 freq: 0.00537109375\n",
      "feat 3563 freq: 0.0072021484375\n",
      "feat 3596 freq: 0.0032958984375\n",
      "feat 3608 freq: 0.0081787109375\n",
      "feat 3612 freq: 0.007568359375\n",
      "feat 3660 freq: 0.0069580078125\n",
      "feat 3665 freq: 0.008544921875\n",
      "feat 3670 freq: 0.00732421875\n",
      "feat 3675 freq: 0.0035400390625\n",
      "feat 3702 freq: 0.0072021484375\n",
      "feat 3704 freq: 0.007568359375\n",
      "feat 3717 freq: 0.005126953125\n",
      "feat 3729 freq: 0.0059814453125\n",
      "feat 3768 freq: 0.0052490234375\n",
      "feat 3775 freq: 0.00732421875\n",
      "feat 3779 freq: 0.0074462890625\n",
      "feat 3840 freq: 0.0068359375\n",
      "feat 3859 freq: 0.00732421875\n",
      "feat 3874 freq: 0.00732421875\n",
      "feat 3923 freq: 0.0035400390625\n",
      "feat 3929 freq: 0.009521484375\n",
      "feat 3941 freq: 0.00732421875\n",
      "feat 3976 freq: 0.0072021484375\n",
      "feat 3983 freq: 0.007568359375\n",
      "feat 3994 freq: 0.006591796875\n",
      "feat 4000 freq: 0.0064697265625\n",
      "feat 4012 freq: 0.00732421875\n",
      "feat 4033 freq: 0.0072021484375\n",
      "feat 4041 freq: 0.0068359375\n",
      "feat 4081 freq: 0.00732421875\n",
      "feat 4086 freq: 0.0079345703125\n",
      "feat 4090 freq: 0.0052490234375\n"
     ]
    }
   ],
   "source": [
    "for idx, freq in enumerate(freqs):\n",
    "    if 3e-3 < freq and freq < 1e-2:\n",
    "        print(f\"feat {idx} freq: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_decode(x):\n",
    "    if isinstance(x, int):\n",
    "        return model.tokenizer.decode(x)\n",
    "    else:\n",
    "        return [list_decode(y) for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 30\n",
    "feat = 400\n",
    "acts = dictionary_activations[:, :, feat].cpu()\n",
    "flattened_acts = rearrange(acts, 'b l -> (b l)')\n",
    "topk_indices = t.argsort(flattened_acts, dim=0, descending=True)[:k]\n",
    "batch_indices = topk_indices // acts.shape[1]\n",
    "token_indices = topk_indices % acts.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-e0364f9e-6888\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-e0364f9e-6888\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [[\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"All\", \"'s\", \" I\", \" need\", \" now\"], [\"1\", \".\", \" Field\", \" of\", \" the\", \" Invention\", \"\\n\", \"This\", \" invention\", \" relates\", \" to\", \" patient\", \" transport\", \" systems\", \",\", \" and\", \" more\", \" particularly\"], [\"For\", \" Journal\", \"ists\", \"\\n\", \"\\n\", \"Media\", \" T\", \"rends\", \" Award\", \" goes\", \" to\", \" K\", \"RR\", \"i\", \"T\", \" for\", \" digit\", \"isation\", \" process\", \"\\n\", \"\\n\", \"16\", \".\", \"05\", \".\", \"2014\", \"\\n\", \"\\n\", \"The\", \" Ministry\", \" of\", \" Administration\", \" and\", \" Dig\", \"it\", \"isation\", \",\", \" the\", \" National\", \" Broadcasting\", \" Council\", \" and\", \" the\", \" Office\", \" of\", \" Electronic\", \" Communications\", \" have\", \" received\", \" a\", \" Media\", \" T\", \"rends\", \" award\", \" in\", \" the\", \" category\", \" of\", \" Event\", \" of\", \" the\", \" Year\", \" on\", \" the\", \" media\", \" market\", \" for\", \" \\u201c\", \"the\", \" introduction\", \" of\", \" Poland\", \" to\", \" the\", \" group\", \" of\", \" countries\", \" which\", \" have\", \" completely\", \" switched\", \" off\", \" their\", \" analogue\", \" terrestrial\", \" signal\", \",\", \" thereby\", \" providing\", \" millions\", \" of\", \" people\", \" with\", \" the\", \" opportunity\", \" to\", \" receive\", \" the\", \" television\", \" of\", \" tomorrow\", \"\\u201d.\", \"\\n\", \"\\n\", \"The\", \" jury\", \" also\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"Friday\", \",\", \" February\", \" 08\", \",\", \" 2013\", \"\\n\", \"\\n\", \"\\\"\", \"Nothing\", \" in\", \" the\", \" world\", \" is\", \" more\", \" dangerous\", \" than\", \" sincere\", \" ignorance\", \" and\", \" conscient\", \"ious\", \" stupid\", \"ity\", \".\\\"\", \"\\n\", \"\\n\", \"The\", \" quote\", \" \\\"\", \"Nothing\", \" in\", \" the\", \" world\", \" is\", \" more\", \" dangerous\", \"...\\\"\", \" first\", \" appeared\", \" in\", \" Martin\", \" Luther\", \" King\", \" Jr\", \"'s\", \" speech\", \",\", \" \\\"\", \"St\", \"rength\", \" to\", \" Love\", \"\\\"\", \" which\", \" he\", \" gave\", \" in\", \" 1963\", \".\", \" 1963\", \" is\", \" also\"], [\"Q\", \":\", \"\\n\", \"\\n\", \"How\", \" to\", \" stream\", \" data\", \" that\", \" is\", \" generated\", \" by\", \" an\", \" IoT\", \" gateway\", \" application\", \" to\", \" pred\", \"ix\", \" cloud\", \"?\", \"\\n\", \"\\n\", \"I\", \"'ve\", \" devices\", \" configured\", \" with\", \" IoT\", \" gateway\", \" application\", \"(\", \"built\", \" with\", \" ME\", \"AN\", \".\", \"JS\", \"),\", \" these\", \" devices\", \" send\", \" data\", \" to\", \" IoT\", \" gateway\", \" where\", \" i\", \" can\", \" access\", \" data\", \" now\"], [\"Imagine\", \" watching\", \" the\", \" Empire\", \" State\", \" Building\", \" suddenly\"], [\"Rom\", \"ney\", \" Wins\", \" New\", \" Hampshire\", \" Primary\", \",\", \" Ron\", \" Paul\", \" Second\", \"\\n\", \"\\n\", \"Former\", \" Massachusetts\", \" Gov\", \".\", \" Mitt\", \" Romney\", \" has\", \" won\", \" the\", \" New\", \" Hampshire\", \" Republican\", \" primary\", \".\", \"\\n\", \"\\n\", \"Rom\", \"ney\", \" is\", \" the\", \" first\", \" Republican\", \" to\", \" win\", \" both\", \" the\", \" Iowa\", \" cauc\", \"uses\", \" and\", \" the\", \" New\", \" Hampshire\", \" primary\", \" in\", \" a\", \" competitive\", \" race\", \" since\", \" Iowa\", \" took\", \" the\", \" lead\", \"off\", \" role\", \" in\", \" 1976\", \".\", \"\\n\", \"\\n\", \"Rom\", \"ney\", \" is\", \" telling\", \" supporters\", \",\", \" \\\"\", \"Ton\", \"ight\", \" we\", \" celebrate\", \".\", \" Tomorrow\", \" we\", \" go\", \" back\", \" to\", \" work\", \".\\\"\", \"\\n\", \"\\n\", \"At\", \" a\", \" victory\", \" celebration\", \" following\", \" his\", \" win\", \" in\", \" the\", \" New\", \" Hampshire\", \" primary\", \",\", \" Romney\", \" said\", \" his\", \" campaign\", \" had\", \" \\\"\", \"made\", \" history\", \".\\\"\", \"\\n\", \"\\n\", \"Rom\", \"ney\", \" focused\", \" on\", \" President\", \" Barack\", \" Obama\", \" in\", \" his\", \" remarks\", \" to\", \" supporters\", \" --\", \" mentioning\", \" his\", \" own\", \" GOP\", \" rivals\", \" only\"], [\"It\", \" sounds\", \" like\", \" a\", \" scene\", \" out\", \" of\", \" Oliver\", \" Tw\", \"ist\", \" \\u2014\", \" except\", \" it\", \"'s\", \" happening\", \" in\", \" Pennsylvania\", \",\", \" in\", \" 2016\", \".\", \"\\n\", \"\\n\", \"Earlier\", \" this\", \" month\", \",\", \" Canon\", \"-\", \"Mc\", \"Mill\", \"an\", \" school\", \" district\", \" caf\", \"eter\", \"ia\", \" worker\", \" St\", \"acy\", \" K\", \"olt\", \"iska\", \" was\", \" ordered\", \" to\", \" take\", \" away\", \" a\", \" hot\", \" lunch\", \" she\", \" served\", \" to\", \" a\", \" hungry\", \" Grade\", \" One\", \" student\", \" because\", \" he\", \" couldn\", \"'t\", \" pay\", \" for\", \" it\", \".\", \"\\n\", \"\\n\", \"So\", \" she\", \" quit\", \" on\", \" the\", \" spot\", \".\", \"\\n\", \"\\n\", \"He\", \" just\", \" looked\", \" like\", \" a\", \" deer\", \" in\", \" head\", \"lights\", \",\", \" like\", \" '\", \"What\", \" is\", \" happening\", \" here\", \"?'\", \" And\", \" then\", \" his\", \" little\", \" eyes\", \" just\"], [\"You\", \" guys\", \" know\", \" I\", \"'ve\", \" been\", \" evangel\", \"izing\", \" 5\", \"G\", \" and\", \" 10\", \"G\", \"bit\", \"/\", \"s\", \" Ethernet\", \" to\", \" be\", \" embedded\", \" into\", \" mother\", \"boards\", \" for\", \" a\", \" while\", \" now\"], [\"Imagine\", \" watching\", \" the\", \" Empire\", \" State\", \" Building\", \" suddenly\", \" transform\", \" into\", \" giant\", \" spur\", \"ting\", \" penis\", \" to\", \" ej\", \"ac\", \"ulate\", \" a\", \" figure\", \" dressed\", \" in\", \" a\", \" major\", \" King\", \" Kong\", \" look\", \" across\", \" a\", \" stage\", \".\", \" Now\", \",\", \" imagine\", \" this\", \" mysterious\", \" figure\", \" shedding\", \" the\", \" a\", \"pe\", \" costume\", \" and\", \" emerging\", \" as\", \" the\", \" most\", \" fabulous\", \" Mar\", \"lene\", \" Diet\", \"rich\", \" you\", \"\\u2019\", \"ve\", \" ever\", \" seen\", \".\", \"\\n\", \"\\n\", \"No\", \",\", \" this\", \" isn\", \"\\u2019\", \"t\", \" the\", \" fever\", \" dream\", \" of\", \" a\", \" Hell\", \"\\u2019\", \"s\", \" Kitchen\", \" gay\", \" after\", \" watching\", \" Kong\", \":\", \" Sk\", \"ull\", \" Island\", \".\", \" This\", \" fl\", \"amb\", \"oy\", \"ant\", \" and\", \" provocative\", \" series\", \" of\", \" events\", \" almost\"], [\"The\", \" treatment\", \" of\", \" agitation\", \" during\", \" initial\", \" hospitalization\", \" after\", \" traumatic\", \" brain\", \" injury\", \".\", \"\\n\", \"Ag\", \"itation\", \" after\", \" traumatic\", \" brain\", \" injury\", \" is\", \" disruptive\", \" for\", \" patient\", \" care\", \",\", \" dist\", \"ressing\", \",\", \" and\", \" difficult\", \" to\", \" treat\", \".\", \" The\", \" use\", \" of\", \" pro\", \"pr\", \"anol\", \"ol\", \" has\", \" been\", \" advocated\", \" to\", \" control\", \" agitation\", \" after\", \" brain\", \" injury\", \".\", \" It\", \" reportedly\"], [\"\\n\", \"Ask\", \" HN\", \":\", \" How\", \" do\", \" you\", \" measure\", \" risk\", \" with\", \" an\", \" open\", \" source\", \" project\", \"?\", \" -\", \" b\", \"az\", \"M\", \"VP\", \"\\n\", \"If\", \" you\", \"&#\", \"x\", \"27\", \";\", \"ve\", \" found\", \" a\", \" new\", \" project\", \" on\", \" GitHub\", \" that\", \" you\", \" want\", \" to\", \" use\", \",\", \" how\", \" do\", \" you\", \" quantify\", \" the\", \" associated\", \" risk\", \" of\", \" using\", \" it\", \" in\", \" a\", \" &\", \"quot\", \";\", \"production\", \"&\", \"quot\", \";\", \" application\", \"?\", \" For\", \" example\", \":\", \" the\", \" project\", \" is\", \" only\"], [\"1\", \".\", \" Field\", \" of\", \" the\", \" Invention\", \"\\n\", \"This\", \" invention\", \" relates\", \" to\", \" patient\", \" transport\", \" systems\", \",\", \" and\", \" more\", \" particularly\", \",\", \" to\", \" a\", \" patient\", \" transport\", \" system\", \" for\", \" transferring\", \" an\", \" imm\", \"obile\", \" patient\", \" from\", \" a\", \" bed\", \" to\", \" a\", \" g\", \"ur\", \"ney\", \" or\", \" vice\", \" versa\", \".\", \"\\n\", \"2\", \".\", \" Description\", \" of\", \" the\", \" Prior\", \" Art\", \"\\n\", \"It\", \" appears\", \" to\", \" be\", \" widely\", \" accepted\", \" that\", \" a\", \" major\", \",\", \" if\", \" not\", \" the\", \" major\", \",\", \" work\", \"-\", \"related\", \" complaint\", \" among\", \" nurses\", \" and\", \" hospital\", \" nursing\", \" staff\", \" is\", \" back\", \" injuries\", \" caused\", \" by\", \" lifting\", \" patients\", \" and\", \" getting\", \" them\", \" in\", \" and\", \" out\", \" of\", \" a\", \" bed\", \" and\", \" to\", \" and\", \" from\", \" a\", \" g\", \"ur\", \"ney\", \" or\", \" a\", \" stret\", \"cher\", \" as\", \" it\", \" is\", \" commonly\"], [\"It\", \" sounds\", \" like\", \" a\", \" scene\", \" out\", \" of\", \" Oliver\", \" Tw\", \"ist\", \" \\u2014\", \" except\", \" it\", \"'s\", \" happening\", \" in\", \" Pennsylvania\", \",\", \" in\", \" 2016\", \".\", \"\\n\", \"\\n\", \"Earlier\", \" this\", \" month\", \",\", \" Canon\", \"-\", \"Mc\", \"Mill\", \"an\", \" school\", \" district\", \" caf\", \"eter\", \"ia\", \" worker\", \" St\", \"acy\", \" K\", \"olt\", \"iska\", \" was\", \" ordered\", \" to\", \" take\", \" away\", \" a\", \" hot\", \" lunch\", \" she\", \" served\", \" to\", \" a\", \" hungry\", \" Grade\", \" One\", \" student\", \" because\", \" he\", \" couldn\", \"'t\", \" pay\", \" for\", \" it\", \".\", \"\\n\", \"\\n\", \"So\", \" she\", \" quit\", \" on\", \" the\", \" spot\", \".\", \"\\n\", \"\\n\", \"He\", \" just\"], [\"The\", \" deposition\", \" of\", \" latex\", \" polymer\", \" coatings\", \" on\", \" solid\", \" substrates\", \" has\", \" long\", \" been\", \" utilized\", \" to\", \" impart\", \" certain\", \" end\", \"-\", \"use\", \" performance\", \" properties\", \" to\", \" those\", \" substrates\", \",\", \" such\", \" as\", \" hydroph\", \"ob\", \"icity\", \",\", \" strength\", \",\", \" adhesive\", \" properties\", \",\", \" compatibility\", \",\", \" and\", \" the\", \" like\", \".\", \" Depending\", \" upon\", \" the\", \" selection\", \" of\", \" the\", \" starting\", \" monomers\", \",\", \" surfact\", \"ants\", \",\", \" emulsion\", \" polymerization\", \" conditions\", \",\", \" and\", \" other\", \" parameters\", \",\", \" the\", \" deposited\", \" polymers\", \" can\", \" be\", \" designed\", \" to\", \" carry\", \" an\", \" an\", \"ionic\", \",\", \" a\", \" cationic\", \",\", \" or\", \" an\", \" am\", \"phot\", \"eric\", \" charge\", \",\", \" a\", \" feature\", \" which\", \" directly\", \" influences\", \" coating\", \" performance\", \".\", \" Further\", \",\", \" the\", \" resulting\", \" latex\", \" polymer\", \" can\", \" be\", \" blended\", \" with\", \" a\", \" range\", \" of\", \" other\", \" functional\", \" materials\", \" to\", \" impart\", \" additional\", \" or\", \" enhanced\", \" features\", \" to\", \" the\", \" final\", \" coating\", \" material\", \".\", \"\\n\", \"One\", \" particularly\"], [\"1\", \"..\", \" Introduction\", \"\\n\", \"================\", \"\\n\", \"\\n\", \"Tra\", \"umatic\", \" brain\", \" injury\", \" (\", \"T\", \"BI\", \")\", \" triggers\", \" a\", \" cascade\", \" of\", \" changes\", \" inducing\", \" the\", \" development\", \" of\", \" secondary\", \" brain\", \" damage\", \" with\", \" nitric\", \" oxide\", \" (\", \"NO\", \")\", \" playing\", \" an\", \" important\", \" role\", \".\", \" NO\", \" is\", \" a\", \" strong\", \" vas\", \"odil\", \"ator\", \" and\", \" regulates\", \" cerebro\", \"vascular\", \" tone\", \" and\", \" perfusion\", \" \\\\[[@\", \"b\", \"1\", \"-\", \"ijms\", \"-\", \"15\", \"-\", \"040\", \"88\", \"]\\\\].\", \" Therefore\", \",\", \" it\", \" is\", \" not\", \" surprising\", \" that\", \" NO\", \" is\", \" one\", \" of\", \" the\", \" key\", \" players\", \" in\", \" the\", \" development\", \" of\", \" cerebral\", \" vas\", \"osp\", \"asm\", \" after\", \" traumatic\", \" and\", \" aneurys\", \"mal\", \" sub\", \"ar\", \"ach\", \"n\", \"oid\", \" hemorrhage\", \" (\", \"SA\", \"H\", \")\", \" as\", \" well\"], [\"\\n\", \"Stream\", \" you\", \" terminal\", \" in\", \" real\", \"-\", \"time\", \" with\", \" anyone\", \" \\u2013\", \" without\", \" installing\", \" anything\", \" -\", \" sn\", \"oot\", \"y\", \"\\n\", \"https\", \"://\", \"stream\", \"h\", \"ut\", \".\", \"io\", \"/\", \"\\n\", \"======\", \"\\n\", \"nv\", \"ien\", \"not\", \"\\n\", \"Author\", \" of\", \" t\", \"mate\", \" here\", \".\", \" This\", \" is\", \" really\"], [\"Search\", \" form\", \"\\n\", \"\\n\", \"The\", \" Search\", \" for\", \" the\", \" Moon\", \"stone\", \"\\n\", \"\\n\", \"Un\", \"f\", \"ortunate\", \" advent\", \"urer\", \"!\", \" When\", \" your\", \" ship\", \" was\", \" wreck\", \"ed\", \" on\", \" the\", \" coral\", \" re\", \"efs\", \" of\", \" an\", \" un\", \"chart\", \"ed\", \" is\", \"le\", \",\", \" you\", \" knew\", \" that\", \" the\", \" curse\", \" of\", \" the\", \" legendary\", \" Moon\", \"stone\", \" had\", \" claimed\", \" another\", \" victim\", \".\", \"\\n\", \"\\n\", \"Of\", \" course\"], [\"Q\", \":\", \"\\n\", \"\\n\", \"Could\", \" I\", \" use\", \" just\"], [\"The\", \" deposition\", \" of\", \" latex\", \" polymer\", \" coatings\", \" on\", \" solid\", \" substrates\", \" has\", \" long\", \" been\", \" utilized\", \" to\", \" impart\", \" certain\", \" end\", \"-\", \"use\", \" performance\", \" properties\", \" to\", \" those\", \" substrates\", \",\", \" such\", \" as\", \" hydroph\", \"ob\", \"icity\", \",\", \" strength\", \",\", \" adhesive\", \" properties\", \",\", \" compatibility\", \",\", \" and\", \" the\", \" like\", \".\", \" Depending\", \" upon\", \" the\", \" selection\", \" of\", \" the\", \" starting\", \" monomers\", \",\", \" surfact\", \"ants\", \",\", \" emulsion\", \" polymerization\", \" conditions\", \",\", \" and\", \" other\", \" parameters\", \",\", \" the\", \" deposited\", \" polymers\", \" can\", \" be\", \" designed\", \" to\", \" carry\", \" an\", \" an\", \"ionic\", \",\", \" a\", \" cationic\", \",\", \" or\", \" an\", \" am\", \"phot\", \"eric\", \" charge\", \",\", \" a\", \" feature\", \" which\", \" directly\"], [\"Product\", \" Photography\", \":\", \" All\", \" in\", \" the\", \" details\", \"\\n\", \"\\n\", \"Not\", \" too\", \" long\", \" ago\", \" I\", \" was\", \" hired\", \" to\", \" shoot\", \" some\", \" product\", \" photos\", \" for\", \" a\", \" local\", \" tun\", \"er\", \",\", \" H\", \"PA\", \" Motors\", \"port\", \",\", \" their\", \" new\", \" K\", \"04\", \" Hybrid\", \" Tur\", \"bo\", \" upgrade\", \" kit\", \" for\", \" V\", \"W\", \" and\", \" Audi\", \" 2\", \".\", \"0\", \"T\", \"\\u2019\", \"s\", \".\", \"\\n\", \"\\n\", \"M\", \"arcel\", \" (\", \"owner\", \" of\", \" H\", \"PA\", \")\", \" had\", \" specifically\"], [\"The\", \" Weight\", \" of\", \" Sil\", \"ence\", \"by\", \" Heather\", \" G\", \"uden\", \"k\", \"auf\", \" is\", \" actually\"], [\"\\n\", \"Ask\", \" HN\", \":\", \" How\", \" do\", \" you\", \" measure\", \" risk\", \" with\", \" an\", \" open\", \" source\", \" project\", \"?\", \" -\", \" b\", \"az\", \"M\", \"VP\", \"\\n\", \"If\", \" you\", \"&#\", \"x\", \"27\", \";\", \"ve\", \" found\", \" a\", \" new\", \" project\", \" on\", \" GitHub\", \" that\", \" you\", \" want\", \" to\", \" use\", \",\", \" how\", \" do\", \" you\", \" quantify\", \" the\", \" associated\", \" risk\", \" of\", \" using\", \" it\", \" in\", \" a\", \" &\", \"quot\", \";\", \"production\", \"&\", \"quot\", \";\", \" application\", \"?\", \" For\", \" example\", \":\", \" the\", \" project\", \" is\", \" only\", \" open\", \" for\", \" 6\", \" months\", \",\", \" or\", \" has\", \" many\", \" more\", \" open\", \" issues\", \" vs\", \".\", \" closed\", \" issues\", \",\", \" or\", \" has\", \" negative\", \" sentiment\", \" in\", \" commit\", \" messages\", \".\", \" Of\", \" course\"], [\"Em\", \"otional\", \" enhancement\", \" of\", \" memory\", \" via\", \" amygdala\", \"-\", \"driven\", \" facil\", \"itation\", \" of\", \" rh\", \"inal\", \" interactions\", \".\", \"\\n\", \"Em\", \"otions\", \" generally\"], [\"Evidence\", \"-\", \"based\", \" education\", \" and\", \" nursing\", \" pressure\", \" ulcer\", \" prevention\", \" textbooks\", \":\", \" does\", \" it\", \" match\", \"?\", \"\\n\", \"The\", \" education\", \" of\", \" nurses\", \" has\", \" influenced\", \" the\", \" way\", \" of\", \" nursing\", \" practice\", \" for\", \" a\", \" long\", \" time\", \".\", \" Nurse\", \" educators\", \" are\", \" required\", \" to\", \" offer\", \" up\", \"-\", \"to\", \"-\", \"date\", \" educational\", \" material\", \",\", \" and\", \" textbooks\", \" are\", \" the\", \" most\", \" frequently\"], [\"Besides\", \" Pur\", \"kin\", \"je\", \" cells\", \" and\", \" gran\", \"ule\", \" neurons\", \":\", \" an\", \" appraisal\", \" of\", \" the\", \" cell\", \" biology\", \" of\", \" the\", \" inter\", \"neur\", \"ons\", \" of\", \" the\", \" cerebellar\", \" cortex\", \".\", \"\\n\", \"Ever\", \" since\", \" the\", \" ground\", \"breaking\", \" work\", \" of\", \" Ram\", \"on\", \" y\", \" C\", \"aj\", \"al\", \",\", \" the\", \" cerebellar\", \" cortex\", \" has\", \" been\", \" recognized\", \" as\", \" one\", \" of\", \" the\", \" most\", \" regularly\", \" structured\", \" and\", \" wired\", \" parts\", \" of\", \" the\", \" brain\", \" formed\", \" by\", \" a\", \" rather\", \" limited\", \" set\", \" of\", \" distinct\", \" cells\", \".\", \" Its\", \" rather\", \" prot\", \"racted\", \" course\", \" of\", \" development\", \",\", \" which\", \" persists\", \" well\", \" into\", \" postnatal\", \" life\", \",\", \" the\", \" availability\", \" of\", \" multiple\", \" natural\", \" mutants\", \",\", \" and\", \",\", \" more\", \" recently\"], [\"XXX\", \"t\", \"ent\", \"acion\", \"\\u2018\", \"s\", \" ex\", \"-\", \"girl\", \"friend\", \" has\", \" received\", \" thousands\", \" of\", \" dollars\", \" for\", \" her\", \" medical\", \" bills\", \" following\", \" the\", \" rapper\", \"\\u2019\", \"s\", \" death\", \".\", \"\\n\", \"\\n\", \"Gen\", \"eva\", \" Ay\", \"ala\", \" accused\", \" XX\", \"t\", \"ent\", \"acion\", \" \\u2013\", \" real\", \" name\", \" Jah\", \"se\", \"h\", \" D\", \"way\", \"ne\", \" On\", \"f\", \"roy\", \" \\u2013\", \" of\", \" abuse\", \".\", \" The\", \" South\", \" Florida\", \" artist\", \" was\", \" arrested\", \" in\", \" 2016\", \",\", \" facing\", \" charges\", \" of\", \" aggravated\", \" battery\", \" on\", \" a\", \" pregnant\", \" victim\", \",\", \" domestic\", \" battery\", \" by\", \" strang\", \"ulation\", \",\", \" false\", \" imprisonment\", \",\", \" and\", \" witness\", \" tam\", \"pering\", \" on\", \" Ay\", \"ala\", \".\", \"\\n\", \"\\n\", \"Ay\", \"ala\", \" created\", \" a\", \" Go\", \" Fund\", \" Me\", \" page\", \" in\", \" October\", \" 2016\", \" to\", \" help\", \" pay\", \" for\", \" her\", \" surgery\", \" following\", \" the\", \" alleged\", \" battery\", \".\", \" She\", \" detailed\", \" two\", \" fractures\", \" in\", \" her\", \" left\", \" eye\", \" socket\", \" as\", \" well\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"The\", \" turt\", \"les\", \" were\", \" originally\"], [\"Evidence\", \"-\", \"based\", \" education\", \" and\", \" nursing\", \" pressure\", \" ulcer\", \" prevention\", \" textbooks\", \":\", \" does\", \" it\", \" match\", \"?\", \"\\n\", \"The\", \" education\", \" of\", \" nurses\", \" has\", \" influenced\", \" the\", \" way\", \" of\", \" nursing\", \" practice\", \" for\", \" a\", \" long\", \" time\", \".\", \" Nurse\", \" educators\", \" are\", \" required\", \" to\", \" offer\", \" up\", \"-\", \"to\", \"-\", \"date\", \" educational\", \" material\", \",\", \" and\", \" textbooks\", \" are\", \" the\", \" most\", \" frequently\", \" used\", \" sources\", \" of\", \" knowledge\", \" during\", \" a\", \" nurse\", \"'s\", \" education\", \".\", \" In\", \" this\", \" study\", \",\", \" researchers\", \" investigated\", \" the\", \" extent\", \" to\", \" which\", \" textbooks\", \" were\", \" evidence\", \" based\", \" regarding\", \" preventing\", \" pressure\", \" ulcers\", \" and\", \" knowing\", \" what\", \" recommendations\", \" to\", \" make\", \" for\", \" nursing\", \" students\", \" and\", \" publishers\", \" of\", \" nursing\", \" textbooks\", \".\", \" Educ\", \"ators\", \" at\", \" nursing\", \" schools\", \" in\", \" Germany\", \" were\", \" contacted\", \" by\", \" telephone\", \" to\", \" identify\", \" the\", \" most\", \" often\"], [\"Besides\", \" Pur\", \"kin\", \"je\", \" cells\", \" and\", \" gran\", \"ule\", \" neurons\", \":\", \" an\", \" appraisal\", \" of\", \" the\", \" cell\", \" biology\", \" of\", \" the\", \" inter\", \"neur\", \"ons\", \" of\", \" the\", \" cerebellar\", \" cortex\", \".\", \"\\n\", \"Ever\", \" since\", \" the\", \" ground\", \"breaking\", \" work\", \" of\", \" Ram\", \"on\", \" y\", \" C\", \"aj\", \"al\", \",\", \" the\", \" cerebellar\", \" cortex\", \" has\", \" been\", \" recognized\", \" as\", \" one\", \" of\", \" the\", \" most\", \" regularly\", \" structured\", \" and\", \" wired\", \" parts\", \" of\", \" the\", \" brain\", \" formed\", \" by\", \" a\", \" rather\"]], \"activations\": [[[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.0133044719696045]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.7759755849838257]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.714420199394226]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.3374223709106445]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.027970552444458]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.013509750366211]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.9757603406906128]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.6137932538986206]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.9147084355354309]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.8979464769363403]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.013509750366211]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.031990982592105865]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.8248355388641357]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7216745615005493]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.718329906463623]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.7759755849838257]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7017878890037537]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.6137932538986206]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.4337156414985657]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.5915736556053162]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.4833393096923828]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.45382004976272583]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.44495201110839844]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.4380696415901184]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.4337156414985657]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.41053617000579834]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.3850829005241394]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.718329906463623]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.36115479469299316]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.3584716320037842]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.30950748920440674]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.21968917548656464]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.019924938678741455]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.3032737374305725]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.29493504762649536]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.2916690707206726]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.30950748920440674]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.267059326171875]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.21968917548656464]]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f59dc452fa0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from circuitsvis.activations import text_neuron_activations\n",
    "\n",
    "tokens = [\n",
    "    inputs['input_ids'][batch_idx, :token_idx+1].tolist() for batch_idx, token_idx in zip(batch_indices, token_indices)\n",
    "]\n",
    "tokens = list_decode(tokens)\n",
    "activations = [\n",
    "    acts[batch_idx, :token_id+1, None, None] for batch_idx, token_id in zip(batch_indices, token_indices)\n",
    "]\n",
    "text_neuron_activations(tokens, activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([40, 40, 40, 40, 40, 40, 40, 40, 15,  7,  7,  1, 15, 15, 15, 40, 15, 15,\n",
       "        15,  7, 53, 53, 15, 15,  7, 15, 15, 46,  1, 15])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "972 P.2d 566 (1999)\n",
      "CITY OF TACOMA, a municipal corporation, Appellant,\n",
      "v.\n",
      "FRANCISCAN FOUNDATION, aka St. Joseph Hospital And Healthcare Center, a Washington nonprofit corporation, Respondent.\n",
      "No. 23107-7-II.\n",
      "Court of Appeals of Washington, Division 2.\n",
      "March 5, 1999.\n",
      "*567 Jean P Homan, Assistant City Attorney, Tacoma, for Appellant.\n",
      "Linda Gayle White Atkins, Davis Wright Tremaine, Bellevue, for Respondent.\n",
      "HOUGHTON, J.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.tokenizer.decode(inputs[40].ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/share/u/smarks/dictionary-circuits/testing.ipynb Cell 13\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.200.205.162/share/u/smarks/dictionary-circuits/testing.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m found_indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margsort(dictionary_activations, descending\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[:k]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.200.205.162/share/u/smarks/dictionary-circuits/testing.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m num_datapoints \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(dictionary_activations\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m/\u001b[39m\u001b[39m128\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.200.205.162/share/u/smarks/dictionary-circuits/testing.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m datapoint_indices \u001b[39m=\u001b[39m[np\u001b[39m.\u001b[39munravel_index(i, (\u001b[39m64\u001b[39m, \u001b[39m128\u001b[39m)) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m found_indices]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.200.205.162/share/u/smarks/dictionary-circuits/testing.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m text_list \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.200.205.162/share/u/smarks/dictionary-circuits/testing.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m full_text \u001b[39m=\u001b[39m []\n",
      "\u001b[1;32m/share/u/smarks/dictionary-circuits/testing.ipynb Cell 13\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.200.205.162/share/u/smarks/dictionary-circuits/testing.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m found_indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margsort(dictionary_activations, descending\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[:k]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.200.205.162/share/u/smarks/dictionary-circuits/testing.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m num_datapoints \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(dictionary_activations\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m/\u001b[39m\u001b[39m128\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.200.205.162/share/u/smarks/dictionary-circuits/testing.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m datapoint_indices \u001b[39m=\u001b[39m[np\u001b[39m.\u001b[39;49munravel_index(i, (\u001b[39m64\u001b[39;49m, \u001b[39m128\u001b[39;49m)) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m found_indices]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.200.205.162/share/u/smarks/dictionary-circuits/testing.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m text_list \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.200.205.162/share/u/smarks/dictionary-circuits/testing.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m full_text \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36munravel_index\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:1030\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[39m.\u001b[39m__array__, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m   1029\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1030\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnumpy()\n\u001b[1;32m   1031\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1032\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "k = 10\n",
    "found_indices = torch.argsort(dictionary_activations, descending=True)[:k]\n",
    "num_datapoints = int(dictionary_activations.shape[0]/128)\n",
    "datapoint_indices =[np.unravel_index(i, (64, 128)) for i in found_indices]\n",
    "text_list = []\n",
    "full_text = []\n",
    "token_list = []\n",
    "full_token_list = []\n",
    "for md, s_ind in datapoint_indices:\n",
    "    md = int(md)\n",
    "    s_ind = int(s_ind)\n",
    "    full_tok = torch.tensor(dataset[md][\"input_ids\"])\n",
    "    full_text.append(tokenizer.decode(full_tok))\n",
    "    tok = dataset[md][\"input_ids\"][:s_ind+1]\n",
    "    text = tokenizer.decode(tok)\n",
    "    text_list.append(text)\n",
    "    token_list.append(tok)\n",
    "    full_token_list.append(full_tok)\n",
    "text_list, full_text, token_list, full_token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can use the model to get the activations\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "import torch \n",
    "# num_features, d_model = autoencoder.encoder.shape # Fix this for shape purposes\n",
    "texts = buffer.text_batch()\n",
    "datapoints = len(texts)\n",
    "batch_size = 64\n",
    "neuron_activations = torch.zeros((datapoints*max_length, d_model))\n",
    "dictionary_activations = torch.zeros((datapoints*max_length))\n",
    "\n",
    "with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "    dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "    for i, batch in enumerate(tqdm(dl)):\n",
    "        # Replace this with your residual stream stuff\n",
    "        # _, cache = model.run_with_cache(batch.to(device))\n",
    "        # batched_neuron_activations = rearrange(cache[cache_name], \"b s n -> (b s) n\" )\n",
    "\n",
    "        # Replace with your projection to probe direction\n",
    "        # batched_dictionary_activations = smaller_auto_encoder.encode(batched_neuron_activations)\n",
    "        dictionary_activations[i*batch_size*max_length:(i+1)*batch_size*max_length] = batched_dictionary_activations.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "def entropy(p):\n",
    "    p = p/p.sum(dim=-1, keepdim=True)\n",
    "    log_p = p.log().nan_to_num()\n",
    "    entropies = -(p * log_p).sum(dim=-1)\n",
    "    out = entropies.nan_to_num().mean()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4778)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.Tensor([[0, 0,0], [1, 4, 2]])\n",
    "entropy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [1., 4., 2.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   nan,    nan,    nan],\n",
       "        [0.0000, 5.5452, 1.3863]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x * x.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0 * float(\"-inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 0.6302395462989807\n",
      "Gradients: tensor([[ 0.5003,  0.2692, -0.1484,  6.2063,  6.2063],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0384,  0.0384, -0.0575,  6.1106,  6.1106]])\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "\n",
    "def entropy(p):\n",
    "    eps = 1e-8\n",
    "    # Calculate the sum along the last dimension (i.e., sum of each vector in the batch)\n",
    "    p_sum = p.sum(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Avoid in-place operations that can interfere with autograd\n",
    "    p_normed = p / (p_sum + eps)  # Add eps to prevent division by zero\n",
    "    \n",
    "    # Compute the log safely, adding eps inside the log to prevent log(0)\n",
    "    p_log = t.log(p_normed + eps)  # Add eps to prevent log(0)\n",
    "\n",
    "    # Compute the entropy, this will give zero for elements where p_normed is zero\n",
    "    ent = -(p_normed * p_log)\n",
    "    \n",
    "    # Zero out the entropy where the sum of p is zero (i.e., for all-zero vectors)\n",
    "    ent = t.where(p_sum > 0, ent, t.zeros_like(ent))\n",
    "\n",
    "    # Sum the entropy across the features and then take the mean across the batch\n",
    "    return ent.sum(dim=-1).mean()\n",
    "\n",
    "# Example usage:\n",
    "batch_size = 3\n",
    "vector_length = 5\n",
    "p = t.tensor([[0.1, 0.2, 0.7, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0],  # All-zero vector\n",
    "              [0.3, 0.3, 0.4, 0.0, 0.0]], requires_grad=True)\n",
    "\n",
    "entropy_value = entropy(p)\n",
    "entropy_value.backward()\n",
    "\n",
    "print(\"Entropy:\", entropy_value.item())\n",
    "print(\"Gradients:\", p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
