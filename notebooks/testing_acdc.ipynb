{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing patching functions in acdc.py\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/can/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import zstandard as zstd\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from einops import rearrange\n",
    "import torch as t\n",
    "from nnsight import LanguageModel\n",
    "from dictionary_learning.buffer import ActivationBuffer\n",
    "from dictionary_learning.dictionary import AutoEncoder\n",
    "from dictionary_learning.training import trainSAE\n",
    "from acdc import patching_on_y, patching_on_downstream_feature\n",
    "from loading_utils import DictionaryCfg\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import imageio\n",
    "from circuitsvis.activations import text_neuron_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "ACTIVATION_DIM = 512\n",
    "DICTIONARY_SIZE = 64 * ACTIVATION_DIM # This is the dict_size sam mostly works with.\n",
    "EPS = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Model and SAEs\n",
    "model = LanguageModel('EleutherAI/pythia-70m-deduped', device_map='cuda:0')\n",
    "submodules = [\n",
    "    layer.mlp for layer in model.gpt_neox.layers\n",
    "]\n",
    "submodule_name_generic = 'model.gpt_neox.layers.{}.mlp.dense_4h_to_h'\n",
    "submodule_names = [submodule_name_generic.format(str(layer)) for layer in range(model.config.num_hidden_layers)]\n",
    "\n",
    "dictionaries = []\n",
    "for i in range(len(submodules)):\n",
    "    dictionary = AutoEncoder(ACTIVATION_DIM, DICTIONARY_SIZE).to('cuda:0')\n",
    "    dictionary.load_state_dict(t.load(f'/share/projects/dictionary_circuits/autoencoders/pythia-70m-deduped/mlp_out_layer{i}/1_32768/ae.pt'))\n",
    "    dictionaries.append(dictionary)\n",
    "\n",
    "dict_cfg = DictionaryCfg(\n",
    "    dictionary_dir='/share/projects/dictionary_circuits/autoencoders/pythia-70m-deduped/',\n",
    "    dictionary_size=32768\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([510, 637])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Toy dataset for the \"Plural\" task\n",
    "\n",
    "plural_token_pos = 1\n",
    "tok = lambda x: t.tensor(model.tokenizer.encode(x))\n",
    "toy_dataset = [dict(\n",
    "    clean_prefix=tok(\"The man\"),\n",
    "    clean_answer=tok(\" is\"),\n",
    "    patch_prefix=tok(\"The men\"),\n",
    "    patch_answer=tok(\" are\"),\n",
    "\n",
    ")]\n",
    "\n",
    "toy_dataset[0]['clean_prefix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diverse dataset for Circuitvis evaluation\n",
    "\n",
    "# set up data as a generator\n",
    "data_path = '/share/data/datasets/pile/the-eye.eu/public/AI/pile/train/00.jsonl.zst'\n",
    "compressed_file = open(data_path, 'rb')\n",
    "dctx = zstd.ZstdDecompressor()\n",
    "reader = dctx.stream_reader(compressed_file)\n",
    "text_stream = io.TextIOWrapper(reader, encoding='utf-8')\n",
    "def generator():\n",
    "    for line in text_stream:\n",
    "        yield json.loads(line)['text']\n",
    "data = generator()\n",
    "\n",
    "# Buffer tied to one specific submodule!\n",
    "buffer = ActivationBuffer(\n",
    "    data,\n",
    "    model,\n",
    "    submodules[0], # we only use data from the buffer, not acttivations. Thus we can pass any submodule here\n",
    "    io='out',\n",
    "    in_feats=ACTIVATION_DIM,\n",
    "    out_feats=ACTIVATION_DIM,\n",
    "    in_batch_size=512,\n",
    "    out_batch_size=2 ** 15,\n",
    "    n_ctxs=1e4,\n",
    ")\n",
    "\n",
    "tokenized_prompts = buffer.tokenized_batch(batch_size=512) # [batch, n_ctx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Circuitsvis visualization\n",
    "\n",
    "def get_feature_activations(layer):\n",
    "    '''\n",
    "    Function for retrieving feature activations from tokenized batch\n",
    "    returns activations for all submodules, dictionaries if layer == None\n",
    "    \n",
    "    '''\n",
    "    with model.generate(max_new_tokens=1, pad_token_id=model.tokenizer.pad_token_id) as generator:\n",
    "        with generator.invoke(tokenized_prompts['input_ids'], scan=False) as invoker:\n",
    "            hidden_states = submodules[layer].output.save() # hidden_states.value: [batch, n_ctx, d_mlp]\n",
    "    dictionary_activation = dictionaries[layer].encode(hidden_states.value)\n",
    "    return dictionary_activation\n",
    "\n",
    "def list_decode(x):\n",
    "    if isinstance(x, int):\n",
    "        return model.tokenizer.decode(x)\n",
    "    else:\n",
    "        return [list_decode(y) for y in x]\n",
    "    \n",
    "def topk_prompts_provider(feature_layer: int, feature_id: int, k: int = 30):\n",
    "    # Sort examples in batch by max feature activation\n",
    "    dictionary_activations = get_feature_activations(feature_layer)\n",
    "    acts = dictionary_activations[:, :, feature_id].cpu() # acts: [batch, pos]\n",
    "    flattened_acts = rearrange(acts, 'b l -> (b l)')\n",
    "    topk_indices = t.argsort(flattened_acts, dim=0, descending=True)[:k] \n",
    "    batch_indices = topk_indices // acts.shape[1]\n",
    "    token_indices = topk_indices % acts.shape[1]\n",
    "\n",
    "    # Visualize\n",
    "    tokens = [\n",
    "    tokenized_prompts['input_ids'][batch_idx, :token_idx+1].tolist() for batch_idx, token_idx in zip(batch_indices, token_indices)\n",
    "    ]\n",
    "    tokens = list_decode(tokens)\n",
    "    activations = [\n",
    "        acts[batch_idx, :token_id+1, None, None] for batch_idx, token_id in zip(batch_indices, token_indices)\n",
    "    ]\n",
    "    return tokens, activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing `patching_on_y`\n",
    "Evaluate causal effects of features in layer 5 on the logit diff between clean and patch answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_patching_on_y = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded submod_type='mlp'submod_layer=0\n",
      "loaded submod_type='mlp'submod_layer=1\n",
      "loaded submod_type='mlp'submod_layer=2\n",
      "loaded submod_type='mlp'submod_layer=3\n",
      "loaded submod_type='mlp'submod_layer=4\n",
      "loaded submod_type='mlp'submod_layer=5\n",
      "loaded submod_type='mlp'submod_layer=0\n",
      "loaded submod_type='mlp'submod_layer=1\n",
      "loaded submod_type='mlp'submod_layer=2\n",
      "loaded submod_type='mlp'submod_layer=3\n",
      "loaded submod_type='mlp'submod_layer=4\n",
      "loaded submod_type='mlp'submod_layer=5\n",
      "total metric diff after replacing clean prefix with patch prefix: -1.527494192123413\n"
     ]
    }
   ],
   "source": [
    "effects_on_y, total_effect_on_y = patching_on_y(\n",
    "    toy_dataset,\n",
    "    model,\n",
    "    upstream_submodule_names=submodule_names,\n",
    "    dict_cfg=dict_cfg,\n",
    "    method='all-folded'\n",
    ")\n",
    "\n",
    "print(f'total metric diff after replacing clean prefix with patch prefix: {total_effect_on_y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 features in layer 5 with highest impact on logit diff\n",
      "feat 22606:\t 0.005298633594065905\n",
      "feat 27530:\t 0.005288595799356699\n",
      "feat 6571:\t 0.005187406204640865\n",
      "feat 24750:\t 0.0051690055988729\n",
      "feat 23543:\t 0.0038971665780991316\n",
      "feat 16556:\t 0.00376796443015337\n",
      "feat 29434:\t 0.0036463357973843813\n",
      "feat 8810:\t 0.003528831759467721\n",
      "feat 32034:\t 0.0030332545284181833\n",
      "feat 8011:\t 0.0018159766914322972\n"
     ]
    }
   ],
   "source": [
    "# Top 10 features in layer `layer_patching_on_y` with highest impact on logit diff\n",
    "\n",
    "# Only effect on token position `plural_token_pos` matters for \"Plurals\" task\n",
    "effects_on_y = effects_on_y[submodule_names[layer_patching_on_y]][plural_token_pos].detach().cpu()\n",
    "top_effects_on_y = t.argsort(effects_on_y, descending=True)[:10]\n",
    "top_features_on_y = []\n",
    "print(f'Top 10 features in layer {layer_patching_on_y} with highest impact on logit diff')\n",
    "for i in top_effects_on_y:\n",
    "    top_features_on_y.append(i)\n",
    "    print(f'feat {i}:\\t {effects_on_y[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(effects_on_y[effects_on_y.abs() > EPS], bins=100)\n",
    "plt.xscale('log')\n",
    "plt.title(f'Feature effect on logit diff distribution\\nMLP={layer_patching_on_y}');\n",
    "plt.xlabel('Mean feature activation')\n",
    "plt.ylabel('Frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-19f0c60f-8a99\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-19f0c60f-8a99\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [[\"S\", \"ter\", \"oid\", \" hormone\", \" modulation\", \" of\", \" olfactory\", \" processing\", \" in\", \" the\", \" context\", \" of\", \" socio\", \"-\", \"sexual\", \" behaviors\", \" in\", \" rodents\", \" and\", \" humans\", \".\", \"\\n\", \"Prim\", \"er\", \" phe\", \"rom\", \"ones\", \" and\", \" other\", \" chem\", \"os\", \"ensory\", \" cues\", \" are\", \" important\", \" factors\", \" governing\", \" social\", \" interactions\", \" and\", \" reproductive\", \" physiology\", \" in\", \" many\", \" species\", \" of\", \" mammals\", \".\", \" Respons\", \"es\"], [\"Three\", \"-\", \"dimensional\", \" structures\", \" of\", \" H\", \"-\", \"ras\", \" p\", \"21\", \" mutants\", \":\", \" molecular\", \" basis\", \" for\", \" their\", \" inability\", \" to\", \" function\", \" as\", \" signal\", \" switch\", \" molecules\", \".\", \"\\n\", \"The\", \" X\", \"-\", \"ray\", \" structures\", \" of\", \" the\", \" gu\", \"anine\", \" nucleotide\", \" binding\", \" domains\", \" (\", \"amino\", \" acids\", \" 1\", \"-\", \"166\", \")\", \" of\", \" five\", \" mutants\", \" of\", \" the\", \" H\", \"-\", \"ras\", \" onc\", \"ogene\", \" product\", \" p\", \"21\", \" were\", \" determined\", \".\", \" The\", \" mutations\", \" described\", \" are\", \" Gly\", \"-\", \"12\", \"----\", \"Arg\", \",\", \" Gly\", \"-\", \"12\", \"----\", \"Val\", \",\", \" Gl\", \"n\", \"-\", \"61\", \"----\", \"His\", \",\", \" Gl\", \"n\", \"-\", \"61\", \"----\", \"Le\", \"u\", \",\", \" which\", \" are\", \" all\", \" onc\", \"ogenic\", \",\", \" and\", \" the\", \" effector\", \" region\", \" mutant\", \" Asp\", \"-\", \"38\", \"----\", \"Glu\", \".\", \" The\", \" resolutions\"], [\"Effect\", \" of\", \" two\", \" prophylaxis\", \" methods\", \" on\", \" adherence\", \" of\", \" St\", \"reptococcus\", \" mut\", \"ans\", \" to\", \" micro\", \"filled\", \" composite\", \" resin\", \" and\", \" gi\", \"omer\", \" surfaces\", \".\", \"\\n\", \"Surface\", \" attributes\"], [\"Ab\", \"ility\", \" of\", \" MR\", \" chol\", \"angi\", \"ography\", \" to\", \" reveal\", \" stent\", \" position\", \" and\", \" luminal\", \" diameter\", \" in\", \" patients\", \" with\", \" biliary\", \" end\", \"op\", \"rost\", \"heses\", \":\", \" in\", \" vitro\", \" measurements\", \" and\", \" in\", \" vivo\", \" results\", \" in\", \" 30\", \" patients\", \".\", \"\\n\", \"Our\", \" goal\", \" was\", \" to\", \" evaluate\", \" the\", \" ability\", \" of\", \" MR\", \" chol\", \"angi\", \"ography\", \" to\", \" show\", \" stent\", \" position\", \" and\", \" luminal\", \" diameter\", \" in\", \" patients\", \" with\", \" biliary\", \" end\", \"op\", \"rost\", \"heses\", \".\", \" Sus\", \"cept\", \"ibility\", \" artifacts\"], [\"Three\", \"-\", \"dimensional\", \" structures\", \" of\", \" H\", \"-\", \"ras\", \" p\", \"21\", \" mutants\", \":\", \" molecular\", \" basis\", \" for\", \" their\", \" inability\", \" to\", \" function\", \" as\", \" signal\", \" switch\", \" molecules\", \".\", \"\\n\", \"The\", \" X\", \"-\", \"ray\", \" structures\"]], \"activations\": [[[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.075770854949951]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.164558410644531]]], [[[0.0]], [[0.0]], [[0.0]], [[2.0260353088378906]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.14972615242004395]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[3.9559545516967773]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.8739017248153687]], [[0.0]], [[0.0]], [[0.5592746734619141]], [[0.0]], [[0.0]], [[0.0]], [[0.20697367191314697]], [[0.0]], [[0.0]], [[0.15469741821289062]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[3.2769317626953125]], [[0.8118845224380493]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.26222193241119385]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.457262992858887]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.25761604309082]]], [[[0.0]], [[0.4871096611022949]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.235532760620117]], [[0.0]], [[0.0]], [[0.0]], [[0.4282470941543579]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.3315916061401367]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.08666300773620605]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.3164713382720947]], [[3.989047050476074]]], [[[0.0]], [[0.0]], [[0.0]], [[2.0260353088378906]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.14972615242004395]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[3.9559545516967773]]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f68ee7d6ee0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Need to stack functions to display circuitvis properly\n",
    "text_neuron_activations(\n",
    "    *topk_prompts_provider(\n",
    "        feature_layer=layer_patching_on_y, \n",
    "        feature_id=top_features_on_y[1],\n",
    "        k=5\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5/22167 seems to activate on singular nouns related do people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing `patching_on_downstream_feature`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "downstream_feat_layer = 5\n",
    "downstream_feat_id = 22167\n",
    "\n",
    "k_upstream_feats = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded submod_type='mlp'submod_layer=5\n",
      "loaded submod_type='mlp'submod_layer=0\n",
      "loaded submod_type='mlp'submod_layer=1\n",
      "loaded submod_type='mlp'submod_layer=2\n",
      "loaded submod_type='mlp'submod_layer=3\n",
      "loaded submod_type='mlp'submod_layer=4\n",
      "loaded submod_type='mlp'submod_layer=0\n",
      "loaded submod_type='mlp'submod_layer=1\n",
      "loaded submod_type='mlp'submod_layer=2\n",
      "loaded submod_type='mlp'submod_layer=3\n",
      "loaded submod_type='mlp'submod_layer=4\n",
      "loaded submod_type='mlp'submod_layer=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  5.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded submod_type='mlp'submod_layer=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:04<00:00,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded submod_type='mlp'submod_layer=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:02<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded submod_type='mlp'submod_layer=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:09<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded submod_type='mlp'submod_layer=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:02<00:00,  6.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total metric diff after replacing clean prefix with patch prefix: -2.3897130489349365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "effects_on_dsfeat, total_effect_on_dsfeat = patching_on_downstream_feature(\n",
    "    toy_dataset,\n",
    "    model,\n",
    "    upstream_submodule_names=submodule_names[:downstream_feat_layer],\n",
    "    downstream_submodule_name=submodule_names[downstream_feat_layer],\n",
    "    downstream_feature_id=downstream_feat_id,\n",
    "    dict_cfg=dict_cfg,\n",
    "    method='exact'\n",
    ")\n",
    "\n",
    "print(f'total metric diff after replacing clean prefix with patch prefix: {total_effect_on_dsfeat}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 features per layer with highest impact on downstream feat activation\n",
      "\n",
      "Layer 0:\n",
      "feat 21840:\t 0.0\n",
      "feat 21855:\t 0.0\n",
      "feat 21854:\t 0.0\n",
      "\n",
      "Layer 1:\n",
      "feat 14400:\t 0.030630111694335938\n",
      "feat 25045:\t 0.009372711181640625\n",
      "feat 10964:\t 0.006834506988525391\n",
      "\n",
      "Layer 2:\n",
      "feat 12654:\t 0.029448509216308594\n",
      "feat 27186:\t 0.018366336822509766\n",
      "feat 13579:\t 0.009025096893310547\n",
      "\n",
      "Layer 3:\n",
      "feat 26928:\t 0.03757333755493164\n",
      "feat 20945:\t 0.03383207321166992\n",
      "feat 17355:\t 0.03262805938720703\n",
      "\n",
      "Layer 4:\n",
      "feat 2871:\t 0.12388277053833008\n",
      "feat 11028:\t 0.004107952117919922\n",
      "feat 9616:\t 0.0036187171936035156\n"
     ]
    }
   ],
   "source": [
    "# Top k features per layer with highest impact on downstream feat activation\n",
    "\n",
    "topk_upstream_feats = t.zeros((len(submodules), k_upstream_feats), dtype=int)\n",
    "print(f'Top {k_upstream_feats} features per layer with highest impact on downstream feat activation')\n",
    "\n",
    "for layer in range(len(submodules)-1):\n",
    "    print(f'\\nLayer {layer}:')\n",
    "    effects_on_dsfeat_per_layer = effects_on_dsfeat[submodule_names[layer]][plural_token_pos].detach().cpu() # Only effect on token position `plural_token_pos` matters for \"Plurals\" task\n",
    "    topk_upstream_feats[layer] = t.argsort(effects_on_dsfeat_per_layer, descending=True)[:k_upstream_feats]\n",
    "    for i in topk_upstream_feats[layer]:\n",
    "        print(f'feat {i}:\\t {effects_on_dsfeat_per_layer[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 1 features per layer found seem to be related to the plurals task!\n",
    "\n",
    "Values in same or later layer are generally uninterpretable.\n",
    "I expect that, xact patching should yield 0 unconnected features in same or later layer. (No bwdpass involved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-a0f3f47b-235c\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-a0f3f47b-235c\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [[\"Ben\", \"jamin\", \" Lok\", \" describes\", \" how\", \" the\", \" robot\", \" butt\", \" sensors\", \" work\", \" 2\", \":\", \"26\", \"\\n\", \"\\n\", \"Pro\", \"state\", \" exams\", \" are\", \" potentially\", \"-\", \"life\", \" saving\", \".\", \" But\", \" the\", \" process\", \" of\", \" getting\", \" one\", \" can\", \" be\", \" nerve\", \"-\", \"r\", \"acking\", \" \\u2014\", \" both\", \" for\", \" the\", \" doctor\", \" and\", \" the\", \" patient\", \".\", \"\\n\", \"\\n\", \"A\", \" group\", \" of\", \" scientists\", \" from\", \" D\", \"rex\", \"el\", \" University\", \" and\", \" the\", \" Univers\", \"ities\", \" of\", \" Wisconsin\", \" and\", \" Florida\", \" are\", \" hoping\", \" to\", \" assist\", \" with\", \" that\", \".\", \" They\", \"'ve\", \" designed\", \" a\", \" robot\", \" to\", \" help\", \" medical\", \" students\"], [\"Common\", \"wealth\", \" Bank\", \" and\", \" the\", \" Australian\", \" Chamber\", \" Orchestra\", \" kick\", \" off\", \" the\", \" 2009\", \" Great\", \" Rom\", \"antics\", \" national\", \" tour\", \"\\n\", \"\\n\", \"S\", \"yd\", \"ney\", \",\", \" 11\", \" June\", \" 2009\", \":\", \" The\", \" Commonwealth\", \" Bank\", \" today\", \" congrat\", \"ulated\", \" the\", \" Australian\", \" Chamber\", \" Orchestra\", \" (\", \"AC\", \"O\", \")\", \" on\", \" the\", \" commencement\", \" of\", \" its\", \" Great\", \" Rom\", \"antics\", \" Tour\", \".\", \"\\n\", \"\\n\", \"Common\", \"wealth\", \" Bank\", \" Group\", \" Executive\", \" Human\", \" Resources\", \" and\", \" Group\", \" Services\", \",\", \" Ms\", \" Barbara\", \" Chapman\", \",\", \" said\", \" the\", \" Group\", \" was\", \" committed\", \" to\", \" supporting\", \" the\", \" Arts\", \" in\", \" Australia\", \" and\", \" helping\", \" its\", \" customers\"], [\"Ben\", \"jamin\", \" Lok\", \" describes\", \" how\", \" the\", \" robot\", \" butt\", \" sensors\", \" work\", \" 2\", \":\", \"26\", \"\\n\", \"\\n\", \"Pro\", \"state\", \" exams\", \" are\", \" potentially\", \"-\", \"life\", \" saving\", \".\", \" But\", \" the\", \" process\", \" of\", \" getting\", \" one\", \" can\", \" be\", \" nerve\", \"-\", \"r\", \"acking\", \" \\u2014\", \" both\", \" for\", \" the\", \" doctor\", \" and\", \" the\", \" patient\", \".\", \"\\n\", \"\\n\", \"A\", \" group\", \" of\", \" scientists\", \" from\", \" D\", \"rex\", \"el\", \" University\", \" and\", \" the\", \" Univers\", \"ities\", \" of\", \" Wisconsin\", \" and\", \" Florida\", \" are\", \" hoping\", \" to\", \" assist\", \" with\", \" that\", \".\", \" They\", \"'ve\", \" designed\", \" a\", \" robot\", \" to\", \" help\", \" medical\", \" students\", \" give\", \" better\", \" prostate\", \" exams\", \".\", \"\\n\", \"\\n\", \"The\", \" robot\", \"'s\", \" name\", \" is\", \" \\\"\", \"Patrick\", \"\\\"\", \" and\", \" he\", \"'s\", \" an\", \" interactive\", \" butt\", \".\", \"\\n\", \"\\n\", \"Professor\", \" Benjamin\", \" Lok\", \"\\n\", \"\\n\", \"\\\"\", \"Patrick\", \" is\", \" part\", \" of\", \" a\", \" simulation\", \" where\", \" students\"], [\"New\", \" York\", \" (\", \"CNN\", \" Business\", \")\", \" On\", \" Sunday\", \",\", \" Ford\", \" will\", \" announce\", \" what\", \" is\", \" possibly\", \" the\", \" biggest\", \" change\", \" ever\", \" in\", \" the\", \" 55\", \"-\", \"year\", \" history\", \" of\", \" the\", \" Must\", \"ang\", \".\", \"\\n\", \"\\n\", \"The\", \" autom\", \"aker\", \" will\", \" unve\", \"il\", \" a\", \" vehicle\", \" bearing\", \" the\", \" Must\", \"ang\", \" brand\", \" that\", \"'s\", \" not\", \" a\", \" two\", \"-\", \"door\", \" car\", \".\", \"\\n\", \"\\n\", \"Call\", \"ed\", \" the\", \" Ford\", \" Must\", \"ang\", \" Mach\", \"-\", \"E\", \",\", \" it\", \"'s\", \" a\", \" fully\", \" electric\", \" crossover\", \" SUV\", \" that\", \" will\", \" wear\", \" the\", \" Must\", \"ang\", \"'s\", \" chrome\", \" pony\", \".\", \"\\n\", \"\\n\", \"F\", \"ord\", \" has\", \" apparently\", \" learned\", \" from\", \" brands\", \" like\", \" P\", \"orsche\", \",\", \" Lamb\", \"org\", \"h\", \"ini\", \" and\", \" Je\", \"ep\", \" that\", \" even\", \" ar\", \"dent\", \" fans\"], [\"It\", \" is\", \" a\", \" tr\", \"u\", \"ism\", \" that\", \" modern\", \" cell\", \" phones\", \" feature\", \" a\", \" multitude\", \" of\", \" features\", \" that\", \" expand\", \" on\", \" the\", \" traditional\", \" cell\", \" phone\", \" functionality\", \".\", \" For\", \" example\", \",\", \" today\", \" cell\", \" phone\", \" users\"]], \"activations\": [[[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.21400916576385498]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.2162028551101685]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.16583633422851562]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.01831185817718506]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.5287742614746094]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.24253559112548828]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.10928785800933838]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.253542900085449]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.21400916576385498]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.2162028551101685]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.16583633422851562]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.01831185817718506]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.5287742614746094]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.2006239891052246]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7577693462371826]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.1463074684143066]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.011271357536315918]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.1062750816345215]]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7fa17f809dc0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens, activations = topk_prompts_provider(\n",
    "    feature_layer=3,\n",
    "    feature_id=26928,\n",
    "    k=5\n",
    ")\n",
    "text_neuron_activations(tokens, activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check wheter different patching methods agree in which upstream features have the highest effect on the downstream feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded submod_type='mlp'submod_layer=5\n",
      "loaded submod_type='mlp'submod_layer=0\n",
      "loaded submod_type='mlp'submod_layer=1\n",
      "loaded submod_type='mlp'submod_layer=2\n",
      "loaded submod_type='mlp'submod_layer=3\n",
      "loaded submod_type='mlp'submod_layer=4\n",
      "loaded submod_type='mlp'submod_layer=0\n",
      "loaded submod_type='mlp'submod_layer=1\n",
      "loaded submod_type='mlp'submod_layer=2\n",
      "loaded submod_type='mlp'submod_layer=3\n",
      "loaded submod_type='mlp'submod_layer=4\n",
      "loaded submod_type='mlp'submod_layer=5\n",
      "loaded submod_type='mlp'submod_layer=0\n",
      "loaded submod_type='mlp'submod_layer=1\n",
      "loaded submod_type='mlp'submod_layer=2\n",
      "loaded submod_type='mlp'submod_layer=3\n",
      "loaded submod_type='mlp'submod_layer=4\n",
      "loaded submod_type='mlp'submod_layer=0\n",
      "loaded submod_type='mlp'submod_layer=1\n",
      "loaded submod_type='mlp'submod_layer=2\n",
      "loaded submod_type='mlp'submod_layer=3\n",
      "loaded submod_type='mlp'submod_layer=4\n",
      "loaded submod_type='mlp'submod_layer=5\n",
      "loaded submod_type='mlp'submod_layer=0\n",
      "loaded submod_type='mlp'submod_layer=1\n",
      "loaded submod_type='mlp'submod_layer=2\n",
      "loaded submod_type='mlp'submod_layer=3\n",
      "loaded submod_type='mlp'submod_layer=4\n",
      "loaded submod_type='mlp'submod_layer=0\n",
      "loaded submod_type='mlp'submod_layer=1\n",
      "loaded submod_type='mlp'submod_layer=2\n",
      "loaded submod_type='mlp'submod_layer=3\n",
      "loaded submod_type='mlp'submod_layer=4\n",
      "loaded submod_type='mlp'submod_layer=0\n",
      "loaded submod_type='mlp'submod_layer=1\n",
      "loaded submod_type='mlp'submod_layer=2\n",
      "loaded submod_type='mlp'submod_layer=3\n",
      "loaded submod_type='mlp'submod_layer=4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all-folded</th>\n",
       "      <th>separate</th>\n",
       "      <th>ig</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>layer0_top1</th>\n",
       "      <td>15149</td>\n",
       "      <td>15149</td>\n",
       "      <td>21840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer0_top2</th>\n",
       "      <td>21848</td>\n",
       "      <td>21848</td>\n",
       "      <td>21855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer0_top3</th>\n",
       "      <td>21841</td>\n",
       "      <td>21841</td>\n",
       "      <td>21854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer1_top1</th>\n",
       "      <td>14400</td>\n",
       "      <td>14400</td>\n",
       "      <td>14400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer1_top2</th>\n",
       "      <td>1566</td>\n",
       "      <td>25045</td>\n",
       "      <td>25045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer1_top3</th>\n",
       "      <td>14721</td>\n",
       "      <td>10964</td>\n",
       "      <td>10964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer2_top1</th>\n",
       "      <td>27186</td>\n",
       "      <td>12654</td>\n",
       "      <td>12654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer2_top2</th>\n",
       "      <td>12654</td>\n",
       "      <td>27186</td>\n",
       "      <td>27186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer2_top3</th>\n",
       "      <td>31615</td>\n",
       "      <td>13579</td>\n",
       "      <td>13579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer3_top1</th>\n",
       "      <td>26928</td>\n",
       "      <td>26928</td>\n",
       "      <td>26928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer3_top2</th>\n",
       "      <td>20945</td>\n",
       "      <td>20945</td>\n",
       "      <td>20945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer3_top3</th>\n",
       "      <td>17355</td>\n",
       "      <td>17355</td>\n",
       "      <td>17355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer4_top1</th>\n",
       "      <td>2871</td>\n",
       "      <td>2871</td>\n",
       "      <td>2871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer4_top2</th>\n",
       "      <td>11028</td>\n",
       "      <td>11028</td>\n",
       "      <td>11028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer4_top3</th>\n",
       "      <td>9616</td>\n",
       "      <td>9616</td>\n",
       "      <td>9616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             all-folded  separate     ig\n",
       "layer0_top1       15149     15149  21840\n",
       "layer0_top2       21848     21848  21855\n",
       "layer0_top3       21841     21841  21854\n",
       "layer1_top1       14400     14400  14400\n",
       "layer1_top2        1566     25045  25045\n",
       "layer1_top3       14721     10964  10964\n",
       "layer2_top1       27186     12654  12654\n",
       "layer2_top2       12654     27186  27186\n",
       "layer2_top3       31615     13579  13579\n",
       "layer3_top1       26928     26928  26928\n",
       "layer3_top2       20945     20945  20945\n",
       "layer3_top3       17355     17355  17355\n",
       "layer4_top1        2871      2871   2871\n",
       "layer4_top2       11028     11028  11028\n",
       "layer4_top3        9616      9616   9616"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "methods = ['all-folded', 'separate', 'ig'] #, 'exact']\n",
    "topk_upstream_feats_across_methods = pd.DataFrame()\n",
    "df_idx = [f'layer{layer}_top{i+1}' for layer in range(downstream_feat_layer) for i in range(k_upstream_feats)]\n",
    "for method in methods:\n",
    "    topk_upstream_feats = t.zeros((downstream_feat_layer, k_upstream_feats), dtype=int)\n",
    "\n",
    "    effects_on_dsfeat, total_effect_on_dsfeat = patching_on_downstream_feature(\n",
    "        toy_dataset,\n",
    "        model,\n",
    "        upstream_submodule_names=submodule_names[:downstream_feat_layer],\n",
    "        downstream_submodule_name=submodule_names[downstream_feat_layer],\n",
    "        downstream_feature_id=downstream_feat_id,\n",
    "        dict_cfg=dict_cfg,\n",
    "        method=method,\n",
    "    )\n",
    "    for layer in range(downstream_feat_layer):\n",
    "        effects_on_dsfeat_per_layer = effects_on_dsfeat[submodule_names[layer]][plural_token_pos].detach().cpu() # Only effect on token position `plural_token_pos` matters for \"Plurals\" task\n",
    "        topk_upstream_feats[layer] = t.argsort(effects_on_dsfeat_per_layer, descending=True)[:k_upstream_feats]\n",
    "\n",
    "    topk_upstream_feats_across_methods[method] = topk_upstream_feats.flatten()\n",
    "\n",
    "topk_upstream_feats_across_methods.index = df_idx\n",
    "topk_upstream_feats_across_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
