{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing patching functions in acdc.py\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/can/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import zstandard as zstd\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from einops import rearrange\n",
    "import torch as t\n",
    "from nnsight import LanguageModel\n",
    "from dictionary_learning.buffer import ActivationBuffer\n",
    "from dictionary_learning.dictionary import AutoEncoder\n",
    "from dictionary_learning.training import trainSAE\n",
    "from acdc import patching_on_y, patching_on_downstream_feature\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import imageio\n",
    "from circuitsvis.activations import text_neuron_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "ACTIVATION_DIM = 512\n",
    "DICTIONARY_SIZE = 64 * ACTIVATION_DIM # This is the dict_size sam mostly works with.\n",
    "EPS = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Model and SAEs\n",
    "model = LanguageModel('EleutherAI/pythia-70m-deduped', device_map='cuda:0')\n",
    "submodules = [\n",
    "    layer.mlp for layer in model.gpt_neox.layers\n",
    "]\n",
    "\n",
    "dictionaries = []\n",
    "for i in range(len(submodules)):\n",
    "    dictionary = AutoEncoder(ACTIVATION_DIM, DICTIONARY_SIZE).to('cuda:0')\n",
    "    dictionary.load_state_dict(t.load(f'/share/projects/dictionary_circuits/autoencoders/pythia-70m-deduped/mlp_out_layer{i}/1_32768/ae.pt'))\n",
    "    dictionaries.append(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([510, 637])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Toy dataset for the \"Plural\" task\n",
    "\n",
    "plural_token_pos = 1\n",
    "tok = lambda x: t.tensor(model.tokenizer.encode(x))\n",
    "toy_dataset = [dict(\n",
    "    clean_prefix=tok(\"The man\"),\n",
    "    clean_answer=tok(\" is\"),\n",
    "    patch_prefix=tok(\"The men\"),\n",
    "    patch_answer=tok(\" are\"),\n",
    "\n",
    ")]\n",
    "\n",
    "toy_dataset[0]['clean_prefix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diverse dataset for Circuitvis evaluation\n",
    "\n",
    "# set up data as a generator\n",
    "data_path = '/share/data/datasets/pile/the-eye.eu/public/AI/pile/train/00.jsonl.zst'\n",
    "compressed_file = open(data_path, 'rb')\n",
    "dctx = zstd.ZstdDecompressor()\n",
    "reader = dctx.stream_reader(compressed_file)\n",
    "text_stream = io.TextIOWrapper(reader, encoding='utf-8')\n",
    "def generator():\n",
    "    for line in text_stream:\n",
    "        yield json.loads(line)['text']\n",
    "data = generator()\n",
    "\n",
    "# Buffer tied to one specific submodule!\n",
    "buffer = ActivationBuffer(\n",
    "    data,\n",
    "    model,\n",
    "    submodules[0], # we only use data from the buffer, not acttivations. Thus we can pass any submodule here\n",
    "    io='out',\n",
    "    in_feats=ACTIVATION_DIM,\n",
    "    out_feats=ACTIVATION_DIM,\n",
    "    in_batch_size=512,\n",
    "    out_batch_size=2 ** 15,\n",
    "    n_ctxs=1e4,\n",
    ")\n",
    "\n",
    "tokenized_prompts = buffer.tokenized_batch(batch_size=512) # [batch, n_ctx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Circuitsvis visualization\n",
    "\n",
    "def get_feature_activations(layer):\n",
    "    '''\n",
    "    Function for retrieving feature activations from tokenized batch\n",
    "    returns activations for all submodules, dictionaries if layer == None\n",
    "    \n",
    "    '''\n",
    "    with model.generate(max_new_tokens=1, pad_token_id=model.tokenizer.pad_token_id) as generator:\n",
    "        with generator.invoke(tokenized_prompts['input_ids'], scan=False) as invoker:\n",
    "            hidden_states = submodules[layer].output.save() # hidden_states.value: [batch, n_ctx, d_mlp]\n",
    "    dictionary_activation = dictionaries[layer].encode(hidden_states.value)\n",
    "    return dictionary_activation\n",
    "\n",
    "def list_decode(x):\n",
    "    if isinstance(x, int):\n",
    "        return model.tokenizer.decode(x)\n",
    "    else:\n",
    "        return [list_decode(y) for y in x]\n",
    "    \n",
    "def topk_prompts_provider(feature_layer: int, feature_id: int, k: int = 30):\n",
    "    # Sort examples in batch by max feature activation\n",
    "    dictionary_activations = get_feature_activations(feature_layer)\n",
    "    acts = dictionary_activations[:, :, feature_id].cpu() # acts: [batch, pos]\n",
    "    flattened_acts = rearrange(acts, 'b l -> (b l)')\n",
    "    topk_indices = t.argsort(flattened_acts, dim=0, descending=True)[:k] \n",
    "    batch_indices = topk_indices // acts.shape[1]\n",
    "    token_indices = topk_indices % acts.shape[1]\n",
    "\n",
    "    # Visualize\n",
    "    tokens = [\n",
    "    tokenized_prompts['input_ids'][batch_idx, :token_idx+1].tolist() for batch_idx, token_idx in zip(batch_indices, token_indices)\n",
    "    ]\n",
    "    tokens = list_decode(tokens)\n",
    "    activations = [\n",
    "        acts[batch_idx, :token_id+1, None, None] for batch_idx, token_id in zip(batch_indices, token_indices)\n",
    "    ]\n",
    "    return tokens, activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing `patching_on_y`\n",
    "Evaluate causal effects of features in layer 5 on the logit diff between clean and patch answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_patching_on_y = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total metric diff after replacing clean prefix with patch prefix: 8.8028564453125\n"
     ]
    }
   ],
   "source": [
    "effects_on_y, total_effect_on_y = patching_on_y(\n",
    "    toy_dataset,\n",
    "    model,\n",
    "    submodules,\n",
    "    dictionaries,\n",
    "    method='all-folded'\n",
    ")\n",
    "\n",
    "print(f'total metric diff after replacing clean prefix with patch prefix: {total_effect_on_y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 features in layer 5 with highest impact on logit diff\n",
      "feat 22167:\t 0.15118589997291565\n",
      "feat 7352:\t 0.0881505087018013\n",
      "feat 10880:\t 0.07269265502691269\n",
      "feat 14747:\t 0.06640410423278809\n",
      "feat 13505:\t 0.05375915765762329\n",
      "feat 19033:\t 0.023533198982477188\n",
      "feat 20780:\t 0.02276180312037468\n",
      "feat 30204:\t 0.013442937284708023\n",
      "feat 5580:\t 0.012757625430822372\n",
      "feat 25775:\t 0.011507009156048298\n"
     ]
    }
   ],
   "source": [
    "# Top 10 features in layer `layer_patching_on_y` with highest impact on logit diff\n",
    "\n",
    "# Only effect on token position `plural_token_pos` matters for \"Plurals\" task\n",
    "effects_on_y = effects_on_y[submodules[layer_patching_on_y]][plural_token_pos].detach().cpu()\n",
    "top_effects_on_y = t.argsort(effects_on_y, descending=True)[:10]\n",
    "top_features_on_y = []\n",
    "print(f'Top 10 features in layer {layer_patching_on_y} with highest impact on logit diff')\n",
    "for i in top_effects_on_y:\n",
    "    top_features_on_y.append(i)\n",
    "    print(f'feat {i}:\\t {effects_on_y[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(effects_on_y[effects_on_y.abs() > EPS], bins=100)\n",
    "plt.xscale('log')\n",
    "plt.title(f'Feature effect on logit diff distribution\\nMLP={layer_patching_on_y}');\n",
    "plt.xlabel('Mean feature activation')\n",
    "plt.ylabel('Frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-5729b72a-0f27\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-5729b72a-0f27\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [[\"It\", \"\\u2019\", \"s\", \" a\", \" crime\", \" so\", \" horrible\", \" that\", \" it\", \"\\u2019\", \"s\", \" hard\", \" to\", \" imagine\", \".\", \" On\", \" Thursday\", \",\", \" a\", \" man\"], [\"It\", \"\\u2019\", \"s\", \" a\", \" crime\", \" so\", \" horrible\", \" that\", \" it\", \"\\u2019\", \"s\", \" hard\", \" to\", \" imagine\", \".\", \" On\", \" Thursday\", \",\", \" a\", \" man\", \" in\", \" C\", \"ang\", \"gu\", \" cut\", \" off\", \" his\", \" wife\", \"\\u2019\", \"s\", \" left\", \" foot\", \" and\", \" attempted\", \" to\", \" cle\", \"ave\", \" off\", \" her\", \" right\", \" foot\", \" as\", \" well\", \".\", \" The\", \" attack\", \" occurred\", \" in\", \" front\", \" of\", \" the\", \" couple\", \"\\u2019\", \"s\", \" young\", \" children\", \",\", \" who\", \" have\", \" now\", \" been\", \" sent\", \" to\", \" their\", \" grandmother\", \" in\", \" Sing\", \"ar\", \"aja\", \" for\", \" care\", \".\", \"\\n\", \"\\n\", \"The\", \" woman\"], [\"Seven\", \" rare\", \" rh\", \"inos\", \" spotted\", \" in\", \" Indonesian\", \" jungle\", \"\\n\", \"\\n\", \"August\", \" 9\", \",\", \" 2012\", \" in\", \" Biology\", \" /\", \" Ec\", \"ology\", \"\\n\", \"\\n\", \"In\", \" this\", \" und\", \"ated\", \" photo\", \" released\", \" by\", \" Le\", \"user\", \" International\", \" Foundation\", \",\", \" a\", \" Sum\", \"at\", \"ran\", \" rh\", \"ino\", \" ro\", \"ams\", \" at\", \" Gun\", \"ung\", \" Le\", \"user\", \" National\", \" Park\", \" in\", \" Ace\", \"h\", \" province\", \",\", \" Indonesia\", \".\", \" A\", \" conservation\", \"ist\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"MI\", \"AM\", \"I\", \" \\u2013\", \" City\", \" of\", \" Miami\", \" police\", \" and\", \" fire\", \" rescue\", \" were\", \" able\", \" to\", \" convince\", \" a\", \" man\"], [\"\\\"\", \"Ryan\", \" got\", \" six\", \" months\", \" probation\", \" and\", \" five\", \" public\", \" work\", \" days\", \",\\\"\", \" a\", \" source\", \" told\", \" Rad\", \"ar\", \".\", \" \\\"\", \"He\", \" has\", \" random\", \" drug\", \" tests\", \" in\", \" between\", \".\\\"\", \"\\n\\n\\n\", \"\\n\", \"Ed\", \"wards\", \",\", \" 30\", \",\", \" showed\", \" up\", \" to\", \" Red\", \" Bank\", \" City\", \" Court\", \" in\", \" a\", \" pink\", \" button\", \"-\", \"down\", \" shirt\", \" without\", \" his\", \" supportive\", \" wife\", \" Mack\", \"enzie\", \" by\", \" his\", \" side\", \".\", \"\\n\", \"\\n\", \"The\", \" legal\", \" trouble\", \" started\", \" when\", \" Mac\", \"i\", \" Book\", \"out\", \" '\", \"s\", \" baby\", \" daddy\", \" was\", \" hit\", \" with\", \" a\", \" citation\", \" on\", \" March\", \" 12\", \",\", \" 2017\", \".\", \" Police\", \" pulled\", \" him\", \" over\", \" during\", \" a\", \" traffic\", \" stop\", \" for\", \" expired\", \" registration\", \".\", \" During\", \" the\", \" stop\", \",\", \" an\", \" officer\"]], \"activations\": [[[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[6.451082706451416]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[6.451082706451416]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.29218006134033203]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.788418292999268]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.4353809356689453]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.09580636024475098]], [[0.0]], [[0.0]], [[3.778489589691162]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[3.607823371887207]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.5565948486328125]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.24161767959594727]], [[0.0]], [[0.0]], [[0.8047651052474976]], [[2.14845609664917]], [[0.11858808994293213]], [[0.0]], [[0.0]], [[0.0]], [[0.2730443477630615]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.15681684017181396]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.023138761520385742]], [[3.190941333770752]]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f34a06985e0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Need to stack functions to display circuitvis properly\n",
    "text_neuron_activations(\n",
    "    *topk_prompts_provider(\n",
    "        feature_layer=layer_patching_on_y, \n",
    "        feature_id=top_features_on_y[0],\n",
    "        k=5\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5/22167 seems to activate on singular nouns related do people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing `patching_on_downstream_feature`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "downstream_feat_layer = 5\n",
    "downstream_feat_id = 22167\n",
    "\n",
    "k_upstream_feats = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m effects_on_dsfeat, total_effect_on_dsfeat \u001b[38;5;241m=\u001b[39m \u001b[43mpatching_on_downstream_feature\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoy_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubmodules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdictionaries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownstream_feature_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownstream_feat_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownstream_feature_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownstream_feat_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mall-folded\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal metric diff after replacing clean prefix with patch prefix: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_effect_on_dsfeat\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/dictionary-circuits/notebooks/../acdc.py:55\u001b[0m, in \u001b[0;36mpatching_on_downstream_feature\u001b[0;34m(dataset, model, submodules, dictionaries, downstream_feature_layer, downstream_feature_id, method, steps)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m consolidated_patching_on(dataset, model, submodules, dictionaries, metric_fn, method, steps)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Outdated patching_on_y; split up into two functions now\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03mdef patching_on_y(\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m    dataset,\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m    model,\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m    submodules,\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m    dictionaries,\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m    method='all-folded',\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m    steps=10,\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m):\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    clean_inputs = t.cat([example['clean_prefix'] for example in dataset], dim=0)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    patch_inputs = t.cat([example['patch_prefix'] for example in dataset], dim=0)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    clean_answer_idxs = t.Tensor([example['clean_answer'] for example in dataset]).long() # shape [n_examples]\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m    patch_answer_idxs = t.Tensor([example['patch_answer'] for example in dataset]).long()\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m    def metric_fn(model):\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m        logits = model.embed_out.output[:, -1, :] # shape [n_examples, vocab_size]\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m        logit_diff = t.gather(\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m            logits,\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m            dim=-1,\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m            index=patch_answer_idxs.unsqueeze(-1)\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m        ) - t.gather(\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03m            logits,\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m            dim=-1,\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m            index=clean_answer_idxs.unsqueeze(-1)\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03m        )\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03m        return logit_diff.squeeze(-1) # shape [n_examples]\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m    effects, total_effect = patching_effect(\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m        clean_inputs,\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m        patch_inputs,\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m        model,\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m        submodules,\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m        dictionaries,\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m        metric_fn,\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03m        method=method,\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m        steps=steps,\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m    )\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m    return EffectOut(\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m        effects={k : v.mean(dim=0) for k, v in effects.items()}, # v shape: [pos, d_sae]\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m        total_effect=total_effect.mean(dim=0),\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    )\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Outdated patching_on_feature_activation()\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03mdef get_hook(submodule_idx, patch, clean, threshold, mean_effects):\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    def hook(grad):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;124;03m    return mean_effects\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n",
      "File \u001b[0;32m~/dictionary-circuits/notebooks/../acdc.py:11\u001b[0m, in \u001b[0;36mconsolidated_patching_on\u001b[0;34m(dataset, model, submodules, dictionaries, metric_fn, method, steps)\u001b[0m\n\u001b[1;32m      8\u001b[0m clean_inputs \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mcat([example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_prefix\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m dataset], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      9\u001b[0m patch_inputs \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mcat([example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatch_prefix\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m dataset], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m effects, total_effect \u001b[38;5;241m=\u001b[39m \u001b[43mpatching_effect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatch_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubmodules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdictionaries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m EffectOut(\n\u001b[1;32m     23\u001b[0m     effects\u001b[38;5;241m=\u001b[39m{k : v\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m effects\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[1;32m     24\u001b[0m     total_effect\u001b[38;5;241m=\u001b[39mtotal_effect\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     25\u001b[0m )\n",
      "File \u001b[0;32m~/dictionary-circuits/notebooks/../attribution.py:203\u001b[0m, in \u001b[0;36mpatching_effect\u001b[0;34m(clean, patch, model, submodules, dictionaries, metric_fn, method, steps)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpatching_effect\u001b[39m(\n\u001b[1;32m    193\u001b[0m         clean,\n\u001b[1;32m    194\u001b[0m         patch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m         steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m    201\u001b[0m ):\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall-folded\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pe_attrib_all_folded\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubmodules\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdictionaries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseparate\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _pe_attrib_separate(clean, patch, model, submodules, dictionaries, metric_fn)\n",
      "File \u001b[0;32m~/dictionary-circuits/notebooks/../attribution.py:27\u001b[0m, in \u001b[0;36m_pe_attrib_all_folded\u001b[0;34m(clean, patch, model, submodules, dictionaries, metric_fn)\u001b[0m\n\u001b[1;32m     25\u001b[0m         residual \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m-\u001b[39m x_hat)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     26\u001b[0m         submodule\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m x_hat \u001b[38;5;241m+\u001b[39m residual\n\u001b[0;32m---> 27\u001b[0m     metric_clean \u001b[38;5;241m=\u001b[39m metric_fn(model)\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m     28\u001b[0m metric_clean\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model\u001b[38;5;241m.\u001b[39minvoke(patch):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nnsight/contexts/DirectInvoker.py:37\u001b[0m, in \u001b[0;36mDirectInvoker.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfwd_args\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 37\u001b[0m \u001b[43mRunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nnsight/contexts/Runner.py:69\u001b[0m, in \u001b[0;36mRunner.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_server()\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nnsight/contexts/Runner.py:74\u001b[0m, in \u001b[0;36mRunner.run_local\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs the local_model using it's chosen method.\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Run the model and store the output.\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nnsight/models/AbstractModel.py:200\u001b[0m, in \u001b[0;36mAbstractModel.__call__\u001b[0;34m(self, fn, inputs, graph, edits, inference, *args, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode(mode\u001b[38;5;241m=\u001b[39minference):\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m HookModel(\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_model,\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;28mlist\u001b[39m(graph\u001b[38;5;241m.\u001b[39margument_node_names\u001b[38;5;241m.\u001b[39mkeys()),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    198\u001b[0m         ),\n\u001b[1;32m    199\u001b[0m     ):\n\u001b[0;32m--> 200\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m increment_hook\u001b[38;5;241m.\u001b[39mremove()\n\u001b[1;32m    204\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepoid_path_clsname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nnsight/models/LanguageModel.py:159\u001b[0m, in \u001b[0;36mLanguageModel._forward\u001b[0;34m(self, prepared_inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, prepared_inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_model(\n\u001b[0;32m--> 159\u001b[0m         \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mprepared_inputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    160\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:789\u001b[0m, in \u001b[0;36mBatchEncoding.to\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 789\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    791\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:789\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 789\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    791\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "effects_on_dsfeat, total_effect_on_dsfeat = patching_on_downstream_feature(\n",
    "    toy_dataset,\n",
    "    model,\n",
    "    submodules,\n",
    "    dictionaries,\n",
    "    downstream_feature_layer=downstream_feat_layer,\n",
    "    downstream_feature_id=downstream_feat_id, \n",
    "    method='all-folded'\n",
    ")\n",
    "\n",
    "print(f'total metric diff after replacing clean prefix with patch prefix: {total_effect_on_dsfeat}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 features per layer with highest impact on downstream feat activation\n",
      "\n",
      "Layer 0:\n",
      "feat 21840:\t 0.0\n",
      "feat 21855:\t 0.0\n",
      "feat 21854:\t 0.0\n",
      "\n",
      "Layer 1:\n",
      "feat 14400:\t 0.06060613691806793\n",
      "feat 1566:\t 0.014259410090744495\n",
      "feat 10964:\t 0.013167855329811573\n",
      "\n",
      "Layer 2:\n",
      "feat 12654:\t 0.02732522413134575\n",
      "feat 27186:\t 0.02288554236292839\n",
      "feat 31615:\t 0.014543569646775723\n",
      "\n",
      "Layer 3:\n",
      "feat 26928:\t 0.09637608379125595\n",
      "feat 20945:\t 0.028065158054232597\n",
      "feat 30457:\t 0.015654182061553\n",
      "\n",
      "Layer 4:\n",
      "feat 2871:\t 0.07770593464374542\n",
      "feat 28130:\t 0.00389579264447093\n",
      "feat 11028:\t 0.0014464699197560549\n",
      "\n",
      "Layer 5:\n",
      "feat 7352:\t 0.056569550186395645\n",
      "feat 5580:\t 0.05062364414334297\n",
      "feat 20780:\t 0.03504457697272301\n"
     ]
    }
   ],
   "source": [
    "# Top k features per layer with highest impact on downstream feat activation\n",
    "\n",
    "topk_upstream_feats = t.zeros((len(submodules), k_upstream_feats), dtype=int)\n",
    "print(f'Top {k_upstream_feats} features per layer with highest impact on downstream feat activation')\n",
    "\n",
    "for layer in range(len(submodules)):\n",
    "    print(f'\\nLayer {layer}:')\n",
    "    effects_on_dsfeat_per_layer = effects_on_dsfeat[submodules[layer]][plural_token_pos].detach().cpu() # Only effect on token position `plural_token_pos` matters for \"Plurals\" task\n",
    "    topk_upstream_feats[layer] = t.argsort(effects_on_dsfeat_per_layer, descending=True)[:k_upstream_feats]\n",
    "    for i in topk_upstream_feats[layer]:\n",
    "        print(f'feat {i}:\\t {effects_on_dsfeat_per_layer[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 1 features per layer found seem to be related to the plurals task!\n",
    "\n",
    "Values in same or later layer are generally uninterpretable.\n",
    "I expect that, xact patching should yield 0 unconnected features in same or later layer. (No bwdpass involved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-585e25e9-cdb6\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-585e25e9-cdb6\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [[\"Ben\", \"jamin\", \" Lok\", \" describes\", \" how\", \" the\", \" robot\", \" butt\", \" sensors\", \" work\", \" 2\", \":\", \"26\", \"\\n\", \"\\n\", \"Pro\", \"state\", \" exams\", \" are\", \" potentially\", \"-\", \"life\", \" saving\", \".\", \" But\", \" the\", \" process\", \" of\", \" getting\", \" one\", \" can\", \" be\", \" nerve\", \"-\", \"r\", \"acking\", \" \\u2014\", \" both\", \" for\", \" the\", \" doctor\", \" and\", \" the\", \" patient\", \".\", \"\\n\", \"\\n\", \"A\", \" group\", \" of\", \" scientists\", \" from\", \" D\", \"rex\", \"el\", \" University\", \" and\", \" the\", \" Univers\", \"ities\", \" of\", \" Wisconsin\", \" and\", \" Florida\", \" are\", \" hoping\", \" to\", \" assist\", \" with\", \" that\", \".\", \" They\", \"'ve\", \" designed\", \" a\", \" robot\", \" to\", \" help\", \" medical\", \" students\"], [\"Common\", \"wealth\", \" Bank\", \" and\", \" the\", \" Australian\", \" Chamber\", \" Orchestra\", \" kick\", \" off\", \" the\", \" 2009\", \" Great\", \" Rom\", \"antics\", \" national\", \" tour\", \"\\n\", \"\\n\", \"S\", \"yd\", \"ney\", \",\", \" 11\", \" June\", \" 2009\", \":\", \" The\", \" Commonwealth\", \" Bank\", \" today\", \" congrat\", \"ulated\", \" the\", \" Australian\", \" Chamber\", \" Orchestra\", \" (\", \"AC\", \"O\", \")\", \" on\", \" the\", \" commencement\", \" of\", \" its\", \" Great\", \" Rom\", \"antics\", \" Tour\", \".\", \"\\n\", \"\\n\", \"Common\", \"wealth\", \" Bank\", \" Group\", \" Executive\", \" Human\", \" Resources\", \" and\", \" Group\", \" Services\", \",\", \" Ms\", \" Barbara\", \" Chapman\", \",\", \" said\", \" the\", \" Group\", \" was\", \" committed\", \" to\", \" supporting\", \" the\", \" Arts\", \" in\", \" Australia\", \" and\", \" helping\", \" its\", \" customers\"], [\"Ben\", \"jamin\", \" Lok\", \" describes\", \" how\", \" the\", \" robot\", \" butt\", \" sensors\", \" work\", \" 2\", \":\", \"26\", \"\\n\", \"\\n\", \"Pro\", \"state\", \" exams\", \" are\", \" potentially\", \"-\", \"life\", \" saving\", \".\", \" But\", \" the\", \" process\", \" of\", \" getting\", \" one\", \" can\", \" be\", \" nerve\", \"-\", \"r\", \"acking\", \" \\u2014\", \" both\", \" for\", \" the\", \" doctor\", \" and\", \" the\", \" patient\", \".\", \"\\n\", \"\\n\", \"A\", \" group\", \" of\", \" scientists\", \" from\", \" D\", \"rex\", \"el\", \" University\", \" and\", \" the\", \" Univers\", \"ities\", \" of\", \" Wisconsin\", \" and\", \" Florida\", \" are\", \" hoping\", \" to\", \" assist\", \" with\", \" that\", \".\", \" They\", \"'ve\", \" designed\", \" a\", \" robot\", \" to\", \" help\", \" medical\", \" students\", \" give\", \" better\", \" prostate\", \" exams\", \".\", \"\\n\", \"\\n\", \"The\", \" robot\", \"'s\", \" name\", \" is\", \" \\\"\", \"Patrick\", \"\\\"\", \" and\", \" he\", \"'s\", \" an\", \" interactive\", \" butt\", \".\", \"\\n\", \"\\n\", \"Professor\", \" Benjamin\", \" Lok\", \"\\n\", \"\\n\", \"\\\"\", \"Patrick\", \" is\", \" part\", \" of\", \" a\", \" simulation\", \" where\", \" students\"], [\"New\", \" York\", \" (\", \"CNN\", \" Business\", \")\", \" On\", \" Sunday\", \",\", \" Ford\", \" will\", \" announce\", \" what\", \" is\", \" possibly\", \" the\", \" biggest\", \" change\", \" ever\", \" in\", \" the\", \" 55\", \"-\", \"year\", \" history\", \" of\", \" the\", \" Must\", \"ang\", \".\", \"\\n\", \"\\n\", \"The\", \" autom\", \"aker\", \" will\", \" unve\", \"il\", \" a\", \" vehicle\", \" bearing\", \" the\", \" Must\", \"ang\", \" brand\", \" that\", \"'s\", \" not\", \" a\", \" two\", \"-\", \"door\", \" car\", \".\", \"\\n\", \"\\n\", \"Call\", \"ed\", \" the\", \" Ford\", \" Must\", \"ang\", \" Mach\", \"-\", \"E\", \",\", \" it\", \"'s\", \" a\", \" fully\", \" electric\", \" crossover\", \" SUV\", \" that\", \" will\", \" wear\", \" the\", \" Must\", \"ang\", \"'s\", \" chrome\", \" pony\", \".\", \"\\n\", \"\\n\", \"F\", \"ord\", \" has\", \" apparently\", \" learned\", \" from\", \" brands\", \" like\", \" P\", \"orsche\", \",\", \" Lamb\", \"org\", \"h\", \"ini\", \" and\", \" Je\", \"ep\", \" that\", \" even\", \" ar\", \"dent\", \" fans\"], [\"It\", \" is\", \" a\", \" tr\", \"u\", \"ism\", \" that\", \" modern\", \" cell\", \" phones\", \" feature\", \" a\", \" multitude\", \" of\", \" features\", \" that\", \" expand\", \" on\", \" the\", \" traditional\", \" cell\", \" phone\", \" functionality\", \".\", \" For\", \" example\", \",\", \" today\", \" cell\", \" phone\", \" users\"]], \"activations\": [[[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.21400916576385498]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.2162028551101685]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.16583633422851562]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.01831185817718506]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.5287742614746094]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.24253559112548828]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.10928785800933838]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.253542900085449]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.21400916576385498]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.2162028551101685]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.16583633422851562]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.01831185817718506]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.5287742614746094]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.2006239891052246]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7577693462371826]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.1463074684143066]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.011271357536315918]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.1062750816345215]]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f34a0bc7f40>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens, activations = topk_prompts_provider(\n",
    "    feature_layer=3,\n",
    "    feature_id=26928,\n",
    "    k=5\n",
    ")\n",
    "text_neuron_activations(tokens, activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check wheter different patching methods agree in which upstream features have the highest effect on the downstream feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all-folded</th>\n",
       "      <th>separate</th>\n",
       "      <th>ig</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>layer0_top1</th>\n",
       "      <td>21840</td>\n",
       "      <td>15149</td>\n",
       "      <td>21840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer0_top2</th>\n",
       "      <td>21855</td>\n",
       "      <td>21848</td>\n",
       "      <td>21855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer0_top3</th>\n",
       "      <td>21854</td>\n",
       "      <td>21841</td>\n",
       "      <td>21854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer1_top1</th>\n",
       "      <td>14400</td>\n",
       "      <td>14400</td>\n",
       "      <td>14400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer1_top2</th>\n",
       "      <td>1566</td>\n",
       "      <td>25045</td>\n",
       "      <td>25045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer1_top3</th>\n",
       "      <td>10964</td>\n",
       "      <td>10964</td>\n",
       "      <td>10964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer2_top1</th>\n",
       "      <td>12654</td>\n",
       "      <td>12654</td>\n",
       "      <td>12654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer2_top2</th>\n",
       "      <td>27186</td>\n",
       "      <td>27186</td>\n",
       "      <td>27186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer2_top3</th>\n",
       "      <td>31615</td>\n",
       "      <td>13579</td>\n",
       "      <td>13579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer3_top1</th>\n",
       "      <td>26928</td>\n",
       "      <td>26928</td>\n",
       "      <td>26928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer3_top2</th>\n",
       "      <td>20945</td>\n",
       "      <td>20945</td>\n",
       "      <td>20945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer3_top3</th>\n",
       "      <td>30457</td>\n",
       "      <td>17355</td>\n",
       "      <td>17355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer4_top1</th>\n",
       "      <td>2871</td>\n",
       "      <td>2871</td>\n",
       "      <td>2871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer4_top2</th>\n",
       "      <td>28130</td>\n",
       "      <td>11028</td>\n",
       "      <td>11028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer4_top3</th>\n",
       "      <td>11028</td>\n",
       "      <td>9616</td>\n",
       "      <td>9616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer5_top1</th>\n",
       "      <td>7352</td>\n",
       "      <td>7352</td>\n",
       "      <td>7352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer5_top2</th>\n",
       "      <td>5580</td>\n",
       "      <td>5580</td>\n",
       "      <td>5580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer5_top3</th>\n",
       "      <td>20780</td>\n",
       "      <td>20780</td>\n",
       "      <td>20780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             all-folded  separate     ig\n",
       "layer0_top1       21840     15149  21840\n",
       "layer0_top2       21855     21848  21855\n",
       "layer0_top3       21854     21841  21854\n",
       "layer1_top1       14400     14400  14400\n",
       "layer1_top2        1566     25045  25045\n",
       "layer1_top3       10964     10964  10964\n",
       "layer2_top1       12654     12654  12654\n",
       "layer2_top2       27186     27186  27186\n",
       "layer2_top3       31615     13579  13579\n",
       "layer3_top1       26928     26928  26928\n",
       "layer3_top2       20945     20945  20945\n",
       "layer3_top3       30457     17355  17355\n",
       "layer4_top1        2871      2871   2871\n",
       "layer4_top2       28130     11028  11028\n",
       "layer4_top3       11028      9616   9616\n",
       "layer5_top1        7352      7352   7352\n",
       "layer5_top2        5580      5580   5580\n",
       "layer5_top3       20780     20780  20780"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "methods = ['all-folded', 'separate', 'ig'] #, 'exact']\n",
    "topk_upstream_feats_across_methods = pd.DataFrame()\n",
    "df_idx = []\n",
    "for method in methods:\n",
    "    topk_upstream_feats = t.zeros((len(submodules), k_upstream_feats), dtype=int)\n",
    "\n",
    "    effects_on_dsfeat, total_effect_on_dsfeat = patching_on_downstream_feature(\n",
    "        toy_dataset,\n",
    "        model,\n",
    "        submodules,\n",
    "        dictionaries,\n",
    "        downstream_feature_layer=downstream_feat_layer,\n",
    "        downstream_feature_id=downstream_feat_id, \n",
    "        method=method,\n",
    "    )\n",
    "    for layer in range(len(submodules)):\n",
    "        effects_on_dsfeat_per_layer = effects_on_dsfeat[submodules[layer]][plural_token_pos].detach().cpu() # Only effect on token position `plural_token_pos` matters for \"Plurals\" task\n",
    "        topk_upstream_feats[layer] = t.argsort(effects_on_dsfeat_per_layer, descending=True)[:k_upstream_feats]\n",
    "        if len(df_idx) < len(submodules) * k_upstream_feats:\n",
    "            df_idx += [f'layer{layer}_top{i+1}' for i in range(k_upstream_feats)]\n",
    "\n",
    "    topk_upstream_feats_across_methods[method] = topk_upstream_feats.flatten()\n",
    "\n",
    "topk_upstream_feats_across_methods.index = df_idx\n",
    "topk_upstream_feats_across_methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative circuit discovery\n",
    "TopK features from each layer with highes effect on metric\n",
    "For each feature found: Top k features with highest absolute effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "FeatureEffect = namedtuple('FeatureEffect', ['upstream_feature_layer', 'upstream_feature_id', 'downstream_feature_layer', 'downstream_feature_id', 'effect'])\n",
    "MetricEffect = namedtuple('MetricEffect', ['upstream_feature_layer', 'upstream_feature_id', 'effect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "\n",
    "effect_list = []\n",
    "feature_queue = []\n",
    "\n",
    "# Run patching on y once\n",
    "effects_on_y, _ = patching_on_y(\n",
    "    toy_dataset,\n",
    "    model,\n",
    "    submodules,\n",
    "    dictionaries,\n",
    "    method='all-folded'\n",
    ")\n",
    "\n",
    "for layer, submodule in enumerate(submodules):\n",
    "    topk_features = t.argsort(effects_on_y[submodule], descending=True)[:k]\n",
    "    for feat_id in topk_features:\n",
    "        eff = MetricEffect(\n",
    "            upstream_feature_layer=layer,\n",
    "            upstream_feature_id=feat_id,\n",
    "            effect=effects_on_y[submodule][feat_id]\n",
    "        )\n",
    "        effect_list.append(eff)\n",
    "        feature_queue.append(eff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run patching on all features in queue\n",
    "while len(feature_queue) > 0:\n",
    "    feature_layer, feature_id, _, _, _ = feature_queue.pop(0)\n",
    "    \n",
    "    effects_on_dsfeat, total_effect_on_dsfeat = patching_on_downstream_feature( # TODO adapt this method to only calculate values for features in layers above downstream feature? Possible for attribution patching methods gradent requires full bwd pass?\n",
    "        toy_dataset,\n",
    "        model,\n",
    "        submodules,\n",
    "        dictionaries,\n",
    "        downstream_feature_layer=feature_layer,\n",
    "        downstream_feature_id=feature_id, \n",
    "        method='all-folded'\n",
    "    )\n",
    "\n",
    "    for layer, submodule in enumerate(submodules[:feature_layer]): # Only layers above downstream feature considered.\n",
    "        topk_features = t.argsort(effects_on_y[submodule], descending=True)[:k]\n",
    "        for feat_id in topk_features:\n",
    "            eff = MetricEffect(\n",
    "                upstream_feature_layer=layer,\n",
    "                upstream_feature_id=feat_id,\n",
    "                effect=effects_on_y[submodule][feat_id]\n",
    "            )\n",
    "            effect_list.append(eff)\n",
    "            feature_queue.append(eff)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the context for features in effect list.\n",
    "- Automatically using LLM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
