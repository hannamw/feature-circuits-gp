{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f190fd1860744fc4893cd2342bc546d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from dictionary_learning.interp import examine_dimension\n",
    "from dictionary_learning.utils import zst_to_generator\n",
    "import torch as t\n",
    "import gc\n",
    "import numpy as np\n",
    "from dictionary_learning.dictionary import GatedAutoEncoder, JumpReLUSAE\n",
    "from dictionary_learning.buffer import ActivationBuffer\n",
    "from sae_lens import SparseAutoencoder\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_name = \"google/gemma-2-2b\"\n",
    "# model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=t.float16,\n",
    "                             device_map=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"right\")\n",
    "layer = 13\n",
    "submodule_name = f\"model.layers.{layer}\"\n",
    "submodule = model.model.layers[layer]\n",
    "\n",
    "dictionaries = {}\n",
    "# ae = GatedAutoEncoder(4096, 32768).half().to(\"cuda\")\n",
    "# ae.load_state_dict(t.load(f'llama_saes/layer{layer}/ae_81920.pt'))\n",
    "# dictionaries[submodule] = ae\n",
    "path_to_params = hf_hub_download(\n",
    "    repo_id=\"google/gemma-scope-2b-pt-res\",\n",
    "    filename=f\"layer_{layer}/width_16k/canonical/params.npz\",\n",
    "    force_download=False,\n",
    ")\n",
    "params = np.load(path_to_params)\n",
    "pt_params = {k: t.from_numpy(v).cuda() for k, v in params.items()}\n",
    "ae = JumpReLUSAE(params[\"W_enc\"].shape[0], params[\"W_enc\"].shape[1]).to(\"cuda\")\n",
    "ae.load_state_dict(pt_params)\n",
    "ae = ae.half()\n",
    "dictionaries[submodule] = ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1596, 603, 476, 2067, 235265]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"This is a string.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁is']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenizer.encode(\" is\", add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unhandled sentences: 306 / 12460 (2.46%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89fd17b2259c41efb63a004060a54c02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from collections import deque\n",
    "from dictionary_learning.interp_utils import *\n",
    "\n",
    "max_length = 64\n",
    "\n",
    "def parse_and_load_text(indata, tokenizer, max_length=128, space_char=\"▁\"):\n",
    "    def _parse_morphosyn_feats(morphosyn_str, pos):\n",
    "        if morphosyn_str == \"_\":\n",
    "            return []\n",
    "        \n",
    "        features = []\n",
    "        if \"|\" in morphosyn_str:\n",
    "            morphosyn_list = morphosyn_str.split(\"|\")\n",
    "        else:\n",
    "            morphosyn_list = [morphosyn_str]\n",
    "        for feature in morphosyn_list:\n",
    "            name, value = feature.split(\"=\")\n",
    "            features.append(f\"{pos}:{name}_{value}\")\n",
    "        return features\n",
    "\n",
    "    def _lookahead(lines, idx, word, tokens, sentence_to_labels, sentence_to_deps, morphosyn_feats, dep_label, max_lookahead=1):\n",
    "        lookahead = 1\n",
    "        while lookahead <= max_lookahead:\n",
    "            matched = True\n",
    "            next_word = lines[idx+lookahead].split(\"\\t\")[1]\n",
    "            word = f\"{word}{next_word}\"\n",
    "            word_tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(word, add_special_tokens=False))\n",
    "            # nested munging\n",
    "            for word_token in word_tokens:\n",
    "                if word_token == tokens[0]:\n",
    "                    sentence_to_labels[sentence].append(morphosyn_feats)\n",
    "                    sentence_to_deps[sentence].append(dep_label)\n",
    "                    tokens.popleft()\n",
    "                else:\n",
    "                    lookahead += 1\n",
    "                    matched = False\n",
    "                    break\n",
    "            if matched:\n",
    "                break\n",
    "        return tokens, lookahead\n",
    "\n",
    "    sentences = []\n",
    "    sentence_to_labels = {}\n",
    "    sentence_to_deps = {}\n",
    "    lines = indata.readlines()\n",
    "    num_to_skip = 0\n",
    "    num_sents = 0\n",
    "    num_sents_skipped = 0\n",
    "    sentence_unhandled = False\n",
    "    for idx, line in enumerate(lines):\n",
    "        if num_to_skip > 0:\n",
    "            num_to_skip -= 1\n",
    "            continue\n",
    "\n",
    "        if line.startswith(\"# text\"):\n",
    "            sentence = line.strip().split(\"# text = \")[1]\n",
    "            if sentence.startswith(\"http\") or sentence == \"Smokers Haven\":\n",
    "                sentence = None     # skip this one\n",
    "                num_sents_skipped += 1\n",
    "                continue\n",
    "            sentence = sentence.replace(u\"\\xa0\", \" \").replace(\"  \", \" \")\n",
    "            num_sents += 1\n",
    "            sentences.append(sentence)\n",
    "            tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(sentence, add_special_tokens=False))\n",
    "            tokens_len = len(tokens)\n",
    "            tokens = deque(tokens)  # deques can pop from left much more efficiently than lists\n",
    "            sentence_to_labels[sentence] = []\n",
    "            sentence_to_deps[sentence] = []\n",
    "            continue\n",
    "        elif line.startswith(\"# \"):\n",
    "            continue\n",
    "        elif len(line) < 2:     # Empty line means end-of-sentence\n",
    "            if sentence is None:\n",
    "                continue\n",
    "            assert len(tokens) == 0, f\"Not all tokens have been processed! Remainders: {tokens}\"\n",
    "            assert tokens_len == len(sentence_to_labels[sentence])\n",
    "            continue\n",
    "        \n",
    "        if sentence is None:\n",
    "            continue\n",
    "        # munge sentence word-by-word\n",
    "        row = line.split(\"\\t\")\n",
    "        _id, word, lemma, pos, ptb_pos, morphosyn_feats, dep_to, dep_label, _, notes = row\n",
    "        if _id.endswith(\".1\"):     # word not actually in sentence\n",
    "            continue\n",
    "\n",
    "        morphosyn_feats = _parse_morphosyn_feats(morphosyn_feats, pos)\n",
    "        if tokens[0].startswith(space_char):\n",
    "            word = f\" {word}\"\n",
    "        word_tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(word, add_special_tokens=False))\n",
    "        for token in word_tokens:\n",
    "            if token != tokens[0]:\n",
    "                if pos == \"PUNCT\":  # Try lookahead\n",
    "                    tokens, num_to_skip = _lookahead(lines, idx, word, tokens, sentence_to_labels, sentence_to_deps, morphosyn_feats, dep_label, max_lookahead=3)\n",
    "                    continue\n",
    "                elif idx != len(lines)-1 and len(lines[idx+1]) > 2 and lines[idx+1].split(\"\\t\")[3] == \"PART\":\n",
    "                    tokens, num_to_skip = _lookahead(lines, idx, word, tokens, sentence_to_labels, sentence_to_deps, morphosyn_feats, dep_label)\n",
    "                    continue\n",
    "                else:\n",
    "                    num_sents_skipped += 1\n",
    "                    del sentence_to_labels[sentence]\n",
    "                    del sentence_to_deps[sentence]\n",
    "                    sentence = None\n",
    "                    sentence_unhandled = True\n",
    "                    break\n",
    "                # raise Exception(f\"Mismatched token lists for sentence:\\n{sentence}\\nWord tokens: {word_tokens}\\nSent tokens: {tokens}\")\n",
    "            sentence_to_labels[sentence].append(morphosyn_feats)\n",
    "            sentence_to_deps[sentence].append(dep_label)\n",
    "            tokens.popleft()\n",
    "            # If we're at max_length, stop\n",
    "        #     if len(sentence_to_labels[sentence]) >= max_length:\n",
    "        #         break\n",
    "        # if sentence_unhandled:\n",
    "        #     sentence_unhandled = False\n",
    "        #     continue\n",
    "        # if len(sentence_to_labels[sentence]) >= max_length:\n",
    "        #     continue\n",
    "    \n",
    "    print(f\"Unhandled sentences: {num_sents_skipped} / {num_sents} ({num_sents_skipped / num_sents * 100:.2f}%)\")\n",
    "    return sentences, sentence_to_labels, sentence_to_deps\n",
    "\n",
    "def convert_to_dataset(sentences, tokenizer, max_length=128, num_datapoints=None):\n",
    "    if(num_datapoints):\n",
    "        split_sentences[:num_datapoints]\n",
    "    else:\n",
    "        split_sentences = sentences\n",
    "    df = pd.DataFrame(split_sentences)\n",
    "    dataset = Dataset.from_pandas(df.rename(columns={0: \"text\"}), split=\"train\")\n",
    "    tokenized_dataset = dataset.map(\n",
    "        lambda x: tokenizer(x[\"text\"], padding=True, truncation=True,\n",
    "                            max_length=max_length),\n",
    "        batched=True,\n",
    "    )\n",
    "    # ).filter(\n",
    "    #     lambda x: len(x['input_ids']) > max_length\n",
    "    # ).map(\n",
    "    #     lambda x: {'input_ids': x['input_ids'][:max_length]}\n",
    "    # )\n",
    "    return tokenized_dataset\n",
    "\n",
    "    # dataset = load_dataset(dataset_name, split=split_text).map(\n",
    "    #     lambda x: tokenizer(x['text']),\n",
    "    #     batched=True,\n",
    "    # ).filter(\n",
    "    #     lambda x: len(x['input_ids']) > max_length\n",
    "    # ).map(\n",
    "    #     lambda x: {'input_ids': x['input_ids'][:max_length]}\n",
    "    # )\n",
    "    # return dataset\n",
    "\n",
    "with open(\"data/ud/UD_English/en-ud-train.conllu\", 'r') as indata:\n",
    "    sentences, sentence_to_labels, sentence_to_deps = parse_and_load_text(indata, tokenizer, max_length=max_length)\n",
    "                                                        # space_char=\"Ġ\")\n",
    "dataset = convert_to_dataset(sentences, tokenizer, max_length=max_length)\n",
    "# dataset = download_dataset(dataset_name, tokenizer=tokenizer, max_length=max_seq_length, num_datapoints=7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 1596,\n",
       " 50276,\n",
       " 603,\n",
       " 573,\n",
       " 1872,\n",
       " 5830,\n",
       " 575,\n",
       " 573,\n",
       " 1758,\n",
       " 576,\n",
       " 187987,\n",
       " 575,\n",
       " 573,\n",
       " 8432,\n",
       " 685,\n",
       " 21240,\n",
       " 577,\n",
       " 573,\n",
       " 5086,\n",
       " 235290,\n",
       " 36622,\n",
       " 576,\n",
       " 573,\n",
       " 3170,\n",
       " 2330,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[15]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [01:10<00:00,  2.77it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "from baukit import Trace\n",
    "\n",
    "max_seq_length = 64\n",
    "\n",
    "def get_dictionary_activations(model, dataset, cache_name, max_seq_length, autoencoder, batch_size=32):\n",
    "    # num_features, d_model = autoencoder.encoder.weight.shape\n",
    "    num_features, d_model = params[\"W_enc\"].shape[1], params[\"W_enc\"].shape[0]\n",
    "    datapoints = dataset.num_rows\n",
    "    dictionary_activations = torch.zeros((datapoints*max_seq_length, num_features))\n",
    "    token_list = torch.zeros((datapoints*max_seq_length), dtype=torch.int64)\n",
    "    with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "        dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "        for i, batch in enumerate(tqdm(dl)):\n",
    "            batch = batch.to(model.device)\n",
    "            token_list[i*batch_size*max_seq_length:(i+1)*batch_size*max_seq_length] = rearrange(batch, \"b s -> (b s)\")\n",
    "            with Trace(model, cache_name) as ret:\n",
    "                _ = model(batch).logits\n",
    "                internal_activations = ret.output\n",
    "                # check if instance tuple\n",
    "                if(isinstance(internal_activations, tuple)):\n",
    "                    internal_activations = internal_activations[0]\n",
    "            batched_neuron_activations = rearrange(internal_activations, \"b s n -> (b s) n\" )\n",
    "            batched_dictionary_activations = autoencoder.encode(batched_neuron_activations)\n",
    "            dictionary_activations[i*batch_size*max_seq_length:(i+1)*batch_size*max_seq_length,:] = batched_dictionary_activations.cpu()\n",
    "    return dictionary_activations, token_list\n",
    "\n",
    "batch_size = 64\n",
    "dictionary_activations, tokens_for_each_datapoint = get_dictionary_activations(model, dataset, submodule_name, max_seq_length, ae, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_seqs = int(dictionary_activations.shape[0] / max_length)\n",
    "num_feats = dictionary_activations.shape[-1]\n",
    "dictionary_activations = dictionary_activations.reshape((num_seqs, max_length, num_feats))\n",
    "\n",
    "tokens_for_each_datapoint = tokens_for_each_datapoint.reshape((num_seqs, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_freqs = defaultdict(int)\n",
    "for sentence in sentence_to_labels:\n",
    "    morphosyn_feats = sentence_to_labels[sentence]\n",
    "    dep_feats = sentence_to_deps[sentence]\n",
    "    for i in range(len(morphosyn_feats)):\n",
    "        for feat in morphosyn_feats[i]:\n",
    "            feature_freqs[feat] += 1\n",
    "        feature_freqs[dep_feats[i]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49fa098800a24fac813fc60847add29d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Examples:   0%|          | 0/12460 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def feature_precisions(dictionary_activations, tokens, feature_idx, sentences, sentence_to_labels):\n",
    "    morphosyn_acts = defaultdict(float)\n",
    "    dep_acts = defaultdict(float)\n",
    "    \n",
    "    for idx, sentence in tqdm(enumerate(sentences), total=len(sentences), desc=\"Examples\"):\n",
    "        if sentence not in sentence_to_labels:\n",
    "            continue\n",
    "        sentence_len = len(sentence_to_labels[sentence])\n",
    "        tokens_sent = tokens[idx]\n",
    "        if tokens_sent[0] == 2:\n",
    "            idx_offset = 1\n",
    "        else:\n",
    "            idx_offset = 0\n",
    "        dictionary_sent_acts = dictionary_activations[idx, idx_offset : sentence_len+idx_offset, feature_idx]\n",
    "        \n",
    "        nonzero_idxs = dictionary_sent_acts.nonzero().flatten().tolist()\n",
    "        morphosyn_feats = [sentence_to_labels[sentence][j] for j in nonzero_idxs]\n",
    "        dep_feats = [sentence_to_deps[sentence][j] for j in nonzero_idxs]\n",
    "        for j, feat_list in enumerate(morphosyn_feats):\n",
    "            dep_label = dep_feats[j]\n",
    "            for feat in feat_list:\n",
    "                morphosyn_acts[feat] += dictionary_sent_acts[nonzero_idxs[j]].item()\n",
    "            dep_acts[dep_label] += dictionary_sent_acts[nonzero_idxs[j]].item()\n",
    "    \n",
    "    for feat in morphosyn_acts:\n",
    "        morphosyn_acts[feat] /= dictionary_activations.shape[0]\n",
    "    for dep in dep_acts:\n",
    "        dep_acts[dep] /= dictionary_activations.shape[0]\n",
    "\n",
    "    return morphosyn_acts, dep_acts\n",
    "\n",
    "morphosyn_acts, dep_acts = feature_precisions(dictionary_activations, tokens_for_each_datapoint, 3883, sentences, sentence_to_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('punct', 72.62273650682182),\n",
       " ('nsubj', 62.049668940609955),\n",
       " ('root', 54.59590063202247),\n",
       " ('obl', 42.58081109550562),\n",
       " ('compound', 42.41500175561798),\n",
       " ('obj', 38.44035162520064),\n",
       " ('nmod', 36.894063503210276),\n",
       " ('case', 36.42236531902087),\n",
       " ('det', 28.52585899879615),\n",
       " ('conj', 25.323205507624397),\n",
       " ('amod', 24.603417184991976),\n",
       " ('advmod', 24.106315208667738),\n",
       " ('nummod', 22.44045570826645),\n",
       " ('aux', 17.57957338483146),\n",
       " ('cc', 15.14309540529695),\n",
       " ('appos', 14.464808637640449),\n",
       " ('mark', 13.27491974317817),\n",
       " ('flat', 11.193316111556982),\n",
       " ('cop', 9.043392606340289),\n",
       " ('nmod:poss', 8.709218248394864),\n",
       " ('advcl', 8.551315459470304),\n",
       " ('xcomp', 7.389261888041734),\n",
       " ('list', 6.3388794141252),\n",
       " ('ccomp', 5.584127959470305),\n",
       " ('acl:relcl', 5.505793539325842),\n",
       " ('nsubj:pass', 4.545368930577849),\n",
       " ('parataxis', 4.344797100722311),\n",
       " ('acl', 4.020584620786517),\n",
       " ('discourse', 3.4201921147672554),\n",
       " ('aux:pass', 2.3526798254414127),\n",
       " ('goeswith', 2.3021706962279294),\n",
       " ('obl:tmod', 2.2562211577046547),\n",
       " ('expl', 2.22990569823435),\n",
       " ('nmod:tmod', 2.2249962379614767),\n",
       " ('compound:prt', 1.8958655196629213),\n",
       " ('obl:npmod', 1.5091129113162118),\n",
       " ('iobj', 0.9360353130016051),\n",
       " ('vocative', 0.7882047552166934),\n",
       " ('fixed', 0.7768935593900481),\n",
       " ('csubj', 0.684484099117175),\n",
       " ('nmod:npmod', 0.618420194622793),\n",
       " ('det:predet', 0.2947858146067416),\n",
       " ('cc:preconj', 0.1996902588282504),\n",
       " ('orphan', 0.07925486556982343),\n",
       " ('reparandum', 0.06627207062600321),\n",
       " ('flat:foreign', 0.06428571428571428),\n",
       " ('dep', 0.06127357544141252),\n",
       " ('csubj:pass', 0.013412921348314608),\n",
       " ('dislocated', 0.011230939004815409)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(dep_acts.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NOUN:Number_Sing', 115.8342621388443),\n",
       " ('PROPN:Number_Sing', 79.06908482142858),\n",
       " ('NUM:NumType_Card', 47.24079429173355),\n",
       " ('PRON:PronType_Prs', 45.21140273876404),\n",
       " ('NOUN:Number_Plur', 38.85983396869984),\n",
       " ('PRON:Number_Sing', 33.19192164927769),\n",
       " ('PRON:Case_Nom', 32.79439205457464),\n",
       " ('ADJ:Degree_Pos', 31.908334169341895),\n",
       " ('AUX:VerbForm_Fin', 27.078156350321027),\n",
       " ('DET:PronType_Art', 23.44992350521669),\n",
       " ('PRON:Person_1', 22.584531751605137),\n",
       " ('VERB:VerbForm_Fin', 22.191040078250403),\n",
       " ('VERB:Tense_Past', 18.744511185794543),\n",
       " ('AUX:Mood_Ind', 18.6844878611557),\n",
       " ('VERB:Mood_Ind', 17.9780547752809),\n",
       " ('DET:Definite_Def', 17.83414551565008),\n",
       " ('PRON:Person_3', 16.518686045345106),\n",
       " ('AUX:Tense_Pres', 14.65222838081862),\n",
       " ('VERB:VerbForm_Inf', 13.478255417335474),\n",
       " ('VERB:VerbForm_Part', 12.68010132423756),\n",
       " ('VERB:Tense_Pres', 11.913644913723916),\n",
       " ('PRON:Number_Plur', 9.511666081460675),\n",
       " ('AUX:Number_Sing', 9.497331460674157),\n",
       " ('AUX:Person_3', 9.08046749598716),\n",
       " ('PRON:Case_Acc', 6.895985904895666),\n",
       " ('PRON:Gender_Neut', 6.236467947431782),\n",
       " ('PRON:Person_2', 6.108184941813804),\n",
       " ('DET:Definite_Ind', 5.615777989566613),\n",
       " ('VERB:VerbForm_Ger', 5.554333868378812),\n",
       " ('PRON:Poss_Yes', 5.42649352929374),\n",
       " ('VERB:Number_Sing', 4.626230186597111),\n",
       " ('VERB:Person_3', 4.588188453049759),\n",
       " ('AUX:Tense_Past', 4.396946478731942),\n",
       " ('VERB:Mood_Imp', 4.212985302969503),\n",
       " ('PRON:Gender_Masc', 4.148946629213484),\n",
       " ('VERB:Voice_Pass', 3.697986055377207),\n",
       " ('PROPN:Number_Plur', 3.4763116974317816),\n",
       " ('DET:PronType_Dem', 2.7603155096308187),\n",
       " ('PRON:PronType_Dem', 2.379263643659711),\n",
       " ('DET:Number_Sing', 2.248927819020867),\n",
       " ('PRON:PronType_Rel', 2.1981039325842695),\n",
       " ('ADV:PronType_Int', 2.0941462680577847),\n",
       " ('ADV:PronType_Dem', 2.0323083868378813),\n",
       " ('PRON:Gender_Fem', 1.4456586075441413),\n",
       " ('AUX:VerbForm_Inf', 1.4209482845104333),\n",
       " ('PRON:PronType_Int', 1.3654920746388444),\n",
       " ('ADV:Degree_Pos', 1.0608108447030498),\n",
       " ('ADJ:Degree_Sup', 1.060103581460674),\n",
       " ('ADJ:Degree_Cmp', 1.0427543138041733),\n",
       " ('AUX:Mood_Imp', 0.6506495786516854),\n",
       " ('ADJ:NumType_Ord', 0.576877257223114),\n",
       " ('DET:Number_Plur', 0.5093712379614768),\n",
       " ('AUX:Person_1', 0.414569121187801),\n",
       " ('AUX:VerbForm_Part', 0.36468699839486357),\n",
       " ('ADV:Degree_Cmp', 0.330633025682183),\n",
       " ('DET:PronType_Int', 0.25930602929373997),\n",
       " ('PRON:Reflex_Yes', 0.2570575842696629),\n",
       " ('AUX:VerbForm_Ger', 0.20065584871589084),\n",
       " ('ADV:PronType_Rel', 0.15626630216693418),\n",
       " ('ADV:NumType_Mult', 0.14361205858747994),\n",
       " ('ADV:Degree_Sup', 0.13547476926163723),\n",
       " ('SYM:Number_Sing', 0.12020214686998394),\n",
       " ('X:Foreign_Yes', 0.09374372993579454),\n",
       " ('X:Number_Sing', 0.025699739165329052),\n",
       " ('NUM:Number_Sing', 0.019587680577849115),\n",
       " ('ADJ:Number_Sing', 0.016226926163723916),\n",
       " ('VERB:Person_1', 0.012744532504012841),\n",
       " ('VERB:Number_Plur', 0.01103029695024077),\n",
       " ('NOUN:Degree_Pos', 0.010684189406099519),\n",
       " ('PROPN:Degree_Pos', 0.010094803370786517),\n",
       " ('DET:PronType_Rel', 0.006826845906902087),\n",
       " ('ADP:Number_Sing', 0.004885634028892456),\n",
       " ('ADV:NumType_Ord', 0.0011160714285714285)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(morphosyn_acts.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "features_list = list(sentence_to_labels.values())\n",
    "unique_features = set()\n",
    "for dict_list in features_list:\n",
    "    for dict in dict_list:\n",
    "        unique_features.update([f\"{k}_{v}\" for k, v in dict.items()])\n",
    "print(len(unique_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "othello",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
