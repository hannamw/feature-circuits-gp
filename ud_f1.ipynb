{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f190fd1860744fc4893cd2342bc546d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from dictionary_learning.interp import examine_dimension\n",
    "from dictionary_learning.utils import zst_to_generator\n",
    "import torch as t\n",
    "import gc\n",
    "import numpy as np\n",
    "from dictionary_learning.dictionary import GatedAutoEncoder, JumpReLUSAE\n",
    "from dictionary_learning.buffer import ActivationBuffer\n",
    "from sae_lens import SparseAutoencoder\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_name = \"google/gemma-2-2b\"\n",
    "# model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=t.float16,\n",
    "                             device_map=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"right\")\n",
    "layer = 13\n",
    "submodule_name = f\"model.layers.{layer}\"\n",
    "submodule = model.model.layers[layer]\n",
    "\n",
    "dictionaries = {}\n",
    "# ae = GatedAutoEncoder(4096, 32768).half().to(\"cuda\")\n",
    "# ae.load_state_dict(t.load(f'llama_saes/layer{layer}/ae_81920.pt'))\n",
    "# dictionaries[submodule] = ae\n",
    "path_to_params = hf_hub_download(\n",
    "    repo_id=\"google/gemma-scope-2b-pt-res\",\n",
    "    filename=f\"layer_{layer}/width_16k/canonical/params.npz\",\n",
    "    force_download=False,\n",
    ")\n",
    "params = np.load(path_to_params)\n",
    "pt_params = {k: t.from_numpy(v).cuda() for k, v in params.items()}\n",
    "ae = JumpReLUSAE(params[\"W_enc\"].shape[0], params[\"W_enc\"].shape[1]).to(\"cuda\")\n",
    "ae.load_state_dict(pt_params)\n",
    "ae = ae.half()\n",
    "dictionaries[submodule] = ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1596, 603, 476, 2067, 235265]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"This is a string.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁is']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenizer.encode(\" is\", add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unhandled sentences: 306 / 12460 (2.46%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89fd17b2259c41efb63a004060a54c02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from collections import deque\n",
    "from dictionary_learning.interp_utils import *\n",
    "\n",
    "max_length = 64\n",
    "\n",
    "def parse_and_load_text(indata, tokenizer, max_length=128, space_char=\"▁\"):\n",
    "    def _parse_morphosyn_feats(morphosyn_str, pos):\n",
    "        if morphosyn_str == \"_\":\n",
    "            return []\n",
    "        \n",
    "        features = []\n",
    "        if \"|\" in morphosyn_str:\n",
    "            morphosyn_list = morphosyn_str.split(\"|\")\n",
    "        else:\n",
    "            morphosyn_list = [morphosyn_str]\n",
    "        for feature in morphosyn_list:\n",
    "            name, value = feature.split(\"=\")\n",
    "            features.append(f\"{pos}:{name}_{value}\")\n",
    "        return features\n",
    "\n",
    "    def _lookahead(lines, idx, word, tokens, sentence_to_labels, sentence_to_deps, morphosyn_feats, dep_label, max_lookahead=1):\n",
    "        lookahead = 1\n",
    "        while lookahead <= max_lookahead:\n",
    "            matched = True\n",
    "            next_word = lines[idx+lookahead].split(\"\\t\")[1]\n",
    "            word = f\"{word}{next_word}\"\n",
    "            word_tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(word, add_special_tokens=False))\n",
    "            # nested munging\n",
    "            for word_token in word_tokens:\n",
    "                if word_token == tokens[0]:\n",
    "                    sentence_to_labels[sentence].append(morphosyn_feats)\n",
    "                    sentence_to_deps[sentence].append(dep_label)\n",
    "                    tokens.popleft()\n",
    "                else:\n",
    "                    lookahead += 1\n",
    "                    matched = False\n",
    "                    break\n",
    "            if matched:\n",
    "                break\n",
    "        return tokens, lookahead\n",
    "\n",
    "    sentences = []\n",
    "    sentence_to_labels = {}\n",
    "    sentence_to_deps = {}\n",
    "    lines = indata.readlines()\n",
    "    num_to_skip = 0\n",
    "    num_sents = 0\n",
    "    num_sents_skipped = 0\n",
    "    sentence_unhandled = False\n",
    "    for idx, line in enumerate(lines):\n",
    "        if num_to_skip > 0:\n",
    "            num_to_skip -= 1\n",
    "            continue\n",
    "\n",
    "        if line.startswith(\"# text\"):\n",
    "            sentence = line.strip().split(\"# text = \")[1]\n",
    "            if sentence.startswith(\"http\") or sentence == \"Smokers Haven\":\n",
    "                sentence = None     # skip this one\n",
    "                num_sents_skipped += 1\n",
    "                continue\n",
    "            sentence = sentence.replace(u\"\\xa0\", \" \").replace(\"  \", \" \")\n",
    "            num_sents += 1\n",
    "            sentences.append(sentence)\n",
    "            tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(sentence, add_special_tokens=False))\n",
    "            tokens_len = len(tokens)\n",
    "            tokens = deque(tokens)  # deques can pop from left much more efficiently than lists\n",
    "            sentence_to_labels[sentence] = []\n",
    "            sentence_to_deps[sentence] = []\n",
    "            continue\n",
    "        elif line.startswith(\"# \"):\n",
    "            continue\n",
    "        elif len(line) < 2:     # Empty line means end-of-sentence\n",
    "            if sentence is None:\n",
    "                continue\n",
    "            assert len(tokens) == 0, f\"Not all tokens have been processed! Remainders: {tokens}\"\n",
    "            assert tokens_len == len(sentence_to_labels[sentence])\n",
    "            continue\n",
    "        \n",
    "        if sentence is None:\n",
    "            continue\n",
    "        # munge sentence word-by-word\n",
    "        row = line.split(\"\\t\")\n",
    "        _id, word, lemma, pos, ptb_pos, morphosyn_feats, dep_to, dep_label, _, notes = row\n",
    "        if _id.endswith(\".1\"):     # word not actually in sentence\n",
    "            continue\n",
    "\n",
    "        morphosyn_feats = _parse_morphosyn_feats(morphosyn_feats, pos)\n",
    "        if tokens[0].startswith(space_char):\n",
    "            word = f\" {word}\"\n",
    "        word_tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(word, add_special_tokens=False))\n",
    "        for token in word_tokens:\n",
    "            if token != tokens[0]:\n",
    "                if pos == \"PUNCT\":  # Try lookahead\n",
    "                    tokens, num_to_skip = _lookahead(lines, idx, word, tokens, sentence_to_labels, sentence_to_deps, morphosyn_feats, dep_label, max_lookahead=3)\n",
    "                    continue\n",
    "                elif idx != len(lines)-1 and len(lines[idx+1]) > 2 and lines[idx+1].split(\"\\t\")[3] == \"PART\":\n",
    "                    tokens, num_to_skip = _lookahead(lines, idx, word, tokens, sentence_to_labels, sentence_to_deps, morphosyn_feats, dep_label)\n",
    "                    continue\n",
    "                else:\n",
    "                    num_sents_skipped += 1\n",
    "                    del sentence_to_labels[sentence]\n",
    "                    del sentence_to_deps[sentence]\n",
    "                    sentence = None\n",
    "                    sentence_unhandled = True\n",
    "                    break\n",
    "                # raise Exception(f\"Mismatched token lists for sentence:\\n{sentence}\\nWord tokens: {word_tokens}\\nSent tokens: {tokens}\")\n",
    "            sentence_to_labels[sentence].append(morphosyn_feats)\n",
    "            sentence_to_deps[sentence].append(dep_label)\n",
    "            tokens.popleft()\n",
    "            # If we're at max_length, stop\n",
    "        #     if len(sentence_to_labels[sentence]) >= max_length:\n",
    "        #         break\n",
    "        # if sentence_unhandled:\n",
    "        #     sentence_unhandled = False\n",
    "        #     continue\n",
    "        # if len(sentence_to_labels[sentence]) >= max_length:\n",
    "        #     continue\n",
    "    \n",
    "    print(f\"Unhandled sentences: {num_sents_skipped} / {num_sents} ({num_sents_skipped / num_sents * 100:.2f}%)\")\n",
    "    return sentences, sentence_to_labels, sentence_to_deps\n",
    "\n",
    "def convert_to_dataset(sentences, tokenizer, max_length=128, num_datapoints=None):\n",
    "    if(num_datapoints):\n",
    "        split_sentences[:num_datapoints]\n",
    "    else:\n",
    "        split_sentences = sentences\n",
    "    df = pd.DataFrame(split_sentences)\n",
    "    dataset = Dataset.from_pandas(df.rename(columns={0: \"text\"}), split=\"train\")\n",
    "    tokenized_dataset = dataset.map(\n",
    "        lambda x: tokenizer(x[\"text\"], padding=True, truncation=True,\n",
    "                            max_length=max_length),\n",
    "        batched=True,\n",
    "    )\n",
    "    # ).filter(\n",
    "    #     lambda x: len(x['input_ids']) > max_length\n",
    "    # ).map(\n",
    "    #     lambda x: {'input_ids': x['input_ids'][:max_length]}\n",
    "    # )\n",
    "    return tokenized_dataset\n",
    "\n",
    "    # dataset = load_dataset(dataset_name, split=split_text).map(\n",
    "    #     lambda x: tokenizer(x['text']),\n",
    "    #     batched=True,\n",
    "    # ).filter(\n",
    "    #     lambda x: len(x['input_ids']) > max_length\n",
    "    # ).map(\n",
    "    #     lambda x: {'input_ids': x['input_ids'][:max_length]}\n",
    "    # )\n",
    "    # return dataset\n",
    "\n",
    "with open(\"data/ud/UD_English/en-ud-train.conllu\", 'r') as indata:\n",
    "    sentences, sentence_to_labels, sentence_to_deps = parse_and_load_text(indata, tokenizer, max_length=max_length)\n",
    "                                                        # space_char=\"Ġ\")\n",
    "dataset = convert_to_dataset(sentences, tokenizer, max_length=max_length)\n",
    "# dataset = download_dataset(dataset_name, tokenizer=tokenizer, max_length=max_seq_length, num_datapoints=7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 1596,\n",
       " 50276,\n",
       " 603,\n",
       " 573,\n",
       " 1872,\n",
       " 5830,\n",
       " 575,\n",
       " 573,\n",
       " 1758,\n",
       " 576,\n",
       " 187987,\n",
       " 575,\n",
       " 573,\n",
       " 8432,\n",
       " 685,\n",
       " 21240,\n",
       " 577,\n",
       " 573,\n",
       " 5086,\n",
       " 235290,\n",
       " 36622,\n",
       " 576,\n",
       " 573,\n",
       " 3170,\n",
       " 2330,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[15]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [01:10<00:00,  2.77it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "from baukit import Trace\n",
    "\n",
    "max_seq_length = 64\n",
    "\n",
    "def get_dictionary_activations(model, dataset, cache_name, max_seq_length, autoencoder, batch_size=32):\n",
    "    # num_features, d_model = autoencoder.encoder.weight.shape\n",
    "    num_features, d_model = params[\"W_enc\"].shape[1], params[\"W_enc\"].shape[0]\n",
    "    datapoints = dataset.num_rows\n",
    "    dictionary_activations = torch.zeros((datapoints*max_seq_length, num_features))\n",
    "    token_list = torch.zeros((datapoints*max_seq_length), dtype=torch.int64)\n",
    "    with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "        dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "        for i, batch in enumerate(tqdm(dl)):\n",
    "            batch = batch.to(model.device)\n",
    "            token_list[i*batch_size*max_seq_length:(i+1)*batch_size*max_seq_length] = rearrange(batch, \"b s -> (b s)\")\n",
    "            with Trace(model, cache_name) as ret:\n",
    "                _ = model(batch).logits\n",
    "                internal_activations = ret.output\n",
    "                # check if instance tuple\n",
    "                if(isinstance(internal_activations, tuple)):\n",
    "                    internal_activations = internal_activations[0]\n",
    "            batched_neuron_activations = rearrange(internal_activations, \"b s n -> (b s) n\" )\n",
    "            batched_dictionary_activations = autoencoder.encode(batched_neuron_activations)\n",
    "            dictionary_activations[i*batch_size*max_seq_length:(i+1)*batch_size*max_seq_length,:] = batched_dictionary_activations.cpu()\n",
    "    return dictionary_activations, token_list\n",
    "\n",
    "batch_size = 64\n",
    "dictionary_activations, tokens_for_each_datapoint = get_dictionary_activations(model, dataset, submodule_name, max_seq_length, ae, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_seqs = int(dictionary_activations.shape[0] / max_length)\n",
    "num_feats = dictionary_activations.shape[-1]\n",
    "dictionary_activations = dictionary_activations.reshape((num_seqs, max_length, num_feats))\n",
    "\n",
    "tokens_for_each_datapoint = tokens_for_each_datapoint.reshape((num_seqs, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c297252b494100a641d28cef410d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Examples:   0%|          | 0/12460 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def feature_precisions(dictionary_activations, tokens, feature_idx, sentences, sentence_to_labels):\n",
    "    morphosyn_acts = defaultdict(float)\n",
    "    dep_acts = defaultdict(float)\n",
    "    \n",
    "    for idx, sentence in tqdm(enumerate(sentences), total=len(sentences), desc=\"Examples\"):\n",
    "        if sentence not in sentence_to_labels:\n",
    "            continue\n",
    "        dictionary_sent_acts = dictionary_activations[idx, :, feature_idx]\n",
    "        tokens_sent = tokens[idx]\n",
    "        if tokens_sent[0] == 2:\n",
    "            idx_offset = 1\n",
    "        else:\n",
    "            idx_offset = 0\n",
    "        \n",
    "        nonzero_idxs = dictionary_sent_acts.nonzero().flatten().tolist()\n",
    "        morphosyn_feats = [sentence_to_labels[sentence][j - idx_offset] for j in nonzero_idxs]\n",
    "        dep_feats = [sentence_to_deps[sentence][j - idx_offset] for j in nonzero_idxs]\n",
    "        for j, feat_list in enumerate(morphosyn_feats):\n",
    "            dep_label = dep_feats[j]\n",
    "            for feat in feat_list:\n",
    "                morphosyn_acts[feat] += dictionary_sent_acts[nonzero_idxs[j]].item()\n",
    "            dep_acts[dep_label] += dictionary_sent_acts[nonzero_idxs[j]].item()\n",
    "\n",
    "    return morphosyn_acts\n",
    "\n",
    "morphosyn_acts = feature_precisions(dictionary_activations, tokens_for_each_datapoint, 10620, sentences, sentence_to_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NOUN:Number_Sing', 289070.765625),\n",
       " ('NOUN:Number_Plur', 114875.42578125),\n",
       " ('PROPN:Number_Sing', 76135.7421875),\n",
       " ('ADJ:Degree_Pos', 51599.08203125),\n",
       " ('PRON:PronType_Prs', 31472.89453125),\n",
       " ('PRON:Case_Acc', 28534.890625),\n",
       " ('VERB:Tense_Past', 26732.5),\n",
       " ('PRON:Number_Sing', 24270.69921875),\n",
       " ('VERB:VerbForm_Part', 22911.6796875),\n",
       " ('NUM:NumType_Card', 20824.1796875),\n",
       " ('VERB:VerbForm_Inf', 18637.8515625),\n",
       " ('PRON:Person_3', 18150.828125),\n",
       " ('VERB:VerbForm_Fin', 15897.0078125),\n",
       " ('VERB:Mood_Ind', 15197.0625),\n",
       " ('VERB:Tense_Pres', 11376.2421875),\n",
       " ('VERB:Voice_Pass', 10826.13671875),\n",
       " ('PRON:Gender_Neut', 9074.59375),\n",
       " ('PRON:Person_1', 7381.31640625),\n",
       " ('PRON:Number_Plur', 6350.8046875),\n",
       " ('ADV:PronType_Dem', 6273.48828125),\n",
       " ('PRON:Person_2', 5940.75),\n",
       " ('PROPN:Number_Plur', 4920.89453125),\n",
       " ('VERB:VerbForm_Ger', 3644.49609375),\n",
       " ('VERB:Number_Sing', 3555.01171875),\n",
       " ('ADV:Degree_Pos', 3530.15625),\n",
       " ('VERB:Person_3', 3508.49609375),\n",
       " ('PRON:Gender_Masc', 3244.7421875),\n",
       " ('ADJ:Degree_Cmp', 2867.05078125),\n",
       " ('AUX:VerbForm_Fin', 2427.015625),\n",
       " ('PRON:PronType_Dem', 2082.61328125),\n",
       " ('PRON:Case_Nom', 1990.02734375),\n",
       " ('AUX:Mood_Ind', 1494.03125),\n",
       " ('ADV:Degree_Cmp', 1263.640625),\n",
       " ('PRON:Reflex_Yes', 1146.76171875),\n",
       " ('AUX:Tense_Pres', 1076.18359375),\n",
       " ('PRON:Gender_Fem', 1032.9140625),\n",
       " ('DET:PronType_Dem', 880.12890625),\n",
       " ('ADJ:Degree_Sup', 830.44140625),\n",
       " ('AUX:Number_Sing', 824.3984375),\n",
       " ('DET:Number_Sing', 749.234375),\n",
       " ('AUX:Person_3', 743.7265625),\n",
       " ('VERB:Mood_Imp', 699.9453125),\n",
       " ('AUX:Tense_Past', 453.48828125),\n",
       " ('ADV:NumType_Mult', 407.875),\n",
       " ('AUX:VerbForm_Inf', 353.5390625),\n",
       " ('PRON:Poss_Yes', 315.02734375),\n",
       " ('ADV:Degree_Sup', 274.046875),\n",
       " ('ADV:PronType_Int', 172.58984375),\n",
       " ('DET:Number_Plur', 130.89453125),\n",
       " ('PRON:PronType_Int', 117.21875),\n",
       " ('AUX:Person_1', 80.671875),\n",
       " ('ADJ:NumType_Ord', 67.30859375),\n",
       " ('SYM:Number_Sing', 53.9296875),\n",
       " ('VERB:Number_Plur', 45.8125),\n",
       " ('X:Foreign_Yes', 41.40625),\n",
       " ('AUX:VerbForm_Part', 35.640625),\n",
       " ('DET:PronType_Int', 33.203125),\n",
       " ('VERB:Person_1', 29.09375),\n",
       " ('NOUN:Degree_Pos', 18.109375),\n",
       " ('ADJ:Number_Sing', 14.171875),\n",
       " ('X:Number_Sing', 7.94140625),\n",
       " ('PRON:PronType_Rel', 5.5)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(morphosyn_acts.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "features_list = list(sentence_to_labels.values())\n",
    "unique_features = set()\n",
    "for dict_list in features_list:\n",
    "    for dict in dict_list:\n",
    "        unique_features.update([f\"{k}_{v}\" for k, v in dict.items()])\n",
    "print(len(unique_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "othello",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
