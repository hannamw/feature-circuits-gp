{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/u/smarks/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Matplotlib created a temporary cache directory at /tmp/matplotlib-oygbwsli because the default path (/share/u/smarks/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "parent_dir = os.path.abspath('..')\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from nnsight import LanguageModel\n",
    "import torch as t\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from attribution import patching_effect\n",
    "from dictionary_learning import AutoEncoder, ActivationBuffer\n",
    "from dictionary_learning.dictionary import IdentityDict\n",
    "from dictionary_learning.interp import examine_dimension\n",
    "from dictionary_learning.utils import hf_dataset_to_generator\n",
    "from tqdm import tqdm\n",
    "from activation_utils import SparseAct\n",
    "import gc\n",
    "\n",
    "DEBUGGING = False\n",
    "\n",
    "if DEBUGGING:\n",
    "    tracer_kwargs = dict(scan=True, validate=True)\n",
    "else:\n",
    "    tracer_kwargs = dict(scan=False, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# dataset hyperparameters\n",
    "dataset = load_dataset(\"LabHC/bias_in_bios\")\n",
    "profession_dict = {'professor' : 21, 'nurse' : 13}\n",
    "male_prof = 'professor'\n",
    "female_prof = 'nurse'\n",
    "\n",
    "# model hyperparameters\n",
    "DEVICE = 'cuda:0'\n",
    "model = LanguageModel('EleutherAI/pythia-70m-deduped', device_map=DEVICE, dispatch=True)\n",
    "activation_dim = 512\n",
    "layer = 4\n",
    "\n",
    "# dictionary hyperparameters\n",
    "dict_id = 10\n",
    "expansion_factor = 64\n",
    "dictionary_size = expansion_factor * activation_dim\n",
    "\n",
    "# data preparation hyperparameters\n",
    "batch_size = 1024\n",
    "SEED = 42\n",
    "\n",
    "def get_data(train=True, ambiguous=True, batch_size=128, seed=SEED):\n",
    "    if train:\n",
    "        data = dataset['train']\n",
    "    else:\n",
    "        data = dataset['test']\n",
    "    if ambiguous:\n",
    "        neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        n = min([len(neg), len(pos)])\n",
    "        neg, pos = neg[:n], pos[:n]\n",
    "        data = neg + pos\n",
    "        labels = [0]*n + [1]*n\n",
    "        idxs = list(range(2*n))\n",
    "        random.Random(seed).shuffle(idxs)\n",
    "        data, labels = [data[i] for i in idxs], [labels[i] for i in idxs]\n",
    "        true_labels = spurious_labels = labels\n",
    "    else:\n",
    "        neg_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        neg_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 1]\n",
    "        pos_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 0]\n",
    "        pos_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        n = min([len(neg_neg), len(neg_pos), len(pos_neg), len(pos_pos)])\n",
    "        neg_neg, neg_pos, pos_neg, pos_pos = neg_neg[:n], neg_pos[:n], pos_neg[:n], pos_pos[:n]\n",
    "        data = neg_neg + neg_pos + pos_neg + pos_pos\n",
    "        true_labels     = [0]*n + [0]*n + [1]*n + [1]*n\n",
    "        spurious_labels = [0]*n + [1]*n + [0]*n + [1]*n\n",
    "        idxs = list(range(4*n))\n",
    "        random.Random(seed).shuffle(idxs)\n",
    "        data, true_labels, spurious_labels = [data[i] for i in idxs], [true_labels[i] for i in idxs], [spurious_labels[i] for i in idxs]\n",
    "\n",
    "    batches = [\n",
    "        (data[i:i+batch_size], t.tensor(true_labels[i:i+batch_size], device=DEVICE), t.tensor(spurious_labels[i:i+batch_size], device=DEVICE)) for i in range(0, len(data), batch_size)\n",
    "    ]\n",
    "\n",
    "    return batches\n",
    "\n",
    "def get_subgroups(train=True, ambiguous=True, batch_size=128, seed=SEED):\n",
    "    if train:\n",
    "        data = dataset['train']\n",
    "    else:\n",
    "        data = dataset['test']\n",
    "    if ambiguous:\n",
    "        neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        neg_labels, pos_labels = (0, 0), (1, 1)\n",
    "        subgroups = [(neg, neg_labels), (pos, pos_labels)]\n",
    "    else:\n",
    "        neg_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        neg_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 1]\n",
    "        pos_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 0]\n",
    "        pos_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        neg_neg_labels, neg_pos_labels, pos_neg_labels, pos_pos_labels = (0, 0), (0, 1), (1, 0), (1, 1)\n",
    "        subgroups = [(neg_neg, neg_neg_labels), (neg_pos, neg_pos_labels), (pos_neg, pos_neg_labels), (pos_pos, pos_pos_labels)]\n",
    "    \n",
    "    out = {}\n",
    "    for data, label_profile in subgroups:\n",
    "        out[label_profile] = []\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            text = data[i:i+batch_size]\n",
    "            out[label_profile].append(\n",
    "                (\n",
    "                    text,\n",
    "                    t.tensor([label_profile[0]]*len(text), device=DEVICE),\n",
    "                    t.tensor([label_profile[1]]*len(text), device=DEVICE)\n",
    "                )\n",
    "            )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probe training hyperparameters\n",
    "class Probe(nn.Module):\n",
    "    def __init__(self, activation_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(activation_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "def train_probe(get_acts, label_idx=0, batches=get_data(), lr=1e-2, epochs=1, dim=512, seed=SEED):\n",
    "    t.manual_seed(seed)\n",
    "    probe = Probe(dim).to('cuda:0')\n",
    "    optimizer = t.optim.AdamW(probe.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for batch in batches:\n",
    "            text = batch[0]\n",
    "            labels = batch[label_idx+1] \n",
    "            acts = get_acts(text)\n",
    "            logits = probe(acts)\n",
    "            loss = criterion(logits, labels.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return probe, losses\n",
    "\n",
    "def test_probe(probe, get_acts, label_idx=0, batches=get_data(train=False), seed=SEED):\n",
    "    with t.no_grad():\n",
    "        corrects = []\n",
    "\n",
    "        for batch in batches:\n",
    "            text = batch[0]\n",
    "            labels = batch[label_idx+1]\n",
    "            acts = get_acts(text)\n",
    "            logits = probe(acts)\n",
    "            preds = (logits > 0.0).long()\n",
    "            corrects.append((preds == labels).float())\n",
    "        return t.cat(corrects).mean().item()\n",
    "    \n",
    "def get_acts(text):\n",
    "    with t.no_grad(): \n",
    "        with model.trace(text, **tracer_kwargs):\n",
    "            attn_mask = model.input[1]['attention_mask']\n",
    "            acts = model.gpt_neox.layers[layer].output[0]\n",
    "            acts = acts * attn_mask[:, :, None]\n",
    "            acts = acts.sum(1) / attn_mask.sum(1)[:, None]\n",
    "            acts = acts.save()\n",
    "        return acts.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambiguous test accuracy 0.9318076372146606\n",
      "Oracle probe\n",
      "ground truth accuracy: 0.9302995204925537\n",
      "unintended feature accuracy: 0.49366357922554016\n"
     ]
    }
   ],
   "source": [
    "oracle, _ = train_probe(get_acts, label_idx=0, batches=get_data(ambiguous=False))\n",
    "print(\"ambiguous test accuracy\", test_probe(oracle, get_acts, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print(\"ground truth accuracy:\", test_probe(oracle, get_acts, batches=batches, label_idx=0))\n",
    "print(\"unintended feature accuracy:\", test_probe(oracle, get_acts, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.9468682408332825\n",
      "Accuracy for (0, 1): 0.926398754119873\n",
      "Accuracy for (1, 0): 0.9239631295204163\n",
      "Accuracy for (1, 1): 0.9189126491546631\n"
     ]
    }
   ],
   "source": [
    "# get worst-group accuracy of oracle probe\n",
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(oracle, get_acts, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous test accuracy: 0.9955855011940002\n",
      "Ground truth accuracy: 0.6186636090278625\n",
      "Unintended feature accuracy: 0.8744239807128906\n"
     ]
    }
   ],
   "source": [
    "probe, _ = train_probe(get_acts, label_idx=0)\n",
    "print('Ambiguous test accuracy:', test_probe(probe, get_acts, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(probe, get_acts, batches=batches, label_idx=0))\n",
    "print('Unintended feature accuracy:', test_probe(probe, get_acts, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.9978401064872742\n",
      "Accuracy for (0, 1): 0.24355988204479218\n",
      "Accuracy for (1, 0): 0.2626728117465973\n",
      "Accuracy for (1, 1): 0.9934943914413452\n"
     ]
    }
   ],
   "source": [
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(probe, get_acts, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "submodules = []\n",
    "dictionaries = {}\n",
    "\n",
    "submodules.append(model.gpt_neox.embed_in)\n",
    "ae = AutoEncoder(512, dictionary_size).to(DEVICE)\n",
    "ae.load_state_dict(t.load(f'../dictionaries/pythia-70m-deduped/embed/{dict_id}_{dictionary_size}/ae.pt'))\n",
    "dictionaries[model.gpt_neox.embed_in] = ae\n",
    "\n",
    "for i in range(layer + 1):\n",
    "    submodules.append(model.gpt_neox.layers[i].attention)\n",
    "    ae = AutoEncoder(512, dictionary_size).to(DEVICE)\n",
    "    ae.load_state_dict(t.load(f'../dictionaries/pythia-70m-deduped/attn_out_layer{i}/{dict_id}_{dictionary_size}/ae.pt'))\n",
    "    dictionaries[model.gpt_neox.layers[i].attention] = ae\n",
    "\n",
    "    submodules.append(model.gpt_neox.layers[i].mlp)\n",
    "    ae = AutoEncoder(512, dictionary_size).to(DEVICE)\n",
    "    ae.load_state_dict(t.load(f'../dictionaries/pythia-70m-deduped/mlp_out_layer{i}/{dict_id}_{dictionary_size}/ae.pt'))\n",
    "    dictionaries[model.gpt_neox.layers[i].mlp] = ae\n",
    "\n",
    "    submodules.append(model.gpt_neox.layers[i])\n",
    "    ae = AutoEncoder(512, dictionary_size).to(DEVICE)\n",
    "    ae.load_state_dict(t.load(f'../dictionaries/pythia-70m-deduped/resid_out_layer{i}/{dict_id}_{dictionary_size}/ae.pt'))\n",
    "    dictionaries[model.gpt_neox.layers[i]] = ae\n",
    "\n",
    "def metric_fn(model, labels=None):\n",
    "    attn_mask = model.input[1]['attention_mask']\n",
    "    acts = model.gpt_neox.layers[layer].output[0]\n",
    "    acts = acts * attn_mask[:, :, None]\n",
    "    acts = acts.sum(1) / attn_mask.sum(1)[:, None]\n",
    "    \n",
    "    return t.where(\n",
    "        labels == 0,\n",
    "        probe(acts),\n",
    "        - probe(acts)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:56<00:00,  2.24s/it]\n"
     ]
    }
   ],
   "source": [
    "n_batches = 25\n",
    "batch_size = 4\n",
    "\n",
    "running_total = 0\n",
    "nodes = None\n",
    "\n",
    "for batch_idx, (clean, labels, _) in tqdm(enumerate(get_data(train=True, ambiguous=True, batch_size=batch_size, seed=SEED)), total=n_batches):\n",
    "    if batch_idx == n_batches:\n",
    "        break\n",
    "\n",
    "    effects, _, _, _ = patching_effect(\n",
    "        clean,\n",
    "        None,\n",
    "        model,\n",
    "        submodules,\n",
    "        dictionaries,\n",
    "        metric_fn,\n",
    "        metric_kwargs=dict(labels=labels),\n",
    "        method='ig'\n",
    "    )\n",
    "    with t.no_grad():\n",
    "        if nodes is None:\n",
    "            nodes = {k : len(clean) * v.sum(dim=1).mean(dim=0) for k, v in effects.items()}\n",
    "        else:\n",
    "            for k, v in effects.items():\n",
    "                nodes[k] += len(clean) * v.sum(dim=1).mean(dim=0)\n",
    "        running_total += len(clean)\n",
    "    del effects, _\n",
    "    gc.collect()\n",
    "\n",
    "nodes = {k : v / running_total for k, v in nodes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 0:\n",
      "946 0.21847236156463623\n",
      "5719 0.1465141624212265\n",
      "7392 0.30848637223243713\n",
      "10784 0.1519770622253418\n",
      "17846 0.314985066652298\n",
      "22068 0.1732397824525833\n",
      "23079 0.14993813633918762\n",
      "25904 0.10273231565952301\n",
      "28533 0.18635113537311554\n",
      "29476 0.19284993410110474\n",
      "31461 0.17133042216300964\n",
      "31467 0.1645527184009552\n",
      "32081 0.3282443881034851\n",
      "32469 1.367475986480713\n",
      "Component 1:\n",
      "23752 0.10250095278024673\n",
      "Component 2:\n",
      "2995 0.10375188291072845\n",
      "3842 0.14587925374507904\n",
      "10258 0.2980944812297821\n",
      "13387 0.14533501863479614\n",
      "13968 0.13012699782848358\n",
      "18382 0.25857868790626526\n",
      "19369 0.18626706302165985\n",
      "28127 1.0542335510253906\n",
      "30518 0.1976676732301712\n",
      "Component 3:\n",
      "1022 0.3151320517063141\n",
      "9651 0.5876054763793945\n",
      "10060 2.4383997917175293\n",
      "18967 0.6847831606864929\n",
      "22084 0.2555842399597168\n",
      "23898 0.48170098662376404\n",
      "24799 0.1013026013970375\n",
      "26504 0.3015204668045044\n",
      "29626 0.28801754117012024\n",
      "31201 0.17514200508594513\n",
      "Component 4:\n",
      "8147 0.10604129731655121\n",
      "Component 5:\n",
      "24159 0.24751698970794678\n",
      "25018 0.49414151906967163\n",
      "Component 6:\n",
      "4592 0.3942481279373169\n",
      "8920 0.5914136171340942\n",
      "9877 0.3649440407752991\n",
      "12128 0.6454461812973022\n",
      "15017 3.1603307723999023\n",
      "17369 0.1065029725432396\n",
      "26969 0.10490833222866058\n",
      "30248 1.0401283502578735\n",
      "Component 7:\n",
      "13570 0.1254691779613495\n",
      "27472 1.2396912574768066\n",
      "Component 8:\n",
      "Component 9:\n",
      "1995 1.0041873455047607\n",
      "9128 1.5332838296890259\n",
      "11656 0.1628165990114212\n",
      "12440 0.18826739490032196\n",
      "14638 0.15284165740013123\n",
      "29206 0.21672885119915009\n",
      "29295 0.6555291414260864\n",
      "31098 0.10945925116539001\n",
      "Component 10:\n",
      "2959 0.9746395945549011\n",
      "19128 0.26577478647232056\n",
      "22029 0.1784755140542984\n",
      "Component 11:\n",
      "Component 12:\n",
      "19558 1.5478507280349731\n",
      "23545 0.37521830201148987\n",
      "24806 0.12194683402776718\n",
      "27334 0.1561143398284912\n",
      "31453 0.1720980405807495\n",
      "Component 13:\n",
      "31101 0.5748919248580933\n",
      "Component 14:\n",
      "Component 15:\n",
      "9766 0.23256148397922516\n",
      "12420 1.354140281677246\n",
      "30220 0.2663307189941406\n",
      "total features: 67\n"
     ]
    }
   ],
   "source": [
    "n_features = 0\n",
    "for component_idx, effect in enumerate(nodes.values()):\n",
    "    print(f\"Component {component_idx}:\")\n",
    "    for idx in (effect > 0.1).nonzero():\n",
    "        print(idx.item(), effect[idx].item())\n",
    "        n_features += 1\n",
    "print(f\"total features: {n_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' nursing', 5.101802825927734), (' Nursing', 3.935664653778076), (' nurse', 2.8416316509246826), (' nurses', 2.8025169372558594), (' RN', 1.5293747186660767), (' Teaching', 0.7016168832778931), (' rehabilitation', 0.6920667886734009), ('wife', 0.6749844551086426), ('unte', 0.6074659824371338), (' sewing', 0.6034234762191772), (' caring', 0.4292556643486023), ('ancy', 0.38825851678848267), ('akers', 0.38148021697998047), (' lending', 0.34468209743499756), (' volunteers', 0.3259948790073395), (' drinking', 0.2667080760002136), (' Clinical', 0.2624419331550598), (' inpatient', 0.23919713497161865), (' architect', 0.22629010677337646), (' Medical', 0.2175636887550354), (' relational', 0.19916152954101562), (' Dou', 0.19640293717384338), (' dialysis', 0.176143079996109), (' Leadership', 0.17218172550201416), ('hin', 0.14378610253334045), ('okin', 0.14130336046218872), (' executive', 0.11437082290649414), (' teaching', 0.114189013838768), (' poet', 0.10126757621765137), (' hospitals', 0.0999029278755188)]\n",
      "[('�', 1.2044271230697632), ('��', 1.1849853992462158), ('�', 1.1376994848251343), ('�', 1.1258586645126343), ('�', 1.0496826171875), (' home', 0.9967204928398132), (' homes', 0.9867677688598633), ('�', 0.9811767935752869), ('�', 0.9332031607627869), ('�', 0.91241055727005), ('�', 0.8882569074630737), ('giving', 0.8880900740623474), ('�', 0.8530436754226685), ('�', 0.8423665761947632), ('�', 0.8416789174079895), ('��', 0.8362345695495605), ('��', 0.8250691890716553), ('�', 0.7904134392738342), ('�', 0.7814616560935974), ('��', 0.7765788435935974), ('�', 0.7742025256156921), ('�', 0.7613810896873474), ('��', 0.7597290277481079), ('�', 0.7405924797058105), ('�', 0.7402873039245605), ('�', 0.7377848625183105), (' practitioner', 0.7283490896224976), ('�', 0.7211141586303711), ('��', 0.7182861566543579), ('�', 0.7152547836303711)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-a2752678-fc56\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-a2752678-fc56\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [[\"Computer\", \" assisted\", \" learning\", \":\", \" the\", \" potential\", \" for\", \" teaching\", \" and\", \" assessing\", \" in\", \" nursing\"], [\"Deb\", \"bie\", \" Gregory\", \" D\", \"NP\", \",\", \" RN\", \"\\n\", \"\\n\", \"Dr\", \".\", \" De\", \"bbie\", \" Gregory\", \" is\", \" a\", \" national\", \" leader\", \" in\", \" healthcare\", \" design\", \",\", \" innovation\", \",\", \" and\", \" transformation\", \".\", \" As\", \" a\", \" nurse\", \" executive\", \" and\", \" interior\", \" designer\", \",\", \" Dr\", \".\", \" Gregory\", \" is\", \" passionate\", \" about\", \" \\u201c\", \"Int\", \"entional\", \" Design\", \"\\u201d\", \" that\", \" align\", \"s\", \" People\", \",\", \" Place\", \",\", \" and\", \" Process\", \".\", \" She\", \" creates\", \" and\", \" transforms\", \" environments\", \" into\", \" functional\", \" ecosystems\", \" using\", \" complex\", \" systems\", \" science\", \" and\", \" strategic\", \" thinking\", \".\", \"\\n\", \"\\n\", \"Dr\", \".\", \" Gregory\", \" has\", \" Doctor\", \"ate\", \" of\", \" Nursing\", \" Practice\", \" in\", \" Health\", \" Innovation\", \" and\", \" Leadership\", \" from\", \" the\", \" University\", \" of\", \" Minnesota\", \" and\", \" a\", \" b\", \"achel\", \"ors\", \" in\", \" nursing\"], [\"Computer\", \" assisted\", \" learning\", \":\", \" the\", \" potential\", \" for\", \" teaching\", \" and\", \" assessing\", \" in\", \" nursing\", \".\", \"\\n\", \"This\", \" article\", \" discusses\", \" computer\", \" assisted\", \" learning\", \" (\", \"CAL\", \")\", \" and\", \" the\", \" importance\", \" of\", \" applying\", \" it\", \" in\", \" nurse\", \" education\", \".\", \" The\", \" articles\", \" recognizes\", \" the\", \" general\", \" technological\", \" developments\", \" as\", \" exemplified\", \" by\", \" the\", \" Teaching\", \" and\", \" Learning\", \" Technology\", \" Programme\", \" (\", \"TL\", \"TP\", \")\", \" from\", \" which\", \" ideas\", \" about\", \" application\", \" and\", \" benefits\", \" came\", \".\", \" The\", \" ideas\", \" from\", \" TL\", \"TP\", \" are\", \" hereby\", \" used\", \" in\", \" CAL\", \" and\", \" applied\", \" to\", \" nursing\"], [\"A\", \" small\", \" city\", \" in\", \" Iowa\", \" has\", \" taken\", \" an\", \" action\", \" to\", \" save\", \" the\", \" bees\", \" from\", \" extinction\", \".\", \" Ac\", \"res\", \" of\", \" land\", \" were\", \" donated\", \" to\", \" increase\", \" the\", \" local\", \" habitats\", \" of\", \" the\", \" bees\", \".\", \"\\n\", \"\\n\", \"Over\", \" the\", \" past\", \" decade\", \",\", \" bees\", \" are\", \" steadily\", \" disappearing\", \".\", \" Work\", \"er\", \" bees\", \" disappear\", \" and\", \" leaving\", \" behind\", \" the\", \" queen\", \".\", \" With\", \" a\", \" few\", \" nursing\"], [\"Deb\", \"bie\", \" Gregory\", \" D\", \"NP\", \",\", \" RN\", \"\\n\", \"\\n\", \"Dr\", \".\", \" De\", \"bbie\", \" Gregory\", \" is\", \" a\", \" national\", \" leader\", \" in\", \" healthcare\", \" design\", \",\", \" innovation\", \",\", \" and\", \" transformation\", \".\", \" As\", \" a\", \" nurse\", \" executive\", \" and\", \" interior\", \" designer\", \",\", \" Dr\", \".\", \" Gregory\", \" is\", \" passionate\", \" about\", \" \\u201c\", \"Int\", \"entional\", \" Design\", \"\\u201d\", \" that\", \" align\", \"s\", \" People\", \",\", \" Place\", \",\", \" and\", \" Process\", \".\", \" She\", \" creates\", \" and\", \" transforms\", \" environments\", \" into\", \" functional\", \" ecosystems\", \" using\", \" complex\", \" systems\", \" science\", \" and\", \" strategic\", \" thinking\", \".\", \"\\n\", \"\\n\", \"Dr\", \".\", \" Gregory\", \" has\", \" Doctor\", \"ate\", \" of\", \" Nursing\"], [\"Deb\", \"bie\", \" Gregory\", \" D\", \"NP\", \",\", \" RN\", \"\\n\", \"\\n\", \"Dr\", \".\", \" De\", \"bbie\", \" Gregory\", \" is\", \" a\", \" national\", \" leader\", \" in\", \" healthcare\", \" design\", \",\", \" innovation\", \",\", \" and\", \" transformation\", \".\", \" As\", \" a\", \" nurse\"], [\"Primary\", \" care\", \" for\", \" women\", \".\", \" Comprehensive\", \" assessment\", \" and\", \" management\", \" of\", \" common\", \" mental\", \" health\", \" problems\", \".\", \"\\n\", \"This\", \" article\", \" emphasizes\", \" the\", \" importance\", \" of\", \" the\", \" role\", \" of\", \" the\", \" certified\", \" nurse\"], [\"Computer\", \" assisted\", \" learning\", \":\", \" the\", \" potential\", \" for\", \" teaching\", \" and\", \" assessing\", \" in\", \" nursing\", \".\", \"\\n\", \"This\", \" article\", \" discusses\", \" computer\", \" assisted\", \" learning\", \" (\", \"CAL\", \")\", \" and\", \" the\", \" importance\", \" of\", \" applying\", \" it\", \" in\", \" nurse\", \" education\", \".\", \" The\", \" articles\", \" recognizes\", \" the\", \" general\", \" technological\", \" developments\", \" as\", \" exemplified\", \" by\", \" the\", \" Teaching\", \" and\", \" Learning\", \" Technology\", \" Programme\", \" (\", \"TL\", \"TP\", \")\", \" from\", \" which\", \" ideas\", \" about\", \" application\", \" and\", \" benefits\", \" came\", \".\", \" The\", \" ideas\", \" from\", \" TL\", \"TP\", \" are\", \" hereby\", \" used\", \" in\", \" CAL\", \" and\", \" applied\", \" to\", \" nursing\", \" and\", \" health\", \"-\", \"care\", \" undergraduate\", \" programmes\", \" in\", \" one\", \" university\", \".\", \" In\", \" the\", \" light\", \" of\", \" this\", \" experience\", \" the\", \" main\", \" intention\", \" of\", \" this\", \" article\", \" is\", \" to\", \" consider\", \" the\", \" benefits\", \" and\", \" costs\", \" of\", \" introducing\", \" computer\", \" programmes\", \" as\", \" part\", \" of\", \" the\", \" teaching\", \" provision\", \" for\", \" nurses\"], [\"Computer\", \" assisted\", \" learning\", \":\", \" the\", \" potential\", \" for\", \" teaching\", \" and\", \" assessing\", \" in\", \" nursing\", \".\", \"\\n\", \"This\", \" article\", \" discusses\", \" computer\", \" assisted\", \" learning\", \" (\", \"CAL\", \")\", \" and\", \" the\", \" importance\", \" of\", \" applying\", \" it\", \" in\", \" nurse\"], [\"Deb\", \"bie\", \" Gregory\", \" D\", \"NP\", \",\", \" RN\"], [\"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"As\", \" Charleston\", \" Area\", \" Medical\", \" Center\", \" volunteers\", \",\", \" our\", \" mission\", \" is\", \" to\", \" serve\", \" as\", \" support\", \" for\", \" patients\", \",\", \" families\", \" and\", \" hospital\", \" staff\", \",\", \" and\", \" to\", \" provide\", \" a\", \" caring\", \",\", \" comforting\", \" and\", \" cour\", \"te\", \"ous\", \" environment\", \".\", \"\\n\", \"\\n\", \"Vol\", \"unte\"], [\"No\", \" other\", \" appliance\", \" company\", \" has\", \" a\", \" wider\", \" scope\", \" of\", \" solutions\", \",\", \" nor\", \" the\", \" experience\", \" to\", \" back\", \" them\", \" up\", \",\", \" than\", \" Elect\", \"rol\", \"ux\", \".\", \" Our\", \" long\", \" presence\", \" in\", \" people\", \"\\u2019\", \"s\", \" homes\", \" around\", \" the\", \" world\", \" means\", \" that\", \" no\", \" other\", \" appliance\", \" company\", \"...\", \"\\n\", \"Read\", \" more\", \"\\n\", \"\\n\", \"...\", \" Easy\", \"-\", \"F\", \"lo\", \" vacu\", \"ums\", \" including\", \" parts\", \" and\", \" bags\", \".\", \" Find\", \"lay\", \"'s\", \" also\", \" offers\", \" sales\", \" and\", \" service\", \" for\", \" all\", \" makes\", \" and\", \" models\", \" of\", \" sewing\"], [\"Primary\", \" care\", \" for\", \" women\", \".\", \" Comprehensive\", \" assessment\", \" and\", \" management\", \" of\", \" common\", \" mental\", \" health\", \" problems\", \".\", \"\\n\", \"This\", \" article\", \" emphasizes\", \" the\", \" importance\", \" of\", \" the\", \" role\", \" of\", \" the\", \" certified\", \" nurse\", \"-\"], [\"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"Vol\", \"unte\"], [\"Computer\", \" assisted\", \" learning\", \":\", \" the\", \" potential\", \" for\", \" teaching\", \" and\", \" assessing\", \" in\", \" nursing\", \".\", \"\\n\", \"This\", \" article\", \" discusses\", \" computer\", \" assisted\", \" learning\", \" (\", \"CAL\", \")\", \" and\", \" the\", \" importance\", \" of\", \" applying\", \" it\", \" in\", \" nurse\", \" education\", \".\", \" The\", \" articles\", \" recognizes\", \" the\", \" general\", \" technological\", \" developments\", \" as\", \" exemplified\", \" by\", \" the\", \" Teaching\"], [\"PCI\", \" Alternative\", \" Using\", \" S\", \"ust\", \"ained\", \" Exercise\", \" (\", \"PA\", \"USE\", \"):\", \" R\", \"ational\", \"e\", \" and\", \" trial\", \" design\", \".\", \"\\n\", \"Card\", \"i\", \"ovascular\", \" disease\", \" (\", \"C\", \"VD\", \")\", \" currently\", \" claims\", \" nearly\", \" one\", \" million\", \" lives\", \" yearly\", \" in\", \" the\", \" US\", \",\", \" accounting\", \" for\", \" nearly\", \" 40\", \"%\", \" of\", \" all\", \" deaths\", \".\", \" Coron\", \"ary\", \" artery\", \" disease\", \" (\", \"CAD\", \")\", \" accounts\", \" for\", \" the\", \" largest\", \" number\", \" of\", \" these\", \" deaths\", \".\", \" While\", \" efforts\", \" aimed\", \" at\", \" treating\", \" CAD\", \" in\", \" recent\", \" decades\", \" have\", \" concentrated\", \" on\", \" surgical\", \" and\", \" catheter\", \"-\", \"based\", \" interventions\", \",\", \" limited\", \" resources\", \" have\", \" been\", \" directed\", \" toward\", \" prevention\", \" and\", \" rehabilitation\"], [\"Primary\", \" care\", \" for\", \" women\", \".\", \" Comprehensive\", \" assessment\", \" and\", \" management\", \" of\", \" common\", \" mental\", \" health\", \" problems\", \".\", \"\\n\", \"This\", \" article\", \" emphasizes\", \" the\", \" importance\", \" of\", \" the\", \" role\", \" of\", \" the\", \" certified\", \" nurse\", \"-\", \"mid\", \"wife\"], [\"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"As\", \" Charleston\", \" Area\", \" Medical\"], [\"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"As\", \" Charleston\", \" Area\", \" Medical\", \" Center\", \" volunteers\"], [\"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"As\", \" Charleston\", \" Area\", \" Medical\", \" Center\", \" volunteers\", \",\", \" our\", \" mission\", \" is\", \" to\", \" serve\", \" as\", \" support\", \" for\", \" patients\", \",\", \" families\", \" and\", \" hospital\", \" staff\", \",\", \" and\", \" to\", \" provide\", \" a\", \" caring\"], [\"Account\", \"ing\", \"\\n\", \"\\n\", \"Sur\", \"f\", \" Works\", \" offer\", \" a\", \" range\", \" of\", \" accounting\", \" services\", \" suitable\", \" for\", \" all\", \" types\", \" of\", \" business\", \".\", \" Below\", \",\", \" we\", \" have\", \" listed\", \" packages\", \" suitable\", \" for\", \" sole\", \" traders\", \",\", \" partnerships\", \" and\", \" limited\", \" companies\", \".\", \" The\", \" packages\", \" can\", \" be\", \" fully\", \" tailored\", \" to\", \" your\", \" requirements\", \" by\", \" adding\", \" extra\", \" services\", \" to\", \" create\", \" the\", \" exact\", \" service\", \" that\", \" you\", \" and\", \" your\", \" business\", \" requires\", \".\", \"\\n\", \"\\n\", \"All\", \" services\", \" are\", \" carried\", \" out\", \" on\", \" time\", \" with\", \" the\", \" minimum\", \" of\", \" fuss\", \" by\", \" our\", \" in\", \" house\", \",\", \" fully\", \" qualified\", \" accountant\", \"\\n\", \"\\n\", \"The\", \" list\", \" of\", \" services\", \" offered\", \" is\", \" not\", \" exhaustive\", \" so\", \" please\", \" let\", \" us\", \" know\", \" if\", \" you\", \" require\", \" a\", \" service\", \" not\", \" listed\", \".\", \" If\", \" you\", \" have\", \" specific\", \" needs\", \" we\", \" can\", \" build\", \" a\", \" bes\", \"p\", \"oke\", \" account\", \"ancy\"], [\"Computer\", \" assisted\", \" learning\", \":\", \" the\", \" potential\", \" for\", \" teaching\", \" and\", \" assessing\", \" in\", \" nursing\", \".\", \"\\n\", \"This\", \" article\", \" discusses\", \" computer\", \" assisted\", \" learning\", \" (\", \"CAL\", \")\", \" and\", \" the\", \" importance\", \" of\", \" applying\", \" it\", \" in\", \" nurse\", \" education\", \".\", \" The\", \" articles\", \" recognizes\", \" the\", \" general\", \" technological\", \" developments\", \" as\", \" exemplified\", \" by\", \" the\", \" Teaching\", \" and\", \" Learning\", \" Technology\", \" Programme\", \" (\", \"TL\", \"TP\", \")\", \" from\", \" which\", \" ideas\", \" about\", \" application\", \" and\", \" benefits\", \" came\", \".\", \" The\", \" ideas\", \" from\", \" TL\", \"TP\", \" are\", \" hereby\", \" used\", \" in\", \" CAL\", \" and\", \" applied\", \" to\", \" nursing\", \" and\"], [\"Computer\", \" assisted\", \" learning\", \":\", \" the\", \" potential\", \" for\", \" teaching\", \" and\", \" assessing\", \" in\", \" nursing\", \".\", \"\\n\", \"This\", \" article\", \" discusses\", \" computer\", \" assisted\", \" learning\", \" (\", \"CAL\", \")\", \" and\", \" the\", \" importance\", \" of\", \" applying\", \" it\", \" in\", \" nurse\", \" education\", \".\", \" The\", \" articles\", \" recognizes\", \" the\", \" general\", \" technological\", \" developments\", \" as\", \" exemplified\", \" by\", \" the\", \" Teaching\", \" and\", \" Learning\", \" Technology\", \" Programme\", \" (\", \"TL\", \"TP\", \")\", \" from\", \" which\", \" ideas\", \" about\", \" application\", \" and\", \" benefits\", \" came\", \".\", \" The\", \" ideas\", \" from\", \" TL\", \"TP\", \" are\", \" hereby\", \" used\", \" in\", \" CAL\", \" and\", \" applied\", \" to\", \" nursing\", \" and\", \" health\", \"-\", \"care\", \" undergraduate\", \" programmes\", \" in\", \" one\", \" university\", \".\", \" In\", \" the\", \" light\", \" of\", \" this\", \" experience\", \" the\", \" main\", \" intention\", \" of\", \" this\", \" article\", \" is\", \" to\", \" consider\", \" the\", \" benefits\", \" and\", \" costs\", \" of\", \" introducing\", \" computer\", \" programmes\", \" as\", \" part\", \" of\", \" the\", \" teaching\", \" provision\", \" for\", \" nurses\", \" and\"], [\"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"Vol\", \"unte\", \"er\", \" Services\", \"\\n\", \"\\n\", \"As\", \" Charleston\", \" Area\", \" Medical\", \" Center\", \" volunteers\", \",\", \" our\", \" mission\", \" is\", \" to\", \" serve\", \" as\", \" support\", \" for\", \" patients\", \",\", \" families\", \" and\", \" hospital\", \" staff\", \",\", \" and\", \" to\", \" provide\", \" a\", \" caring\", \",\", \" comforting\", \" and\", \" cour\", \"te\", \"ous\", \" environment\", \".\", \"\\n\", \"\\n\", \"Vol\", \"unte\", \"ers\", \" at\", \" CAM\", \"C\", \" bring\", \" their\", \" unique\", \" personalities\", \" and\", \" skills\", \" to\", \" our\", \" hospital\", \".\", \" They\", \" range\", \" in\", \" age\", \" from\", \" 15\", \" to\", \" 99\", \".\", \" Our\", \" ranks\", \" are\", \" made\", \" up\", \" of\", \" men\", \" and\", \" women\", \";\", \" students\", \" and\", \" retire\", \"es\", \";\", \" homem\", \"akers\"], [\"Got\", \" this\", \" cute\", \" little\", \" sewing\"], [\"Deb\", \"bie\", \" Gregory\", \" D\", \"NP\", \",\", \" RN\", \"\\n\", \"\\n\", \"Dr\", \".\", \" De\", \"bbie\", \" Gregory\", \" is\", \" a\", \" national\", \" leader\", \" in\", \" healthcare\", \" design\", \",\", \" innovation\", \",\", \" and\", \" transformation\", \".\", \" As\", \" a\", \" nurse\", \" executive\", \" and\", \" interior\", \" designer\", \",\", \" Dr\", \".\", \" Gregory\", \" is\", \" passionate\", \" about\", \" \\u201c\", \"Int\", \"entional\", \" Design\", \"\\u201d\", \" that\", \" align\", \"s\", \" People\", \",\", \" Place\", \",\", \" and\", \" Process\", \".\", \" She\", \" creates\", \" and\", \" transforms\", \" environments\", \" into\", \" functional\", \" ecosystems\", \" using\", \" complex\", \" systems\", \" science\", \" and\", \" strategic\", \" thinking\", \".\", \"\\n\", \"\\n\", \"Dr\", \".\", \" Gregory\", \" has\", \" Doctor\", \"ate\", \" of\", \" Nursing\", \" Practice\", \" in\", \" Health\", \" Innovation\", \" and\", \" Leadership\", \" from\", \" the\", \" University\", \" of\", \" Minnesota\", \" and\", \" a\", \" b\", \"achel\", \"ors\"], [\"Micro\", \"-\", \"Lo\", \"an\", \" Program\", \"\\n\", \"\\n\", \"In\", \" order\", \" to\", \" promote\", \" economic\", \" development\", \" in\", \" the\", \" City\", \" of\", \" Al\", \"amo\", \",\", \" the\", \" Al\", \"amo\", \" ED\", \"C\", \" established\", \" the\", \" Al\", \"amo\", \" Small\", \" Business\", \" Micro\", \"-\", \"Lo\", \"an\", \" Program\", \" (\", \"ML\", \"P\", \")\", \" with\", \" assistance\", \" from\", \" USDA\", \" \\u2013\", \" Rural\", \" Development\", \".\", \" The\", \" M\", \"LP\", \" is\", \" a\", \" self\", \"-\", \"s\", \"ust\", \"aining\", \" project\", \" that\", \" works\", \" by\", \" lending\"], [\"Deb\", \"bie\", \" Gregory\", \" D\", \"NP\", \",\", \" RN\", \"\\n\", \"\\n\", \"Dr\", \".\", \" De\", \"bbie\", \" Gregory\", \" is\", \" a\", \" national\", \" leader\", \" in\", \" healthcare\", \" design\", \",\", \" innovation\", \",\", \" and\", \" transformation\", \".\", \" As\", \" a\", \" nurse\", \" executive\"], [\"Computer\", \" assisted\", \" learning\", \":\", \" the\", \" potential\", \" for\", \" teaching\", \" and\", \" assessing\", \" in\", \" nursing\", \".\", \"\\n\", \"This\", \" article\", \" discusses\", \" computer\", \" assisted\", \" learning\", \" (\", \"CAL\", \")\", \" and\", \" the\", \" importance\", \" of\", \" applying\", \" it\", \" in\", \" nurse\", \" education\", \".\", \" The\", \" articles\", \" recognizes\", \" the\", \" general\", \" technological\", \" developments\", \" as\", \" exemplified\", \" by\", \" the\", \" Teaching\", \" and\", \" Learning\", \" Technology\", \" Programme\", \" (\", \"TL\", \"TP\", \")\", \" from\", \" which\", \" ideas\", \" about\", \" application\", \" and\", \" benefits\", \" came\", \".\", \" The\", \" ideas\", \" from\", \" TL\", \"TP\", \" are\", \" hereby\", \" used\", \" in\", \" CAL\", \" and\", \" applied\", \" to\", \" nursing\", \" and\", \" health\", \"-\", \"care\", \" undergraduate\", \" programmes\", \" in\", \" one\", \" university\", \".\", \" In\", \" the\", \" light\", \" of\", \" this\", \" experience\", \" the\", \" main\", \" intention\", \" of\", \" this\", \" article\", \" is\", \" to\", \" consider\", \" the\", \" benefits\", \" and\", \" costs\", \" of\", \" introducing\", \" computer\", \" programmes\", \" as\", \" part\", \" of\", \" the\", \" teaching\"], [\"[\", \"Central\", \" venous\", \" dialysis\", \" catheter\", \".\", \" Sil\", \"ic\", \"one\", \" rubber\", \" dialysis\"]], \"activations\": [[[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.5048909187316895]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.5293747186660767]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.14937126636505127]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.9188828468322754]], [[0.3431124687194824]], [[0.0]], [[0.0]], [[0.010101854801177979]], [[0.13302141427993774]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.02793174982070923]], [[0.0]], [[3.935664653778076]], [[0.0]], [[0.0]], [[0.0]], [[0.0610697865486145]], [[0.0]], [[0.17218172550201416]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.35008376836776733]], [[0.0]], [[5.229028701782227]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.5048909187316895]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.796511173248291]], [[0.08860617876052856]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7016168832778931]], [[0.24174904823303223]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.971462726593018]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.701828956604004]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.5293747186660767]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.14937126636505127]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.9188828468322754]], [[0.3431124687194824]], [[0.0]], [[0.0]], [[0.010101854801177979]], [[0.13302141427993774]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.02793174982070923]], [[0.0]], [[3.935664653778076]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.5293747186660767]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.14937126636505127]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.9188828468322754]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.02190786600112915]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.8095011711120605]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.5048909187316895]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.796511173248291]], [[0.08860617876052856]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7016168832778931]], [[0.24174904823303223]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.971462726593018]], [[0.38621997833251953]], [[0.16560763120651245]], [[0.0]], [[0.0]], [[0.02093595266342163]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.3425670266151428]], [[0.0]], [[0.0]], [[2.8025169372558594]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.5048909187316895]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.796511173248291]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.5293747186660767]]], [[[0.0]], [[0.23981249332427979]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7305059432983398]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.4351273775100708]], [[0.0]], [[0.43192631006240845]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.4292556643486023]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.8520793914794922]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.8406276702880859]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.02190786600112915]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.8095011711120605]], [[0.740138053894043]]], [[[0.0]], [[0.23981249332427979]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7305059432983398]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.5048909187316895]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.796511173248291]], [[0.08860617876052856]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7016168832778931]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.6920667886734009]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.02190786600112915]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.8095011711120605]], [[0.740138053894043]], [[0.0]], [[0.6749844551086426]]], [[[0.0]], [[0.23981249332427979]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7305059432983398]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.4351273775100708]]], [[[0.0]], [[0.23981249332427979]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7305059432983398]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.4351273775100708]], [[0.0]], [[0.43192631006240845]]], [[[0.0]], [[0.23981249332427979]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7305059432983398]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.4351273775100708]], [[0.0]], [[0.43192631006240845]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.4292556643486023]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.38825851678848267]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.5048909187316895]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.796511173248291]], [[0.08860617876052856]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7016168832778931]], [[0.24174904823303223]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.971462726593018]], [[0.38621997833251953]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.5048909187316895]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.796511173248291]], [[0.08860617876052856]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7016168832778931]], [[0.24174904823303223]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.971462726593018]], [[0.38621997833251953]], [[0.16560763120651245]], [[0.0]], [[0.0]], [[0.02093595266342163]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.3425670266151428]], [[0.0]], [[0.0]], [[2.8025169372558594]], [[0.38558435440063477]]], [[[0.0]], [[0.23981249332427979]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7305059432983398]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.4351273775100708]], [[0.0]], [[0.43192631006240845]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.4292556643486023]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.8520793914794922]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.38148021697998047]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.36621934175491333]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.5293747186660767]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.14937126636505127]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.9188828468322754]], [[0.3431124687194824]], [[0.0]], [[0.0]], [[0.010101854801177979]], [[0.13302141427993774]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.02793174982070923]], [[0.0]], [[3.935664653778076]], [[0.0]], [[0.0]], [[0.0]], [[0.0610697865486145]], [[0.0]], [[0.17218172550201416]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.35008376836776733]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.34468209743499756]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.5293747186660767]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.14937126636505127]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.9188828468322754]], [[0.3431124687194824]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.5048909187316895]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.796511173248291]], [[0.08860617876052856]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7016168832778931]], [[0.24174904823303223]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.971462726593018]], [[0.38621997833251953]], [[0.16560763120651245]], [[0.0]], [[0.0]], [[0.02093595266342163]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.3425670266151428]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.3197513222694397]]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f35b0154bd0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "component_idx = 9\n",
    "feat_idx = 31098\n",
    "\n",
    "submodule = submodules[component_idx]\n",
    "dictionary = dictionaries[submodule]\n",
    "\n",
    "# interpret some features\n",
    "data = hf_dataset_to_generator(\"monology/pile-uncopyrighted\")\n",
    "buffer = ActivationBuffer(\n",
    "    data,\n",
    "    model,\n",
    "    submodule,\n",
    "    out_feats=512,\n",
    "    in_batch_size=128,\n",
    "    n_ctxs=512,\n",
    ")\n",
    "\n",
    "out = examine_dimension(\n",
    "    model,\n",
    "    submodule,\n",
    "    buffer,\n",
    "    dictionary,\n",
    "    dim_idx=feat_idx,\n",
    "    n_inputs=256\n",
    ")\n",
    "print(out.top_tokens)\n",
    "print(out.top_affected)\n",
    "out.top_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features to ablate: 55\n"
     ]
    }
   ],
   "source": [
    "feats_to_ablate = {\n",
    "    submodules[0] : [\n",
    "        946, # 'his'\n",
    "        # 5719, # 'research'\n",
    "        7392, # 'He'\n",
    "        # 10784, # 'Nursing'\n",
    "        17846, # 'He'\n",
    "        22068, # 'His'\n",
    "        # 23079, # 'tastes'\n",
    "        # 25904, # 'nursing'\n",
    "        28533, # 'She'\n",
    "        29476, # 'he'\n",
    "        31461, # 'His'\n",
    "        31467, # 'she'\n",
    "        32081, # 'her'\n",
    "        32469, # 'She'\n",
    "    ],\n",
    "    submodules[1] : [\n",
    "        # 23752, # capitalized words, especially pronouns\n",
    "    ],\n",
    "    submodules[2] : [\n",
    "        2995, # 'he'\n",
    "        3842, # 'She'\n",
    "        10258, # female names\n",
    "        13387, # 'she'\n",
    "        13968, # 'He'\n",
    "        18382, # 'her'\n",
    "        19369, # 'His'\n",
    "        28127, # 'She'\n",
    "        30518, # 'He'\n",
    "    ],\n",
    "    submodules[3] : [\n",
    "        1022, # 'she'\n",
    "        9651, # female names\n",
    "        10060, # 'She'\n",
    "        18967, # 'He'\n",
    "        22084, # 'he'\n",
    "        23898, # 'His'\n",
    "        # 24799, # promotes surnames\n",
    "        26504, # 'her'\n",
    "        29626, # 'his'\n",
    "        # 31201, # 'nursing'\n",
    "    ],\n",
    "    submodules[4] : [\n",
    "        # 8147, # unclear, something with names\n",
    "    ],\n",
    "    submodules[5] : [\n",
    "        24159, # 'She', 'she'\n",
    "        25018, # female names\n",
    "    ],\n",
    "    submodules[6] : [\n",
    "        4592, # 'her'\n",
    "        8920, # 'he'\n",
    "        9877, # female names\n",
    "        12128, # 'his'\n",
    "        15017, # 'she'\n",
    "        # 17369, # contact info\n",
    "        # 26969, # related to nursing\n",
    "        30248, # female names\n",
    "    ],\n",
    "    submodules[7] : [\n",
    "        13570, # promotes male-related words\n",
    "        27472, # female names, promotes female-related words\n",
    "    ],\n",
    "    submodules[8] : [\n",
    "    ],\n",
    "    submodules[9] : [\n",
    "        1995, # promotes female-associated words\n",
    "        9128, # feminine pronouns\n",
    "        11656, # promotes male-associated words\n",
    "        12440, # promotes female-associated words\n",
    "        # 14638, # related to contact information?\n",
    "        29206, # gendered pronouns\n",
    "        29295, # female names\n",
    "        # 31098, # nursing-related words\n",
    "    ],\n",
    "    submodules[10] : [\n",
    "        2959, # promotes female-associated words\n",
    "        19128, # promotes male-associated words\n",
    "        22029, # promotes female-associated words\n",
    "    ],\n",
    "    submodules[11] : [\n",
    "    ],\n",
    "    submodules[12] : [\n",
    "        19558, # promotes female-associated words\n",
    "        23545, # 'she'\n",
    "        24806, # 'her'\n",
    "        27334, # promotes male-associated words\n",
    "        31453, # female names\n",
    "    ],\n",
    "    submodules[13] : [\n",
    "        31101, # promotes female-associated words\n",
    "    ],\n",
    "    submodules[14] : [\n",
    "    ],\n",
    "    submodules[15] : [\n",
    "        9766, # promotes female-associated words\n",
    "        12420, # promotes female pronouns\n",
    "        30220, # promotes male pronouns\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"Number of features to ablate: {sum(len(v) for v in feats_to_ablate.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_hot(feats, dim=dictionary_size):\n",
    "    out = t.zeros(dim, dtype=t.bool, device=DEVICE)\n",
    "    for feat in feats:\n",
    "        out[feat] = True\n",
    "    return out\n",
    "\n",
    "feats_to_ablate = {\n",
    "    submodule : n_hot(feats) for submodule, feats in feats_to_ablate.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_tuple = {}\n",
    "with t.no_grad(), model.trace(\"_\"):\n",
    "    for submodule in submodules:\n",
    "        is_tuple[submodule] = type(submodule.output.shape) == tuple\n",
    "\n",
    "def get_acts_ablated(\n",
    "    text,\n",
    "    model,\n",
    "    submodules,\n",
    "    dictionaries,\n",
    "    to_ablate\n",
    "):\n",
    "\n",
    "    with t.no_grad(), model.trace(text, **tracer_kwargs):\n",
    "        for submodule in submodules:\n",
    "            dictionary = dictionaries[submodule]\n",
    "            feat_idxs = to_ablate[submodule]\n",
    "            x = submodule.output\n",
    "            if is_tuple[submodule]:\n",
    "                x = x[0]\n",
    "            x_hat, f = dictionary(x, output_features=True)\n",
    "            res = x - x_hat\n",
    "            f[...,feat_idxs] = 0.\n",
    "            if is_tuple[submodule]:\n",
    "                submodule.output[0][:] = dictionary.decode(f) + res\n",
    "            else:\n",
    "                submodule.output = dictionary.decode(f) + res\n",
    "        attn_mask = model.input[1]['attention_mask']\n",
    "        act = model.gpt_neox.layers[layer].output[0]\n",
    "        act = act * attn_mask[:, :, None]\n",
    "        act = act.sum(1) / attn_mask.sum(1)[:, None]\n",
    "        act = act.save()\n",
    "    return act.value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy after ablating features judged irrelevant by human annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous test accuracy: 0.926347553730011\n",
      "Ground truth accuracy: 0.8853686451911926\n",
      "Spurious accuracy: 0.5397465229034424\n"
     ]
    }
   ],
   "source": [
    "get_acts_abl = lambda text : get_acts_ablated(text, model, submodules, dictionaries, feats_to_ablate)\n",
    "\n",
    "print('Ambiguous test accuracy:', test_probe(probe, get_acts_abl, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(probe, get_acts_abl, batches=batches, label_idx=0))\n",
    "print('Spurious accuracy:', test_probe(probe, get_acts_abl, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.9879666566848755\n",
      "Accuracy for (0, 1): 0.9644010066986084\n",
      "Accuracy for (1, 0): 0.559907853603363\n",
      "Accuracy for (1, 1): 0.7453531622886658\n"
     ]
    }
   ],
   "source": [
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(probe, lambda text : get_acts_abl(text, model, submodules, dictionaries, feats_to_ablate), batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept bottleneck probing baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts = [    \n",
    "    ' nurse',\n",
    "    ' healthcare',\n",
    "    ' hospital',\n",
    "    ' patient',\n",
    "    ' medical',\n",
    "    ' clinic',\n",
    "    ' triage',\n",
    "    ' medication',\n",
    "    ' emergency',\n",
    "    ' surgery',\n",
    "    ' professor',\n",
    "    ' academia',\n",
    "    ' research',\n",
    "    ' university',\n",
    "    ' tenure',\n",
    "    ' faculty',\n",
    "    ' dissertation',\n",
    "    ' sabbatical',\n",
    "    ' publication',\n",
    "    ' grant',\n",
    "]\n",
    "# get concept vectors\n",
    "with t.no_grad(), model.trace(concepts):\n",
    "    concept_vectors = model.gpt_neox.layers[layer].output[0][:, -1, :].save()\n",
    "concept_vectors = concept_vectors.value - concept_vectors.value.mean(0, keepdim=True)\n",
    "\n",
    "def get_bottleneck(text):\n",
    "    with t.no_grad(), model.trace(text, **tracer_kwargs):\n",
    "        attn_mask = model.input[1]['attention_mask']\n",
    "        acts = model.gpt_neox.layers[layer].output[0]\n",
    "        acts = acts * attn_mask[:, :, None]\n",
    "        acts = acts.sum(1) / attn_mask.sum(1)[:, None]\n",
    "        # compute cosine similarity with concept vectors\n",
    "        sims = (acts @ concept_vectors.T) / (acts.norm(dim=-1)[:, None] @ concept_vectors.norm(dim=-1)[None])\n",
    "        sims = sims.save()\n",
    "    return sims.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth accuracy: 0.8335253596305847\n",
      "Unintended feature accuracy: 0.600806474685669\n"
     ]
    }
   ],
   "source": [
    "cbp_probe, _ = train_probe(get_bottleneck, label_idx=0, dim=len(concepts))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(cbp_probe, get_bottleneck, batches=batches, label_idx=0))\n",
    "print('Unintended feature accuracy:', test_probe(cbp_probe, get_bottleneck, batches=batches, label_idx=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.9276148676872253\n",
      "Accuracy for (0, 1): 0.7864062786102295\n",
      "Accuracy for (1, 0): 0.6774193644523621\n",
      "Accuracy for (1, 1): 0.9409851431846619\n"
     ]
    }
   ],
   "source": [
    "# get subgroup accuracies\n",
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(cbp_probe, get_bottleneck, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get skyline neuron performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:29<00:00,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "# get neurons which are most influential for giving gender label\n",
    "neuron_dicts = {\n",
    "    submodule : IdentityDict(activation_dim).to(DEVICE) for submodule in submodules\n",
    "}\n",
    "\n",
    "n_batches = 25\n",
    "batch_size = 4\n",
    "\n",
    "running_total = 0\n",
    "running_nodes = None\n",
    "\n",
    "for batch_idx, (clean, _, labels) in tqdm(enumerate(get_data(train=True, ambiguous=False, batch_size=batch_size, seed=SEED)), total=n_batches):\n",
    "    if batch_idx == n_batches:\n",
    "        break\n",
    "\n",
    "    effects, _, _, _ = patching_effect(\n",
    "        clean,\n",
    "        None,\n",
    "        model,\n",
    "        submodules,\n",
    "        neuron_dicts,\n",
    "        metric_fn,\n",
    "        metric_kwargs=dict(labels=labels),\n",
    "        method='ig'\n",
    "    )\n",
    "    with t.no_grad():\n",
    "        if running_nodes is None:\n",
    "            running_nodes = {k : len(clean) * v.sum(dim=1).mean(dim=0) for k, v in effects.items()}\n",
    "        else:\n",
    "            for k, v in effects.items():\n",
    "                running_nodes[k] += len(clean) * v.sum(dim=1).mean(dim=0)\n",
    "        running_total += len(clean)\n",
    "\n",
    "nodes = {k : v / running_total for k, v in running_nodes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 0:\n",
      "2 0.2645128667354584\n",
      "42 0.23859986662864685\n",
      "57 0.22970567643642426\n",
      "99 0.32683151960372925\n",
      "130 0.2573539912700653\n",
      "135 0.5450218319892883\n",
      "187 0.39180734753608704\n",
      "197 0.33717024326324463\n",
      "256 0.2539202570915222\n",
      "335 0.29710012674331665\n",
      "343 0.2440398782491684\n",
      "400 0.2511880695819855\n",
      "417 0.23313088715076447\n",
      "421 0.24550659954547882\n",
      "Component 1:\n",
      "23 0.21983039379119873\n",
      "111 0.2551604211330414\n",
      "156 0.3130127191543579\n",
      "Component 2:\n",
      "23 0.3266536593437195\n",
      "156 0.4058776795864105\n",
      "Component 3:\n",
      "23 0.8572849631309509\n",
      "66 0.23336535692214966\n",
      "111 0.3592255413532257\n",
      "156 1.0655235052108765\n",
      "162 0.25350555777549744\n",
      "193 0.24525998532772064\n",
      "209 0.221907839179039\n",
      "271 0.40670523047447205\n",
      "334 0.23112164437770844\n",
      "378 0.2777670621871948\n",
      "386 0.21385173499584198\n",
      "394 0.2510640621185303\n",
      "410 0.23709604144096375\n",
      "473 0.2567783296108246\n",
      "503 0.30946648120880127\n",
      "Component 4:\n",
      "Component 5:\n",
      "Component 6:\n",
      "14 0.322757363319397\n",
      "56 0.4079655706882477\n",
      "89 0.22097961604595184\n",
      "98 0.44845256209373474\n",
      "111 1.0405858755111694\n",
      "156 0.2955071032047272\n",
      "271 0.3932470381259918\n",
      "369 0.2944613993167877\n",
      "410 0.39667022228240967\n",
      "503 0.43121686577796936\n",
      "Component 7:\n",
      "Component 8:\n",
      "Component 9:\n",
      "23 0.21631821990013123\n",
      "47 0.5048449635505676\n",
      "89 0.2423127293586731\n",
      "128 0.2238350659608841\n",
      "186 0.3030482232570648\n",
      "263 0.2558574080467224\n",
      "367 0.2286524772644043\n",
      "390 0.2581014931201935\n",
      "416 0.2274618148803711\n",
      "457 0.21728192269802094\n",
      "Component 10:\n",
      "Component 11:\n",
      "Component 12:\n",
      "488 0.23767274618148804\n",
      "Component 13:\n",
      "Component 14:\n",
      "Component 15:\n",
      "total neurons: 55\n"
     ]
    }
   ],
   "source": [
    "neurons_to_ablate = {}\n",
    "total_neurons = 0\n",
    "for component_idx, effect in enumerate(nodes.values()):\n",
    "    print(f\"Component {component_idx}:\")\n",
    "    neurons_to_ablate[submodules[component_idx]] = []\n",
    "    for idx in (effect.act > 0.2135).nonzero():\n",
    "        print(idx.item(), effect[idx].item())\n",
    "        neurons_to_ablate[submodules[component_idx]].append(idx.item())\n",
    "        total_neurons += 1\n",
    "print(f\"total neurons: {total_neurons}\")\n",
    "\n",
    "neurons_to_ablate = {\n",
    "    submodule : n_hot([neuron_idx], dim=512) for submodule, neuron_idx in neurons_to_ablate.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_act_abl(text):\n",
    "    with t.no_grad(), model.trace(text, **tracer_kwargs):\n",
    "        for submodule in submodules:\n",
    "            x = submodule.output\n",
    "            if is_tuple[submodule]:\n",
    "                x = x[0]\n",
    "            x[...,neurons_to_ablate[submodule]] = 0.\n",
    "            if is_tuple[submodule]:\n",
    "                submodule.output[0][:] = x\n",
    "            else:\n",
    "                submodule.output = x\n",
    "\n",
    "        attn_mask = model.input[1]['attention_mask']\n",
    "        act = model.gpt_neox.layers[layer].output[0]\n",
    "        act = act * attn_mask[:, :, None]\n",
    "        act = act.sum(1) / attn_mask.sum(1)[:, None]\n",
    "        act = act.save()\n",
    "    return act.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous test accuracy: 0.7592936754226685\n",
      "Ground truth accuracy: 0.6296082735061646\n",
      "Spurious accuracy: 0.6238479018211365\n"
     ]
    }
   ],
   "source": [
    "print('Ambiguous test accuracy:', test_probe(probe, get_acts_abl, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(probe, get_acts_abl, batches=batches, label_idx=0))\n",
    "print('Spurious accuracy:', test_probe(probe, get_acts_abl, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.5157667398452759\n",
      "Accuracy for (0, 1): 0.0402553491294384\n",
      "Accuracy for (1, 0): 0.9838709831237793\n",
      "Accuracy for (1, 1): 0.999535322189331\n"
     ]
    }
   ],
   "source": [
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(probe, get_acts_abl, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get skyline feature performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:50<00:00,  2.04s/it]\n"
     ]
    }
   ],
   "source": [
    "n_batches = 25\n",
    "batch_size = 4\n",
    "\n",
    "running_total = 0\n",
    "running_nodes = None\n",
    "\n",
    "for batch_idx, (clean, _, labels) in tqdm(enumerate(get_data(train=True, ambiguous=False, batch_size=batch_size, seed=SEED)), total=n_batches):\n",
    "    if batch_idx == n_batches:\n",
    "        break\n",
    "\n",
    "    effects, _, _, _ = patching_effect(\n",
    "        clean,\n",
    "        None,\n",
    "        model,\n",
    "        submodules,\n",
    "        dictionaries,\n",
    "        metric_fn,\n",
    "        metric_kwargs=dict(labels=labels),\n",
    "        method='ig'\n",
    "    )\n",
    "    with t.no_grad():\n",
    "        if running_nodes is None:\n",
    "            running_nodes = {k : len(clean) * v.sum(dim=1).mean(dim=0) for k, v in effects.items()}\n",
    "        else:\n",
    "            for k, v in effects.items():\n",
    "                running_nodes[k] += len(clean) * v.sum(dim=1).mean(dim=0)\n",
    "        running_total += len(clean)\n",
    "    del effects, _\n",
    "    gc.collect()\n",
    "\n",
    "nodes = {k : v / running_total for k, v in running_nodes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 0:\n",
      "946 0.2463148683309555\n",
      "7392 0.7006653547286987\n",
      "17846 0.259246826171875\n",
      "28533 0.3435993492603302\n",
      "29088 0.1107618659734726\n",
      "29476 0.14512550830841064\n",
      "31467 0.34349972009658813\n",
      "32081 0.17352652549743652\n",
      "32469 1.1966392993927002\n",
      "Component 1:\n",
      "Component 2:\n",
      "3842 0.18715900182724\n",
      "10258 0.20733851194381714\n",
      "13387 0.2763107717037201\n",
      "18382 0.1151869148015976\n",
      "19369 0.11484511196613312\n",
      "28127 0.9618880748748779\n",
      "30518 0.20647713541984558\n",
      "31645 0.24555140733718872\n",
      "Component 3:\n",
      "1022 0.49627578258514404\n",
      "3122 0.2066667079925537\n",
      "9651 0.44658446311950684\n",
      "10060 2.178098678588867\n",
      "18967 0.975749135017395\n",
      "22084 0.16960865259170532\n",
      "23898 0.27509018778800964\n",
      "26504 0.135464608669281\n",
      "29626 0.35908496379852295\n",
      "Component 4:\n",
      "Component 5:\n",
      "24159 0.30867698788642883\n",
      "25018 0.32441073656082153\n",
      "Component 6:\n",
      "4592 0.2154429703950882\n",
      "8920 0.5691888332366943\n",
      "9877 0.2772234380245209\n",
      "12128 0.5521409511566162\n",
      "12436 0.2059258371591568\n",
      "15017 3.2283241748809814\n",
      "26204 0.12540720403194427\n",
      "30248 0.6767085790634155\n",
      "Component 7:\n",
      "27472 1.0401958227157593\n",
      "Component 8:\n",
      "Component 9:\n",
      "1995 0.868719220161438\n",
      "9128 1.5642368793487549\n",
      "11656 0.16725711524486542\n",
      "12440 0.15728503465652466\n",
      "29206 0.18692830204963684\n",
      "29295 0.3987314999103546\n",
      "30263 0.12629985809326172\n",
      "Component 10:\n",
      "2959 0.8675453066825867\n",
      "19128 0.2839542031288147\n",
      "22029 0.13722524046897888\n",
      "Component 11:\n",
      "Component 12:\n",
      "19558 1.3076809644699097\n",
      "23545 0.3946571946144104\n",
      "27334 0.16667354106903076\n",
      "Component 13:\n",
      "22821 0.1131003201007843\n",
      "31101 0.4919366240501404\n",
      "Component 14:\n",
      "Component 15:\n",
      "9766 0.17570145428180695\n",
      "12420 1.163404107093811\n",
      "30220 0.2979729175567627\n",
      "total features: 55\n"
     ]
    }
   ],
   "source": [
    "top_feats_to_ablate = {}\n",
    "total_features = 0\n",
    "for component_idx, effect in enumerate(nodes.values()):\n",
    "    print(f\"Component {component_idx}:\")\n",
    "    top_feats_to_ablate[submodules[component_idx]] = []\n",
    "    for idx in (effect > 0.1107).nonzero():\n",
    "        print(idx.item(), effect[idx].item())\n",
    "        top_feats_to_ablate[submodules[component_idx]].append(idx.item())\n",
    "        total_features += 1\n",
    "print(f\"total features: {total_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_feats_to_ablate = {\n",
    "    submodule : n_hot(feats) for submodule, feats in top_feats_to_ablate.items()\n",
    "}\n",
    "get_acts_abl = lambda text : get_acts_ablated(text, model, submodules, dictionaries, top_feats_to_ablate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous test accuracy: 0.9309943914413452\n",
      "Ground truth accuracy: 0.8853686451911926\n",
      "Spurious accuracy: 0.5432027578353882\n"
     ]
    }
   ],
   "source": [
    "print('Ambiguous test accuracy:', test_probe(probe, get_acts_abl, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(probe, get_acts_abl, batches=batches, label_idx=0))\n",
    "print('Spurious accuracy:', test_probe(probe, get_acts_abl, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.9738969206809998\n",
      "Accuracy for (0, 1): 0.9248967170715332\n",
      "Accuracy for (1, 0): 0.7603686451911926\n",
      "Accuracy for (1, 1): 0.8887081742286682\n"
     ]
    }
   ],
   "source": [
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(probe, get_acts_abl, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retraining probe on activations after ablating features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous test accuracy: 0.9572490453720093\n",
      "Ground truth accuracy: 0.9308755993843079\n",
      "Unintended feature accuracy: 0.5195852518081665\n"
     ]
    }
   ],
   "source": [
    "get_acts_abl = lambda text : get_acts_ablated(text, model, submodules, dictionaries, feats_to_ablate)\n",
    "\n",
    "new_probe, _ = train_probe(get_acts_abl, label_idx=0)\n",
    "print('Ambiguous test accuracy:', test_probe(new_probe, get_acts_abl, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(new_probe, get_acts_abl, batches=batches, label_idx=0))\n",
    "print('Unintended feature accuracy:', test_probe(new_probe, get_acts_abl, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.9477938413619995\n",
      "Accuracy for (0, 1): 0.8895981907844543\n",
      "Accuracy for (1, 0): 0.9423962831497192\n",
      "Accuracy for (1, 1): 0.9679368138313293\n"
     ]
    }
   ],
   "source": [
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(new_probe, get_acts_abl, batches=batches, label_idx=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
