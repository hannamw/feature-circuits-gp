{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/u/smarks/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "import pandas as pd\n",
    "import torch as t\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "\n",
    "model = LanguageModel('EleutherAI/pythia-70m-deduped', device_map=device)\n",
    "\n",
    "data = pd.read_json('/share/data/datasets/msgs/syntactic_category_lexical_content_the/train.jsonl', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>condition</th>\n",
       "      <th>linguistic_feature_label</th>\n",
       "      <th>surface_feature_label</th>\n",
       "      <th>UID</th>\n",
       "      <th>linguistic_feature_type</th>\n",
       "      <th>linguistic_feature_description</th>\n",
       "      <th>surface_feature_type</th>\n",
       "      <th>surface_feature_description</th>\n",
       "      <th>control_paradigm</th>\n",
       "      <th>sentenceID</th>\n",
       "      <th>paradigmID</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>All grandsons do resemble the print and Debra ...</td>\n",
       "      <td>training</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>syntactic_category_lexical_content_the</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Is there an adjective present?</td>\n",
       "      <td>lexical_content</td>\n",
       "      <td>Is the word \"the\" present?</td>\n",
       "      <td>False</td>\n",
       "      <td>40000</td>\n",
       "      <td>5000</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>All grandsons do resemble each print and Debra...</td>\n",
       "      <td>training</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>syntactic_category_lexical_content_the</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Is there an adjective present?</td>\n",
       "      <td>lexical_content</td>\n",
       "      <td>Is the word \"the\" present?</td>\n",
       "      <td>False</td>\n",
       "      <td>40001</td>\n",
       "      <td>5000</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Each colleague isn't criticizing the analyses ...</td>\n",
       "      <td>training</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>syntactic_category_lexical_content_the</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Is there an adjective present?</td>\n",
       "      <td>lexical_content</td>\n",
       "      <td>Is the word \"the\" present?</td>\n",
       "      <td>False</td>\n",
       "      <td>40008</td>\n",
       "      <td>5001</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Each colleague isn't criticizing these analyse...</td>\n",
       "      <td>training</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>syntactic_category_lexical_content_the</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Is there an adjective present?</td>\n",
       "      <td>lexical_content</td>\n",
       "      <td>Is the word \"the\" present?</td>\n",
       "      <td>False</td>\n",
       "      <td>40009</td>\n",
       "      <td>5001</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Few governments hadn't sold several hospitals ...</td>\n",
       "      <td>training</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>syntactic_category_lexical_content_the</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Is there an adjective present?</td>\n",
       "      <td>lexical_content</td>\n",
       "      <td>Is the word \"the\" present?</td>\n",
       "      <td>False</td>\n",
       "      <td>40016</td>\n",
       "      <td>5002</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>Every associate caught some driver and a stude...</td>\n",
       "      <td>training</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>syntactic_category_lexical_content_the</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Is there an adjective present?</td>\n",
       "      <td>lexical_content</td>\n",
       "      <td>Is the word \"the\" present?</td>\n",
       "      <td>False</td>\n",
       "      <td>79977</td>\n",
       "      <td>9997</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Some actresses drop by the hospitals and a bos...</td>\n",
       "      <td>training</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>syntactic_category_lexical_content_the</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Is there an adjective present?</td>\n",
       "      <td>lexical_content</td>\n",
       "      <td>Is the word \"the\" present?</td>\n",
       "      <td>False</td>\n",
       "      <td>79984</td>\n",
       "      <td>9998</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Some actresses drop by fewer than three hospit...</td>\n",
       "      <td>training</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>syntactic_category_lexical_content_the</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Is there an adjective present?</td>\n",
       "      <td>lexical_content</td>\n",
       "      <td>Is the word \"the\" present?</td>\n",
       "      <td>False</td>\n",
       "      <td>79985</td>\n",
       "      <td>9998</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Every senator purchases the shirts and a smart...</td>\n",
       "      <td>training</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>syntactic_category_lexical_content_the</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Is there an adjective present?</td>\n",
       "      <td>lexical_content</td>\n",
       "      <td>Is the word \"the\" present?</td>\n",
       "      <td>False</td>\n",
       "      <td>79992</td>\n",
       "      <td>9999</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Every senator purchases most shirts and a pers...</td>\n",
       "      <td>training</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>syntactic_category_lexical_content_the</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>Is there an adjective present?</td>\n",
       "      <td>lexical_content</td>\n",
       "      <td>Is the word \"the\" present?</td>\n",
       "      <td>False</td>\n",
       "      <td>79993</td>\n",
       "      <td>9999</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence condition  \\\n",
       "0     All grandsons do resemble the print and Debra ...  training   \n",
       "1     All grandsons do resemble each print and Debra...  training   \n",
       "2     Each colleague isn't criticizing the analyses ...  training   \n",
       "3     Each colleague isn't criticizing these analyse...  training   \n",
       "4     Few governments hadn't sold several hospitals ...  training   \n",
       "...                                                 ...       ...   \n",
       "9995  Every associate caught some driver and a stude...  training   \n",
       "9996  Some actresses drop by the hospitals and a bos...  training   \n",
       "9997  Some actresses drop by fewer than three hospit...  training   \n",
       "9998  Every senator purchases the shirts and a smart...  training   \n",
       "9999  Every senator purchases most shirts and a pers...  training   \n",
       "\n",
       "      linguistic_feature_label  surface_feature_label  \\\n",
       "0                            1                      1   \n",
       "1                            0                      0   \n",
       "2                            1                      1   \n",
       "3                            0                      0   \n",
       "4                            1                      1   \n",
       "...                        ...                    ...   \n",
       "9995                         0                      0   \n",
       "9996                         1                      1   \n",
       "9997                         0                      0   \n",
       "9998                         1                      1   \n",
       "9999                         0                      0   \n",
       "\n",
       "                                         UID linguistic_feature_type  \\\n",
       "0     syntactic_category_lexical_content_the               syntactic   \n",
       "1     syntactic_category_lexical_content_the               syntactic   \n",
       "2     syntactic_category_lexical_content_the               syntactic   \n",
       "3     syntactic_category_lexical_content_the               syntactic   \n",
       "4     syntactic_category_lexical_content_the               syntactic   \n",
       "...                                      ...                     ...   \n",
       "9995  syntactic_category_lexical_content_the               syntactic   \n",
       "9996  syntactic_category_lexical_content_the               syntactic   \n",
       "9997  syntactic_category_lexical_content_the               syntactic   \n",
       "9998  syntactic_category_lexical_content_the               syntactic   \n",
       "9999  syntactic_category_lexical_content_the               syntactic   \n",
       "\n",
       "      linguistic_feature_description surface_feature_type  \\\n",
       "0     Is there an adjective present?      lexical_content   \n",
       "1     Is there an adjective present?      lexical_content   \n",
       "2     Is there an adjective present?      lexical_content   \n",
       "3     Is there an adjective present?      lexical_content   \n",
       "4     Is there an adjective present?      lexical_content   \n",
       "...                              ...                  ...   \n",
       "9995  Is there an adjective present?      lexical_content   \n",
       "9996  Is there an adjective present?      lexical_content   \n",
       "9997  Is there an adjective present?      lexical_content   \n",
       "9998  Is there an adjective present?      lexical_content   \n",
       "9999  Is there an adjective present?      lexical_content   \n",
       "\n",
       "     surface_feature_description  control_paradigm  sentenceID  paradigmID  \\\n",
       "0     Is the word \"the\" present?             False       40000        5000   \n",
       "1     Is the word \"the\" present?             False       40001        5000   \n",
       "2     Is the word \"the\" present?             False       40008        5001   \n",
       "3     Is the word \"the\" present?             False       40009        5001   \n",
       "4     Is the word \"the\" present?             False       40016        5002   \n",
       "...                          ...               ...         ...         ...   \n",
       "9995  Is the word \"the\" present?             False       79977        9997   \n",
       "9996  Is the word \"the\" present?             False       79984        9998   \n",
       "9997  Is the word \"the\" present?             False       79985        9998   \n",
       "9998  Is the word \"the\" present?             False       79992        9999   \n",
       "9999  Is the word \"the\" present?             False       79993        9999   \n",
       "\n",
       "      split  \n",
       "0     train  \n",
       "1     train  \n",
       "2     train  \n",
       "3     train  \n",
       "4     train  \n",
       "...     ...  \n",
       "9995  train  \n",
       "9996  train  \n",
       "9997  train  \n",
       "9998  train  \n",
       "9999  train  \n",
       "\n",
       "[10000 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Probe(nn.Module):\n",
    "    def __init__(self, activation_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(activation_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x).squeeze(-1)\n",
    "        softmaxed_logits = (t.softmax(logits, dim=-1) * logits).sum(dim=-1)\n",
    "        return t.sigmoid(softmaxed_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "lr = 1e-2\n",
    "\n",
    "probe = Probe(512).to(device)\n",
    "optimizer = t.optim.Adam(probe.parameters(), lr=lr)\n",
    "losses = []\n",
    "\n",
    "for batch_idx in range(len(data) // batch_size):\n",
    "    inputs = data['sentence'][batch_idx * batch_size:(batch_idx + 1) * batch_size].tolist()\n",
    "    label = data['linguistic_feature_label'][batch_idx * batch_size:(batch_idx + 1) * batch_size].tolist()\n",
    "\n",
    "    with model.invoke(inputs) as invoker:\n",
    "        hidden_states = model.gpt_neox.layers[-3].output[0].save()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    logits = probe(hidden_states.value.clone())\n",
    "    loss = nn.BCELoss()(logits, t.Tensor(label).to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7080693244934082,\n",
       " 0.7173736095428467,\n",
       " 0.6817024946212769,\n",
       " 0.6360753774642944,\n",
       " 0.6241862773895264,\n",
       " 0.5873124599456787,\n",
       " 0.5238829851150513,\n",
       " 0.4599684774875641,\n",
       " 0.4041571617126465,\n",
       " 0.33180248737335205,\n",
       " 0.28367525339126587,\n",
       " 0.21787819266319275,\n",
       " 0.20481103658676147,\n",
       " 0.17532525956630707,\n",
       " 0.1501021683216095,\n",
       " 0.12602679431438446,\n",
       " 0.11127845197916031,\n",
       " 0.09764689207077026,\n",
       " 0.07033877074718475,\n",
       " 0.08369386196136475,\n",
       " 0.06978712975978851,\n",
       " 0.057081498205661774,\n",
       " 0.060696087777614594,\n",
       " 0.06456602364778519,\n",
       " 0.048354603350162506,\n",
       " 0.04755610227584839,\n",
       " 0.04461493343114853,\n",
       " 0.05637466534972191,\n",
       " 0.03905995935201645,\n",
       " 0.028560271486639977,\n",
       " 0.04731447994709015,\n",
       " 0.030016174539923668,\n",
       " 0.0315505675971508,\n",
       " 0.02036595344543457,\n",
       " 0.025361012667417526,\n",
       " 0.023468855768442154,\n",
       " 0.023788487538695335,\n",
       " 0.02086271159350872,\n",
       " 0.023363469168543816]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ling acc: 0.7715010683760684\n",
      "surface acc: 0.8924278846153846\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_json('/share/data/datasets/msgs/syntactic_category_lexical_content_the/test.jsonl', lines=True)\n",
    "\n",
    "ling_accs, surface_accs = [], []\n",
    "# get accuracy on test data\n",
    "for batch_idx in range(len(test_data) // batch_size):\n",
    "    inputs = test_data['sentence'][batch_idx * batch_size:(batch_idx + 1) * batch_size].tolist()\n",
    "    ling_labels = test_data['linguistic_feature_label'][batch_idx * batch_size:(batch_idx + 1) * batch_size].tolist()\n",
    "    surface_labels = test_data['surface_feature_label'][batch_idx * batch_size:(batch_idx + 1) * batch_size].tolist()\n",
    "\n",
    "    with model.invoke(inputs) as invoker:\n",
    "        hidden_states = model.gpt_neox.layers[-3].output[0].save()\n",
    "    \n",
    "    with t.no_grad():\n",
    "        preds = probe(hidden_states.value)\n",
    "        ling_acc = (preds.round() == t.Tensor(ling_labels).to('cuda:0')).float().mean()\n",
    "        surface_acc = (preds.round() == t.Tensor(surface_labels).to('cuda:0')).float().mean()\n",
    "        ling_accs.append(ling_acc.item())\n",
    "        surface_accs.append(surface_acc.item())\n",
    "\n",
    "print('ling acc:', sum(ling_accs) / len(ling_accs))\n",
    "print('surface acc:', sum(surface_accs) / len(surface_accs))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attribution import patching_effect\n",
    "from dictionary_learning.dictionary import AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9941, 0.4238], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean = \"All grandsons do resemble the print and Debra is an organized child.\"\n",
    "patch = \"All grandsons do resemble a print and Debra is an organized child.\"\n",
    "\n",
    "with model.invoke([clean, patch]) as invoker:\n",
    "    hidden_states = model.gpt_neox.layers[-3].output[0].save()\n",
    "\n",
    "with t.no_grad():\n",
    "    preds = probe(hidden_states.value)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_fn(model):\n",
    "    return probe(model.gpt_neox.layers[-3].output[0])\n",
    "\n",
    "submodules = [\n",
    "    model.gpt_neox.layers[i].mlp for i in range(4)\n",
    "]\n",
    "dictionaries = []\n",
    "for i in range(len(submodules)):\n",
    "    dictionary = AutoEncoder(512, 64 * 512).to(device)\n",
    "    dictionary.load_state_dict(t.load(f'/share/projects/dictionary_circuits/autoencoders/pythia-70m-deduped/mlp_out_layer{i}/1_32768/ae.pt'))\n",
    "    dictionaries.append(dictionary)\n",
    "\n",
    "out = patching_effect(\n",
    "    clean,\n",
    "    patch,\n",
    "    model,\n",
    "    submodules,\n",
    "    dictionaries,\n",
    "    metric_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total effect: tensor([-0.5703], device='cuda:0', grad_fn=<SubBackward0>)\n",
      "Layer 0:\n",
      "Layer 1:\n",
      "Layer 2:\n",
      "Layer 3:\n"
     ]
    }
   ],
   "source": [
    "effects, total_effect = out\n",
    "print(f\"Total effect: {total_effect}\")\n",
    "for layer, submodule in enumerate(submodules):\n",
    "    print(f\"Layer {layer}:\")\n",
    "    effect = effects[submodule]\n",
    "    for feature_idx in t.nonzero(effect):\n",
    "        value = effect[tuple(feature_idx)]\n",
    "        if value > 0.005:\n",
    "            print(f\"    Multindex: {tuple(feature_idx.tolist())}, Value: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
