{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "from nnsight import LanguageModel\n",
    "import pandas as pd\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from attribution import patching_effect\n",
    "from dictionary_learning import AutoEncoder, ActivationBuffer\n",
    "from dictionary_learning.interp import examine_dimension\n",
    "from dictionary_learning.utils import zst_to_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "\n",
    "model = LanguageModel('EleutherAI/pythia-70m-deduped', device_map=device)\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_ambiguous(split='train', seed=SEED):\n",
    "    t.manual_seed(seed)\n",
    "    data = pd.read_csv(f'data/{split}_data.csv')\n",
    "    labels = t.randint(0, 2, (len(data),)).to(device)\n",
    "    inputs = [\n",
    "        row['singular'].lower() if label == 0 else row['plural'].upper() for (_, row), label in zip(data.iterrows(), labels)\n",
    "    ]\n",
    "    return inputs, labels\n",
    "\n",
    "def data_unambiguous(split='train', seed=SEED):\n",
    "    t.manual_seed(seed)\n",
    "    data = pd.read_csv(f'data/{split}_data.csv')\n",
    "    feat1_labels = t.randint(0, 2, (len(data),)).to(device)\n",
    "    feat2_labels = t.randint(0, 2, (len(data),)).to(device)\n",
    "    inputs = []\n",
    "    for (_, row), label1, label2 in zip(data.iterrows(), feat1_labels, feat2_labels):\n",
    "        if label1 == 0 and label2 == 0:\n",
    "            inputs.append(row['singular'])\n",
    "        elif label1 == 0 and label2 == 1:\n",
    "            inputs.append(row['singular'].upper())\n",
    "        elif label1 == 1 and label2 == 0:\n",
    "            inputs.append(row['plural'])\n",
    "        elif label1 == 1 and label2 == 1:\n",
    "            inputs.append(row['plural'].upper())\n",
    "    return inputs, feat1_labels, feat2_labels\n",
    "\n",
    "def data_gen(ambiguous=True, split='train', seed=SEED):\n",
    "    if ambiguous:\n",
    "        return data_ambiguous(split=split, seed=seed)\n",
    "    else:\n",
    "        return data_unambiguous(split=split, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_ablations(\n",
    "        model,\n",
    "        inputs,\n",
    "        submodules,\n",
    "        dictionaries,\n",
    "        to_ablate,\n",
    "        out_fn,\n",
    "        inference=True,\n",
    "):\n",
    "    with model.invoke(inputs, fwd_args={'inference': inference}):\n",
    "        for submodule, dictionary in zip(submodules, dictionaries):\n",
    "            x = submodule.output\n",
    "            is_resid = (type(x.shape) == tuple)\n",
    "            if is_resid:\n",
    "                x = x[0]\n",
    "            x_hat = dictionary(x)\n",
    "            residual = x - x_hat\n",
    "\n",
    "            f = dictionary.encode(x)\n",
    "            ablation_idxs = t.Tensor(to_ablate[submodule]).long()\n",
    "            f[:, :, ablation_idxs] = 0.\n",
    "            x_hat = dictionary.decode(f)\n",
    "            if is_resid:\n",
    "                submodule.output[0][:] = x_hat + residual\n",
    "            else:\n",
    "                submodule.output = x_hat + residual\n",
    "            \n",
    "        out = out_fn(model).save()\n",
    "    return out.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Probe(nn.Module):\n",
    "    def __init__(self, activation_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(activation_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x).squeeze(-1)\n",
    "        return logits.sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "epochs = 20\n",
    "layer = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Control probe 1 accuracy: 0.9635036587715149\n",
      "Control probe 2 accuracy: 0.9781022071838379\n"
     ]
    }
   ],
   "source": [
    "inputs, feat1_labels, feat2_labels = data_gen(ambiguous=False, split='train')\n",
    "\n",
    "with model.invoke(inputs):\n",
    "    acts = model.gpt_neox.layers[layer].output[0][:,-1,:].save()\n",
    "acts = acts.value.clone()\n",
    "\n",
    "probe1 = Probe(acts.shape[-1]).to(device)\n",
    "probe2 = Probe(acts.shape[-1]).to(device)\n",
    "opt1 = t.optim.AdamW(probe1.parameters(), lr=lr)\n",
    "opt2 = t.optim.AdamW(probe2.parameters(), lr=lr)\n",
    "\n",
    "for _ in range(epochs):\n",
    "    opt1.zero_grad(), opt2.zero_grad()\n",
    "    logits1 = probe1(acts)\n",
    "    logits2 = probe2(acts)\n",
    "    loss1 = nn.BCELoss()(logits1, feat1_labels.float())\n",
    "    loss2 = nn.BCELoss()(logits2, feat2_labels.float())\n",
    "    loss1.backward(), loss2.backward()\n",
    "    opt1.step(), opt2.step()\n",
    "\n",
    "inputs, feat1_labels, feat2_labels = data_gen(ambiguous=False, split='test')\n",
    "\n",
    "with model.invoke(inputs):\n",
    "    acts = model.gpt_neox.layers[layer].output[0][:,-1,:].save()\n",
    "acts = acts.value.clone()\n",
    "\n",
    "probs1 = probe1(acts)\n",
    "probs2 = probe2(acts)\n",
    "preds1, preds2 = probs1.round(), probs2.round()\n",
    "acc1 = (preds1 == feat1_labels).float().mean().item()\n",
    "acc2 = (preds2 == feat2_labels).float().mean().item()\n",
    "\n",
    "print(f'Control probe 1 accuracy: {acc1}')\n",
    "print(f'Control probe 2 accuracy: {acc2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on ambiguous data: 1.0\n",
      "feat1 accuracy: 0.5839415788650513\n",
      "feat2 accuracy: 0.9635036587715149\n"
     ]
    }
   ],
   "source": [
    "t.manual_seed(SEED)\n",
    "probe = Probe(512).to(device)\n",
    "optimizer = t.optim.AdamW(probe.parameters(), lr=lr)\n",
    "\n",
    "# train probe on ambiguous data\n",
    "inputs, labels = data_gen(ambiguous=True, split='train')\n",
    "with model.invoke(inputs):\n",
    "    acts = model.gpt_neox.layers[layer].output[0][:,-1,:].save()\n",
    "acts = acts.value.clone()\n",
    "\n",
    "for _ in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    probs = probe(acts)\n",
    "    loss = nn.BCELoss()(probs, labels.float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# get accuracy on ambiguous test set\n",
    "inputs, labels = data_gen(ambiguous=True, split='test')\n",
    "with model.invoke(inputs):\n",
    "    acts = model.gpt_neox.layers[layer].output[0][:,-1,:].save()\n",
    "acts = acts.value.clone()\n",
    "preds = probe(acts).round()\n",
    "acc = (preds == labels).float().mean().item()\n",
    "print(f'Accuracy on ambiguous data: {acc}')\n",
    "\n",
    "# get accuracy on unambiguous test set\n",
    "inputs, feat1_labels, feat2_labels = data_gen(ambiguous=False, split='test')\n",
    "with model.invoke(inputs):\n",
    "    acts = model.gpt_neox.layers[layer].output[0][:,-1,:].save()\n",
    "acts = acts.value.clone()\n",
    "preds = probe(acts).round()\n",
    "acc = (preds == feat1_labels).float().mean().item()\n",
    "print(f'feat1 accuracy: {acc}')\n",
    "preds = probe(acts).round()\n",
    "acc = (preds == feat2_labels).float().mean().item()\n",
    "print(f'feat2 accuracy: {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0:\n",
      "    Multindex: (23,), Value: 0.001021671574562788\n",
      "    Multindex: (670,), Value: 0.0018523469334468246\n",
      "    Multindex: (4324,), Value: 0.009785940870642662\n",
      "    Multindex: (7466,), Value: 0.0032092721667140722\n",
      "    Multindex: (15637,), Value: 0.0031055279541760683\n",
      "    Multindex: (18773,), Value: 0.0024783434346318245\n",
      "    Multindex: (20334,), Value: 0.0026556900702416897\n",
      "    Multindex: (22464,), Value: 0.0010544315446168184\n",
      "Layer 1:\n",
      "    Multindex: (7633,), Value: 0.0019621977116912603\n",
      "    Multindex: (12833,), Value: 0.007209473289549351\n",
      "    Multindex: (15714,), Value: 0.001227965927682817\n",
      "    Multindex: (19812,), Value: 0.00587422912940383\n",
      "Layer 2:\n",
      "    Multindex: (16421,), Value: 0.0016992193413898349\n",
      "    Multindex: (17187,), Value: 0.0012213450390845537\n",
      "Layer 0:\n",
      "    Multindex: (5493,), Value: -0.001290997490286827\n",
      "    Multindex: (5650,), Value: -0.002565438859164715\n",
      "    Multindex: (10316,), Value: -0.0012732871109619737\n",
      "    Multindex: (13114,), Value: -0.0017375940224155784\n",
      "    Multindex: (22182,), Value: -0.006937430705875158\n",
      "    Multindex: (25864,), Value: -0.01203988678753376\n",
      "    Multindex: (26261,), Value: -0.008188670501112938\n",
      "    Multindex: (26532,), Value: -0.0015825446462258697\n",
      "Layer 1:\n",
      "    Multindex: (2779,), Value: -0.0011607909109443426\n",
      "    Multindex: (4900,), Value: -0.0018414132064208388\n",
      "    Multindex: (5469,), Value: -0.0011607909109443426\n",
      "    Multindex: (5509,), Value: -0.0011849591974169016\n",
      "    Multindex: (7368,), Value: -0.001158823142759502\n",
      "    Multindex: (11305,), Value: -0.0014622160233557224\n",
      "    Multindex: (14465,), Value: -0.005697689484804869\n",
      "    Multindex: (18471,), Value: -0.028234507888555527\n",
      "    Multindex: (25963,), Value: -0.0025828201323747635\n",
      "    Multindex: (26424,), Value: -0.0011607909109443426\n",
      "    Multindex: (28336,), Value: -0.0010012311395257711\n",
      "    Multindex: (32267,), Value: -0.007232098840177059\n",
      "Layer 2:\n",
      "    Multindex: (3428,), Value: -0.0015510988887399435\n",
      "    Multindex: (8540,), Value: -0.0012119706952944398\n",
      "    Multindex: (9068,), Value: -0.002557158935815096\n",
      "    Multindex: (17880,), Value: -0.007695702835917473\n",
      "    Multindex: (22968,), Value: -0.020400989800691605\n",
      "    Multindex: (27888,), Value: -0.013930395245552063\n",
      "    Multindex: (30868,), Value: -0.005351516418159008\n",
      "    Multindex: (31260,), Value: -0.002593507757410407\n"
     ]
    }
   ],
   "source": [
    "submodules = [\n",
    "    model.gpt_neox.layers[i] for i in range(layer + 1)\n",
    "]\n",
    "dictionaries = []\n",
    "for i in range(layer + 1):\n",
    "    ae = AutoEncoder(512, 64 * 512).to(device)\n",
    "    ae.load_state_dict(t.load(f'/share/projects/dictionary_circuits/autoencoders/pythia-70m-deduped/resid_out_layer{i}/0_32768/ae.pt'))\n",
    "    dictionaries.append(ae)\n",
    "\n",
    "def metric_fn(model):\n",
    "    return probe(model.gpt_neox.layers[layer].output[0][:,-1,:])\n",
    "\n",
    "inputs, labels = data_gen(ambiguous=True, split='train')\n",
    "\n",
    "neg_inputs, pos_inputs = [], []\n",
    "for x, label in zip(inputs, labels):\n",
    "    if label == 0:\n",
    "        neg_inputs.append(x)\n",
    "    else:\n",
    "        pos_inputs.append(x)\n",
    "\n",
    "\n",
    "neg_effects, _ = patching_effect(\n",
    "    neg_inputs,\n",
    "    None,\n",
    "    model,\n",
    "    submodules,\n",
    "    dictionaries,\n",
    "    metric_fn,\n",
    "    method='ig'\n",
    ")\n",
    "\n",
    "neg_effects = {k : v.mean(dim=0).mean(dim=0) for k, v in neg_effects.items()}\n",
    "for i, submodule in enumerate(submodules):\n",
    "    print(f\"Layer {i}:\")\n",
    "    effect = neg_effects[submodule]\n",
    "    for feature_idx in t.nonzero(effect):\n",
    "        value = effect[tuple(feature_idx)]\n",
    "        if value > 0.001:\n",
    "            print(f\"    Multindex: {tuple(feature_idx.tolist())}, Value: {value}\")\n",
    "\n",
    "pos_effects, _ = patching_effect(\n",
    "    pos_inputs,\n",
    "    None,\n",
    "    model,\n",
    "    submodules,\n",
    "    dictionaries,\n",
    "    metric_fn,\n",
    "    method='ig'\n",
    ")\n",
    "\n",
    "pos_effects = {k : v.mean(dim=0).mean(dim=0) for k, v in pos_effects.items()}\n",
    "\n",
    "for i, submodule in enumerate(submodules):\n",
    "    print(f\"Layer {i}:\")\n",
    "    effect = pos_effects[submodule]\n",
    "    for feature_idx in t.nonzero(effect):\n",
    "        value = effect[tuple(feature_idx)]\n",
    "        if -value > 0.001:\n",
    "            print(f\"    Multindex: {tuple(feature_idx.tolist())}, Value: {value}\")\n",
    "\n",
    "# total_effects = {\n",
    "#     k : -pos_effects[k] + neg_effects[k] for k in pos_effects.keys()\n",
    "# }\n",
    "# for i, submodule in enumerate(submodules):\n",
    "#     print(f\"Layer {i}:\")\n",
    "#     effect = total_effects[submodule]\n",
    "#     for feature_idx in t.nonzero(effect):\n",
    "#         value = effect[tuple(feature_idx)]\n",
    "#         if value > 0.001:\n",
    "#             print(f\"    Multindex: {tuple(feature_idx.tolist())}, Value: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('def', 8.276412010192871), ('ive', 8.197701454162598), ('ised', 7.696048736572266), (' Alternative', 6.6908488273620605), ('<|endoftext|>', 6.6041448035340675), ('--', 6.535338401794434), ('abc', 5.8198747634887695), ('recurs', 5.672883987426758), ('PCI', 4.438255310058594), ('Standard', 2.646510124206543)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-d1278dfa-cdef\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-d1278dfa-cdef\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [[\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"abc\", \"def\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"--\", \"recurs\", \"ive\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"Standard\", \"ised\"], [\"PCI\", \" Alternative\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\"]], \"activations\": [[[[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604144096374512]], [[6.604145050048828]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604143142700195]], [[6.604142189025879]], [[6.604145526885986]], [[6.604144096374512]], [[6.604144096374512]], [[6.60414457321167]], [[6.6041460037231445]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604145526885986]], [[6.604145526885986]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145526885986]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145050048828]], [[6.6041460037231445]], [[6.604145526885986]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604144096374512]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145526885986]], [[6.604146957397461]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145050048828]], [[6.604144096374512]], [[6.604147434234619]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145050048828]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.6041460037231445]], [[6.604145526885986]], [[6.6041436195373535]], [[6.604145050048828]], [[6.604145050048828]], [[6.60414457321167]], [[6.604146480560303]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.604145526885986]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.604145050048828]], [[6.60414457321167]], [[5.8198747634887695]], [[8.276412010192871]]], [[[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604144096374512]], [[6.604145050048828]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604143142700195]], [[6.604142189025879]], [[6.604145526885986]], [[6.604144096374512]], [[6.604144096374512]], [[6.60414457321167]], [[6.6041460037231445]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604145526885986]], [[6.604145526885986]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145526885986]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145050048828]], [[6.6041460037231445]], [[6.604145526885986]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604144096374512]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145526885986]], [[6.604146957397461]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145050048828]], [[6.604144096374512]], [[6.604147434234619]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145050048828]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.6041460037231445]], [[6.604145526885986]], [[6.6041436195373535]], [[6.604145050048828]], [[6.604145050048828]], [[6.60414457321167]], [[6.604146480560303]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.604145526885986]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.604145050048828]], [[6.60414457321167]], [[6.604145526885986]], [[6.60414457321167]], [[6.6041436195373535]], [[6.604143142700195]], [[6.604145050048828]], [[6.60414457321167]], [[6.604145050048828]], [[6.60414457321167]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604146957397461]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145526885986]], [[6.535338401794434]], [[5.672883987426758]], [[8.197701454162598]]], [[[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604144096374512]], [[6.604145050048828]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604143142700195]], [[6.604142189025879]], [[6.604145526885986]], [[6.604144096374512]], [[6.604144096374512]], [[6.60414457321167]], [[6.6041460037231445]], [[2.646510124206543]], [[7.696048736572266]]], [[[4.438255310058594]], [[6.6908488273620605]]], [[[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604144096374512]], [[6.604145050048828]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604143142700195]], [[6.604142189025879]], [[6.604145526885986]], [[6.604144096374512]], [[6.604144096374512]], [[6.60414457321167]], [[6.6041460037231445]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604145526885986]], [[6.604145526885986]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145526885986]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145050048828]], [[6.6041460037231445]], [[6.604145526885986]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604144096374512]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145526885986]], [[6.604146957397461]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145050048828]], [[6.604144096374512]], [[6.604147434234619]]], [[[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604144096374512]], [[6.604145050048828]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604143142700195]], [[6.604142189025879]], [[6.604145526885986]], [[6.604144096374512]], [[6.604144096374512]], [[6.60414457321167]], [[6.6041460037231445]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604145526885986]], [[6.604145526885986]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145526885986]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145050048828]], [[6.6041460037231445]], [[6.604145526885986]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604144096374512]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145526885986]], [[6.604146957397461]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145050048828]], [[6.604144096374512]], [[6.604147434234619]]], [[[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604144096374512]], [[6.604145050048828]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604143142700195]], [[6.604142189025879]], [[6.604145526885986]], [[6.604144096374512]], [[6.604144096374512]], [[6.60414457321167]], [[6.6041460037231445]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604145526885986]], [[6.604145526885986]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145526885986]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145050048828]], [[6.6041460037231445]], [[6.604145526885986]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604144096374512]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145526885986]], [[6.604146957397461]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145050048828]], [[6.604144096374512]], [[6.604147434234619]]], [[[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604144096374512]], [[6.604145050048828]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604143142700195]], [[6.604142189025879]], [[6.604145526885986]], [[6.604144096374512]], [[6.604144096374512]], [[6.60414457321167]], [[6.6041460037231445]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604145526885986]], [[6.604145526885986]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145526885986]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145050048828]], [[6.6041460037231445]], [[6.604145526885986]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604144096374512]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145526885986]], [[6.604146957397461]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145050048828]], [[6.604144096374512]], [[6.604147434234619]]], [[[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604144096374512]], [[6.604145050048828]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604143142700195]], [[6.604142189025879]], [[6.604145526885986]], [[6.604144096374512]], [[6.604144096374512]], [[6.60414457321167]], [[6.6041460037231445]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604145526885986]], [[6.604145526885986]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145526885986]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145050048828]], [[6.6041460037231445]], [[6.604145526885986]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604144096374512]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145526885986]], [[6.604146957397461]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145050048828]], [[6.604144096374512]], [[6.604147434234619]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145050048828]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.6041460037231445]], [[6.604145526885986]], [[6.6041436195373535]], [[6.604145050048828]], [[6.604145050048828]], [[6.60414457321167]], [[6.604146480560303]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.604145526885986]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.604145050048828]], [[6.60414457321167]], [[6.604145526885986]], [[6.60414457321167]], [[6.6041436195373535]], [[6.604143142700195]], [[6.604145050048828]], [[6.60414457321167]], [[6.604145050048828]], [[6.60414457321167]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604146957397461]]], [[[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604144096374512]], [[6.604145050048828]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604143142700195]], [[6.604142189025879]], [[6.604145526885986]], [[6.604144096374512]], [[6.604144096374512]], [[6.60414457321167]], [[6.6041460037231445]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604145526885986]], [[6.604145526885986]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145526885986]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145050048828]], [[6.6041460037231445]], [[6.604145526885986]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604144096374512]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145526885986]], [[6.604146957397461]]], [[[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604144096374512]], [[6.604145050048828]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604143142700195]], [[6.604142189025879]], [[6.604145526885986]], [[6.604144096374512]], [[6.604144096374512]], [[6.60414457321167]], [[6.6041460037231445]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604145526885986]], [[6.604145526885986]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145526885986]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145050048828]], [[6.6041460037231445]], [[6.604145526885986]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604144096374512]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145526885986]], [[6.604146957397461]]], [[[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604144096374512]], [[6.604145050048828]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604143142700195]], [[6.604142189025879]], [[6.604145526885986]], [[6.604144096374512]], [[6.604144096374512]], [[6.60414457321167]], [[6.6041460037231445]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604145526885986]], [[6.604145526885986]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145526885986]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145050048828]], [[6.6041460037231445]], [[6.604145526885986]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604144096374512]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145526885986]], [[6.604146957397461]]], [[[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604144096374512]], [[6.604145050048828]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604143142700195]], [[6.604142189025879]], [[6.604145526885986]], [[6.604144096374512]], [[6.604144096374512]], [[6.60414457321167]], [[6.6041460037231445]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604145526885986]], [[6.604145526885986]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145526885986]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145050048828]], [[6.6041460037231445]], [[6.604145526885986]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604144096374512]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145526885986]], [[6.604146957397461]]], [[[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604144096374512]], [[6.604145050048828]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604143142700195]], [[6.604142189025879]], [[6.604145526885986]], [[6.604144096374512]], [[6.604144096374512]], [[6.60414457321167]], [[6.6041460037231445]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604145526885986]], [[6.604145526885986]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145526885986]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145050048828]], [[6.6041460037231445]], [[6.604145526885986]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604144096374512]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145526885986]], [[6.604146957397461]]], [[[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604144096374512]], [[6.604145050048828]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604143142700195]], [[6.604142189025879]], [[6.604145526885986]], [[6.604144096374512]], [[6.604144096374512]], [[6.60414457321167]], [[6.6041460037231445]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604145526885986]], [[6.604145526885986]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145526885986]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145050048828]], [[6.6041460037231445]], [[6.604145526885986]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604144096374512]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145526885986]], [[6.604146957397461]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145050048828]], [[6.604144096374512]], [[6.604147434234619]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145050048828]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.6041460037231445]], [[6.604145526885986]], [[6.6041436195373535]], [[6.604145050048828]], [[6.604145050048828]], [[6.60414457321167]], [[6.604146480560303]]], [[[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604144096374512]], [[6.604145050048828]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604143142700195]], [[6.604142189025879]], [[6.604145526885986]], [[6.604144096374512]], [[6.604144096374512]], [[6.60414457321167]], [[6.6041460037231445]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604145526885986]], [[6.604145526885986]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145526885986]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145050048828]], [[6.6041460037231445]], [[6.604145526885986]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604144096374512]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145526885986]], [[6.604146957397461]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145050048828]], [[6.604144096374512]], [[6.604147434234619]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145050048828]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.6041460037231445]], [[6.604145526885986]], [[6.6041436195373535]], [[6.604145050048828]], [[6.604145050048828]], [[6.60414457321167]], [[6.604146480560303]]], [[[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604144096374512]], [[6.604145050048828]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604143142700195]], [[6.604142189025879]], [[6.604145526885986]], [[6.604144096374512]], [[6.604144096374512]], [[6.60414457321167]], [[6.6041460037231445]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604145526885986]], [[6.604145526885986]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145526885986]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145050048828]], [[6.6041460037231445]], [[6.604145526885986]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604144096374512]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145526885986]], [[6.604146957397461]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145050048828]], [[6.604144096374512]], [[6.604147434234619]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145050048828]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.6041460037231445]], [[6.604145526885986]], [[6.6041436195373535]], [[6.604145050048828]], [[6.604145050048828]], [[6.60414457321167]], [[6.604146480560303]]], [[[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604144096374512]], [[6.604145050048828]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604143142700195]], [[6.604142189025879]], [[6.604145526885986]], [[6.604144096374512]], [[6.604144096374512]], [[6.60414457321167]], [[6.6041460037231445]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604145526885986]], [[6.604145526885986]], [[6.6041460037231445]]], [[[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.604144096374512]], [[6.6041460037231445]]], [[[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604144096374512]], [[6.604145050048828]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604143142700195]], [[6.604142189025879]], [[6.604145526885986]], [[6.604144096374512]], [[6.604144096374512]], [[6.60414457321167]], [[6.6041460037231445]]], [[[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604144096374512]], [[6.604145050048828]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604143142700195]], [[6.604142189025879]], [[6.604145526885986]], [[6.604144096374512]], [[6.604144096374512]], [[6.60414457321167]], [[6.6041460037231445]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604145526885986]], [[6.604145526885986]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145526885986]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145050048828]], [[6.6041460037231445]]], [[[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604144096374512]], [[6.604145050048828]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604143142700195]], [[6.604142189025879]], [[6.604145526885986]], [[6.604144096374512]], [[6.604144096374512]], [[6.60414457321167]], [[6.6041460037231445]]], [[[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604144096374512]], [[6.604145050048828]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604143142700195]], [[6.604142189025879]], [[6.604145526885986]], [[6.604144096374512]], [[6.604144096374512]], [[6.60414457321167]], [[6.6041460037231445]], [[6.604144096374512]], [[6.6041460037231445]]], [[[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604144096374512]], [[6.604145050048828]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604143142700195]], [[6.604142189025879]], [[6.604145526885986]], [[6.604144096374512]], [[6.604144096374512]], [[6.60414457321167]], [[6.6041460037231445]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604145526885986]], [[6.604145526885986]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145526885986]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145050048828]], [[6.6041460037231445]], [[6.604145526885986]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604144096374512]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.60414457321167]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145526885986]], [[6.604146957397461]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145050048828]], [[6.604144096374512]], [[6.604147434234619]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145050048828]], [[6.60414457321167]], [[6.604145050048828]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.6041460037231445]]], [[[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604144096374512]], [[6.604145050048828]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604143142700195]], [[6.604142189025879]], [[6.604145526885986]], [[6.604144096374512]], [[6.604144096374512]], [[6.60414457321167]], [[6.6041460037231445]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.604145526885986]], [[6.604145526885986]], [[6.6041460037231445]]], [[[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.604144096374512]], [[6.6041460037231445]]], [[[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604144096374512]], [[6.604145050048828]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604143142700195]], [[6.604142189025879]], [[6.604145526885986]], [[6.604144096374512]], [[6.604144096374512]], [[6.60414457321167]], [[6.6041460037231445]], [[6.604144096374512]], [[6.6041460037231445]]], [[[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604144096374512]], [[6.604145050048828]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604143142700195]], [[6.604142189025879]], [[6.604145526885986]], [[6.604144096374512]], [[6.604144096374512]], [[6.60414457321167]], [[6.6041460037231445]]], [[[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.604144096374512]], [[6.6041460037231445]]], [[[6.604145526885986]], [[6.604144096374512]], [[6.604145050048828]], [[6.604145526885986]], [[6.604145526885986]], [[6.604144096374512]], [[6.6041460037231445]], [[6.604144096374512]], [[6.604145050048828]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.604145050048828]], [[6.604144096374512]], [[6.604145050048828]], [[6.6041436195373535]], [[6.60414457321167]], [[6.60414457321167]], [[6.604144096374512]], [[6.6041460037231445]], [[6.60414457321167]], [[6.604144096374512]], [[6.604143142700195]], [[6.604142189025879]], [[6.604145526885986]], [[6.604144096374512]], [[6.604144096374512]], [[6.60414457321167]], [[6.6041460037231445]], [[6.604144096374512]], [[6.6041460037231445]]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f373a40dbb0>"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "component_idx = 1\n",
    "feat_idx = 19812\n",
    "\n",
    "submodule = submodules[component_idx]\n",
    "dictionary = dictionaries[component_idx]\n",
    "\n",
    "# interpret some features\n",
    "data = zst_to_generator('/share/data/datasets/pile/the-eye.eu/public/AI/pile/train/00.jsonl.zst')\n",
    "buffer = ActivationBuffer(\n",
    "    data,\n",
    "    model,\n",
    "    submodule,\n",
    "    out_feats=512,\n",
    "    in_batch_size=128,\n",
    "    n_ctxs=512,\n",
    "    is_hf=False\n",
    ")\n",
    "\n",
    "out = examine_dimension(\n",
    "    model,\n",
    "    submodule,\n",
    "    buffer,\n",
    "    dictionary,\n",
    "    dim_idx=feat_idx,\n",
    ")\n",
    "print(out['top_tokens'])\n",
    "out['top_contexts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: 0.7661039233207703, after: 0.38638415932655334\n"
     ]
    }
   ],
   "source": [
    "input = \"YARNS\"\n",
    "\n",
    "to_ablate = {\n",
    "    submodules[0] : [\n",
    "        5493,\n",
    "        5650,\n",
    "        13114,\n",
    "        # 22182, # feature for tokens containing 'S'\n",
    "        25864,\n",
    "        # 26261, # another 'S' feature\n",
    "        # 26532, # feature for things starting with 'S'\n",
    "    ],\n",
    "    submodules[1] : [\n",
    "        2779,\n",
    "        # 4900, # plural words that start with a capital letter\n",
    "        5469,\n",
    "        5509,\n",
    "        7368,\n",
    "        # 11305, # sometimes fires on other stuff\n",
    "        # 14465, # all caps bigrams ending in 'S'\n",
    "        18471,\n",
    "        # 25963, # for the 'S' token\n",
    "        26424,\n",
    "        32267,  \n",
    "    ],\n",
    "    submodules[2] : [\n",
    "        # 9068, # for the 'S' token\n",
    "        22968,\n",
    "        27888,\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "with model.invoke(input):\n",
    "    acts = model.gpt_neox.layers[layer].output[0][:,-1,:].save()\n",
    "acts = acts.value.clone()\n",
    "pred_before = probe(acts).item()\n",
    "\n",
    "with model.invoke(input):\n",
    "    for submodule, dictionary in zip(submodules, dictionaries):\n",
    "        x = submodule.output\n",
    "        is_resid = type(x.shape) == tuple\n",
    "        if is_resid:\n",
    "            x = x[0]\n",
    "        x_hat = dictionary(x)\n",
    "        residual = x - x_hat\n",
    "        \n",
    "        f = dictionary.encode(x)\n",
    "        ablation_idxs = to_ablate[submodule]\n",
    "        for idx in ablation_idxs:\n",
    "            f[..., idx] = 0\n",
    "        x_hat = dictionary.decode(f)\n",
    "        if is_resid:\n",
    "            submodule.output[0][:] = x_hat + residual\n",
    "        else:\n",
    "            submodule.output = x_hat + residual\n",
    "    acts = model.gpt_neox.layers[layer].output[0][:,-1,:].save()\n",
    "acts = acts.value.clone()\n",
    "pred_after = probe(acts).item()\n",
    "\n",
    "print(f'before: {pred_before}, after: {pred_after}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on ambiguous data: 0.9781022071838379\n",
      "feat1 accuracy: 0.6569343209266663\n",
      "feat2 accuracy: 0.8613138794898987\n"
     ]
    }
   ],
   "source": [
    "def get_acts(model):\n",
    "    return model.gpt_neox.layers[layer].output[0][:,-1,:]\n",
    "\n",
    "# get accuracy on ambiguous test set\n",
    "inputs, labels = data_gen(ambiguous=True, split='test')\n",
    "acts = run_with_ablations(\n",
    "    model,\n",
    "    inputs,\n",
    "    submodules,\n",
    "    dictionaries,\n",
    "    to_ablate,\n",
    "    get_acts,\n",
    ")\n",
    "preds = probe(acts.clone()).round()\n",
    "acc = (preds == labels).float().mean().item()\n",
    "print(f'Accuracy on ambiguous data: {acc}')\n",
    "\n",
    "# get accuracy on unambiguous test set\n",
    "inputs, feat1_labels, feat2_labels = data_gen(ambiguous=False, split='test')\n",
    "acts = run_with_ablations(\n",
    "    model,\n",
    "    inputs,\n",
    "    submodules,\n",
    "    dictionaries,\n",
    "    to_ablate,\n",
    "    get_acts,\n",
    ")\n",
    "preds = probe(acts.clone()).round()\n",
    "acc = (preds == feat1_labels).float().mean().item()\n",
    "print(f'feat1 accuracy: {acc}')\n",
    "preds = probe(acts.clone()).round()\n",
    "acc = (preds == feat2_labels).float().mean().item()\n",
    "print(f'feat2 accuracy: {acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat1 accuracy: 0.956204354763031\n",
      "feat2 accuracy: 0.970802903175354\n"
     ]
    }
   ],
   "source": [
    "# train probes on unambiguous data\n",
    "inputs, feat1_labels, feat2_labels = data_gen(ambiguous=False, split='train')\n",
    "\n",
    "acts = run_with_ablations(\n",
    "    model,\n",
    "    inputs,\n",
    "    submodules,\n",
    "    dictionaries,\n",
    "    to_ablate,\n",
    "    get_acts,\n",
    ")\n",
    "acts = acts.clone()\n",
    "\n",
    "t.manual_seed(SEED)\n",
    "probe1 = Probe(acts.shape[-1]).to(device)\n",
    "probe2 = Probe(acts.shape[-1]).to(device)\n",
    "opt1 = t.optim.AdamW(probe1.parameters(), lr=lr)\n",
    "opt2 = t.optim.AdamW(probe2.parameters(), lr=lr)\n",
    "\n",
    "for _ in range(epochs):\n",
    "    opt1.zero_grad(), opt2.zero_grad()\n",
    "    logits1 = probe1(acts)\n",
    "    logits2 = probe2(acts)\n",
    "    loss1 = nn.BCELoss()(logits1, feat1_labels.float())\n",
    "    loss2 = nn.BCELoss()(logits2, feat2_labels.float())\n",
    "    loss1.backward(), loss2.backward()\n",
    "    opt1.step(), opt2.step()\n",
    "\n",
    "# get accuracy on unambiguous test set\n",
    "inputs, feat1_labels, feat2_labels = data_gen(ambiguous=False, split='test')\n",
    "acts = run_with_ablations(\n",
    "    model,\n",
    "    inputs,\n",
    "    submodules,\n",
    "    dictionaries,\n",
    "    to_ablate,\n",
    "    get_acts,\n",
    ")\n",
    "acts = acts.clone()\n",
    "\n",
    "probs1 = probe1(acts)\n",
    "probs2 = probe2(acts)\n",
    "preds1, preds2 = probs1.round(), probs2.round()\n",
    "acc1 = (preds1 == feat1_labels).float().mean().item()\n",
    "acc2 = (preds2 == feat2_labels).float().mean().item()\n",
    "\n",
    "print(f'feat1 accuracy: {acc1}')\n",
    "print(f'feat2 accuracy: {acc2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on ambiguous data: 0.985401451587677\n",
      "feat1 accuracy: 0.6788321137428284\n",
      "feat2 accuracy: 0.8540145754814148\n"
     ]
    }
   ],
   "source": [
    "# retrain probe on ablated model\n",
    "t.manual_seed(SEED)\n",
    "probe_new = Probe(512).to(device)\n",
    "optimizer = t.optim.AdamW(probe_new.parameters(), lr=lr)\n",
    "\n",
    "# train probe on ambiguous data\n",
    "inputs, labels = data_gen(ambiguous=True, split='train')\n",
    "acts = run_with_ablations(\n",
    "    model,\n",
    "    inputs,\n",
    "    submodules,\n",
    "    dictionaries,\n",
    "    to_ablate,\n",
    "    get_acts,\n",
    ")\n",
    "acts = acts.clone()\n",
    "\n",
    "for _ in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    probs = probe_new(acts)\n",
    "    loss = nn.BCELoss()(probs, labels.float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# get accuracy on ambiguous test set\n",
    "inputs, labels = data_gen(ambiguous=True, split='test')\n",
    "acts = run_with_ablations(\n",
    "    model,\n",
    "    inputs,\n",
    "    submodules,\n",
    "    dictionaries,\n",
    "    to_ablate,\n",
    "    get_acts,\n",
    ")\n",
    "acts = acts.clone()\n",
    "preds = probe_new(acts).round()\n",
    "acc = (preds == labels).float().mean().item()\n",
    "print(f'Accuracy on ambiguous data: {acc}')\n",
    "\n",
    "# get accuracy on unambiguous test set\n",
    "inputs, feat1_labels, feat2_labels = data_gen(ambiguous=False, split='test')\n",
    "acts = run_with_ablations(\n",
    "    model,\n",
    "    inputs,\n",
    "    submodules,\n",
    "    dictionaries,\n",
    "    to_ablate,\n",
    "    get_acts,\n",
    ")\n",
    "acts = acts.clone()\n",
    "preds = probe_new(acts).round()\n",
    "acc = (preds == feat1_labels).float().mean().item()\n",
    "print(f'feat1 accuracy: {acc}')\n",
    "preds = probe_new(acts).round()\n",
    "acc = (preds == feat2_labels).float().mean().item()\n",
    "print(f'feat2 accuracy: {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat1 accuracy: 0.7591241002082825\n",
      "feat2 accuracy: 0.5985401272773743\n"
     ]
    }
   ],
   "source": [
    "# classify things based on the difference between the probe and probe_new predictions\n",
    "inputs, feat1_labels, feat2_labels = data_gen(ambiguous=False, split='test')\n",
    "\n",
    "with model.invoke(inputs):\n",
    "    acts = model.gpt_neox.layers[layer].output[0][:,-1,:].save()\n",
    "acts = acts.value.clone()\n",
    "\n",
    "acts_new = run_with_ablations(\n",
    "    model,\n",
    "    inputs,\n",
    "    submodules,\n",
    "    dictionaries,\n",
    "    to_ablate,\n",
    "    get_acts,\n",
    ")\n",
    "acts_new = acts_new.clone()\n",
    "\n",
    "probs1 = probe(acts)\n",
    "probs2 = probe_new(acts_new)\n",
    "diffs = probs2 - probs1\n",
    "preds = t.where(\n",
    "    diffs.abs() > .05,\n",
    "    (diffs > 0).float(),\n",
    "    probs2.round()\n",
    ")\n",
    "\n",
    "acc = (preds == feat1_labels).float().mean().item()\n",
    "print(f'feat1 accuracy: {acc}')\n",
    "acc = (preds == feat2_labels).float().mean().item()\n",
    "print(f'feat2 accuracy: {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beef 0.0 0.04246163368225098 0.0424615778028965\n",
      "tomatoes 0.0 0.01407528854906559 0.012785992585122585\n",
      "headphone 0.0 0.0004125593404751271 0.00041255910764448345\n",
      "ELECTRON 0.0 0.9458839893341064 0.4734917879104614\n",
      "seatbelt 0.0 0.002296003745868802 0.002296007238328457\n",
      "GOBLINS 0.0 0.9927135109901428 0.919037938117981\n",
      "TRAVELER 0.0 0.9688680768013 0.4352923631668091\n",
      "DANCE 0.0 0.35750260949134827 0.12069746106863022\n",
      "parrot 0.0 0.0020670697558671236 0.002028270158916712\n",
      "minerals 0.0 0.06108663231134415 0.06108665466308594\n",
      "refugee 0.0 0.09666279703378677 0.09666275233030319\n",
      "cabinet 0.0 0.015743708238005638 0.014384628273546696\n",
      "RUG 0.0 0.6748154759407043 0.4169016480445862\n",
      "doctor 0.0 0.00024147509247995913 0.00024147509247995913\n",
      "cookies 0.0 0.037880077958106995 0.03496574983000755\n",
      "bicycle 0.0 0.019201841205358505 0.018249278888106346\n",
      "toothbrushes 0.0 0.21269121766090393 0.2057013362646103\n",
      "zoos 0.0 0.05740008503198624 0.056083958595991135\n",
      "ROADS 0.0 0.539910614490509 0.3340230882167816\n",
      "butter 0.0 0.008368565700948238 0.007784586865454912\n",
      "POEMS 0.0 0.9880875945091248 0.9033903479576111\n",
      "stream 0.0 0.00015327142318710685 0.00015327142318710685\n",
      "RHINOCEROSES 1.0 0.9991440773010254 0.9888324737548828\n",
      "soldiers 0.0 0.059281572699546814 0.05928147956728935\n",
      "festivals 0.0 0.1231389120221138 0.1231388971209526\n",
      "PIANOS 0.0 0.9580724239349365 0.5291330218315125\n",
      "NAILS 0.0 0.9942517280578613 0.8060845136642456\n",
      "CHAPTERS 1.0 0.9974505305290222 0.9959012866020203\n",
      "trees 0.0 0.000711871194653213 0.0006240722723305225\n",
      "TOASTERS 0.0 0.9562177062034607 0.8166221380233765\n",
      "hawk 0.0 0.027996132150292397 0.027996156364679337\n",
      "plate 0.0 0.00015180400805547833 0.00013921370555181056\n",
      "THERMOMETERS 1.0 0.9987780451774597 0.9680272936820984\n",
      "DAISIES 0.0 0.9886110424995422 0.9326550364494324\n",
      "ducks 1.0 0.9161551594734192 0.8914290070533752\n",
      "zebu 0.0 0.05128225311636925 0.05128219351172447\n",
      "PORKS 1.0 0.9178125262260437 0.8679240942001343\n",
      "HYDRANGEA 0.0 0.9835972189903259 0.921669602394104\n",
      "RACKET 0.0 0.693408191204071 0.14938229322433472\n",
      "scarf 0.0 0.16297408938407898 0.16297423839569092\n",
      "magazine 0.0 0.01252501830458641 0.01120640616863966\n",
      "xylophone 0.0 0.05087375268340111 0.05048496276140213\n",
      "outlets 0.0 0.0026805317029356956 0.0026805305387824774\n",
      "mixers 0.0 0.2907540798187256 0.2907544672489166\n",
      "facts 0.0 0.0004152132314629853 0.00041521285311318934\n",
      "COMPUTERS 1.0 0.9983575940132141 0.9914042949676514\n",
      "houses 0.0 0.0006488774088211358 0.0006488774088211358\n",
      "cover 0.0 0.0011467809090390801 0.0010855498258024454\n",
      "treats 1.0 0.601621150970459 0.6016213297843933\n",
      "VIOLINS 1.0 0.967982828617096 0.9306872487068176\n",
      "achievement 0.0 0.004612194374203682 0.0043045515194535255\n",
      "chords 0.0 0.07726689428091049 0.07726691663265228\n",
      "SQUARE 0.0 0.9374995231628418 0.5820940136909485\n",
      "pens 0.0 0.1501818597316742 0.15018169581890106\n",
      "immigrant 0.0 0.10891028493642807 0.09299492090940475\n",
      "princes 0.0 0.01728411577641964 0.014804813079535961\n",
      "forests 0.0 0.763244092464447 0.6375521421432495\n",
      "jamboree 0.0 0.10090913623571396 0.0985572561621666\n",
      "HOSE 0.0 0.9657225608825684 0.8096193075180054\n",
      "TALENT 0.0 0.903074324131012 0.5509194731712341\n",
      "LIZARD 0.0 0.6439082026481628 0.1560492366552353\n",
      "universe 0.0 0.0871344804763794 0.08095411211252213\n",
      "meal 0.0 0.005332706030458212 0.005332703236490488\n",
      "relaxation 0.0 0.3822648525238037 0.38226404786109924\n",
      "LAKE 1.0 0.9980334639549255 0.9554151296615601\n",
      "breads 0.0 0.156701922416687 0.15670160949230194\n",
      "leaders 0.0 0.1653897613286972 0.1277439445257187\n",
      "COASTER 0.0 0.8607373237609863 0.518534243106842\n",
      "walnuts 0.0 0.005125007126480341 0.004635727498680353\n",
      "REMOTES 0.0 0.9925969839096069 0.9012219309806824\n",
      "QUETZALS 0.0 0.9905427098274231 0.9377087354660034\n",
      "ROCKS 0.0 0.983881950378418 0.8827332854270935\n",
      "cake 0.0 0.0006054620025679469 0.0006054620025679469\n",
      "sugars 0.0 0.05869286507368088 0.05551847442984581\n",
      "rabbit 0.0 0.0012889666249975562 0.0012889666249975562\n",
      "meters 0.0 0.012139041908085346 0.012074397876858711\n",
      "elks 0.0 0.18253420293331146 0.17471711337566376\n",
      "tables 0.0 0.0008255351567640901 0.0006121330079622567\n",
      "panda 0.0 0.0051186541095376015 0.005118651315569878\n",
      "PIZZAS 0.0 0.930083155632019 0.515113353729248\n",
      "oven 0.0 0.002303321845829487 0.002303321845829487\n",
      "ALMONDS 0.0 0.988341748714447 0.8037616014480591\n",
      "ring 0.0 5.2716735808644444e-05 5.063182470621541e-05\n",
      "tunes 0.0 0.05021281912922859 0.050212860107421875\n",
      "tourist 0.0 0.011485638096928596 0.011083679273724556\n",
      "DISCUSSION 0.0 0.6336621642112732 0.1809719353914261\n",
      "STUDENTS 0.0 0.9262995719909668 0.6078335642814636\n",
      "REFEREE 0.0 0.990777850151062 0.6396501660346985\n",
      "SNACKS 0.0 0.9623735547065735 0.7215399742126465\n",
      "knives 0.0 0.08726716786623001 0.07713889330625534\n",
      "DAFFODILS 1.0 0.9918740391731262 0.9631452560424805\n",
      "waterfalls 0.0 0.023064447566866875 0.020251775160431862\n",
      "spheres 0.0 0.009894157759845257 0.009894154034554958\n",
      "ISSUES 1.0 0.9995026588439941 0.9854965806007385\n",
      "BLACKBERRIES 0.0 0.8971524834632874 0.7472043633460999\n",
      "DINOSAURS 0.0 0.9911332726478577 0.9352904558181763\n",
      "GRASSES 0.0 0.998316764831543 0.942377507686615\n",
      "YARNS 0.0 0.7661040425300598 0.3863840401172638\n",
      "UTENSILS 1.0 0.9926578402519226 0.9539926648139954\n",
      "FABLE 0.0 0.5461746454238892 0.21470938622951508\n",
      "STRATEGY 0.0 0.898637056350708 0.48255684971809387\n",
      "helmets 0.0 0.01614484190940857 0.011609584093093872\n",
      "TASKS 0.0 0.9732210636138916 0.8476290702819824\n",
      "STRAWBERRIES 0.0 0.9944102168083191 0.9293003082275391\n",
      "walruses 0.0 0.03083471767604351 0.029121533036231995\n",
      "FLAMINGOS 0.0 0.9813030362129211 0.5710319876670837\n",
      "junipers 0.0 0.3052775263786316 0.223809614777565\n",
      "jackals 0.0 0.1428365409374237 0.14283670485019684\n",
      "PANS 0.0 0.974245011806488 0.9196264147758484\n",
      "picture 0.0 0.00023718008014839143 0.00023717983276583254\n",
      "bears 0.0 0.016821783035993576 0.016821790486574173\n",
      "mirror 0.0 0.005386590491980314 0.0030432757921516895\n",
      "zephyrs 0.0 0.30951857566833496 0.20768757164478302\n",
      "ISLANDS 1.0 0.9965392351150513 0.9818708896636963\n",
      "donkey 0.0 0.0013477120082825422 0.0013477145694196224\n",
      "BULBS 0.0 0.9703730344772339 0.6339048147201538\n",
      "desk 0.0 0.06836309283971786 0.049052588641643524\n",
      "nickels 0.0 0.02281871624290943 0.021354088559746742\n",
      "apples 0.0 0.05062790960073471 0.05062799155712128\n",
      "bird 0.0 0.0008373097516596317 0.0008373097516596317\n",
      "TRUCKS 0.0 0.9919413328170776 0.8903639912605286\n",
      "POOL 0.0 0.605867862701416 0.27046555280685425\n",
      "dolphins 0.0 0.09179390221834183 0.09179390221834183\n",
      "yeti 0.0 0.007653664331883192 0.007653671316802502\n",
      "TURTLE 0.0 0.75026535987854 0.32222872972488403\n",
      "lemons 0.0 0.050334129482507706 0.04659726470708847\n",
      "burgers 0.0 0.32927027344703674 0.2865195870399475\n",
      "miner 0.0 0.14681680500507355 0.14681683480739594\n",
      "VOICES 1.0 0.9923836588859558 0.9772496223449707\n",
      "SEASONS 1.0 0.9965851306915283 0.9539580345153809\n",
      "PROTONS 1.0 0.9329134821891785 0.8856754899024963\n",
      "pencil 0.0 0.004734545946121216 0.004734537098556757\n",
      "fruit 0.0 0.0025156717747449875 0.002515674103051424\n",
      "NEWS 1.0 0.9186640381813049 0.8986961841583252\n",
      "BATTERY 0.0 0.8748708963394165 0.36765098571777344\n",
      "JAR 0.0 0.8483402132987976 0.41982588171958923\n",
      "LINE 0.0 0.2823949158191681 0.12917552888393402\n"
     ]
    }
   ],
   "source": [
    "for x, prob1, prob2, pred in zip(inputs, probs1, probs2, preds):\n",
    "    print(x, pred.item(), prob1.item(), prob2.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.6879e-02,  2.0250e-02, -1.3686e-04,  2.3178e-01,  1.6529e-03,\n",
       "         6.6891e-02,  2.7669e-01,  1.0507e-01, -2.0387e-04,  9.0288e-02,\n",
       "         1.3871e-01,  2.7971e-02,  2.8520e-01, -1.7768e-04,  2.1047e-01,\n",
       "         5.7020e-02,  4.4652e-01, -2.1454e-02,  5.7701e-01,  2.8447e-02,\n",
       "         8.5847e-02,  2.5172e-05,  1.0240e-02,  3.1668e-01,  1.2039e-01,\n",
       "         3.1745e-01,  1.5877e-01,  2.4371e-03, -5.2675e-04,  1.5538e-01,\n",
       "         1.2155e-01, -9.7485e-05,  2.5782e-02,  5.9511e-02,  7.9205e-02,\n",
       "         2.8743e-02,  9.1319e-02,  5.9899e-02,  4.1422e-01,  2.7963e-01,\n",
       "         3.3364e-02,  7.9152e-02,  7.3441e-03,  4.3555e-01, -3.0199e-04,\n",
       "         6.6104e-03, -3.7750e-04, -4.0154e-04,  3.5560e-01,  5.5813e-02,\n",
       "         2.9123e-04,  4.7253e-01,  2.7764e-01,  3.6436e-01, -3.0451e-02,\n",
       "         7.5291e-02,  2.3076e-01,  1.6617e-01,  1.3763e-01,  3.0491e-01,\n",
       "         3.5337e-01,  1.4857e-01, -3.3222e-04,  3.4590e-01,  2.2563e-02,\n",
       "         4.7432e-01,  4.9966e-01,  3.7437e-01,  1.4534e-02,  8.9933e-02,\n",
       "         4.6826e-02,  1.0760e-01, -2.1836e-04,  4.0180e-01, -4.5381e-04,\n",
       "         7.4128e-02,  2.3738e-01, -1.6298e-04,  2.8073e-02,  3.9166e-01,\n",
       "        -1.8924e-03,  1.6548e-01, -2.8274e-05,  2.4951e-01,  3.3708e-02,\n",
       "        -1.6052e-01,  3.1788e-01,  2.6570e-01,  2.4708e-01,  2.0883e-01,\n",
       "         3.2472e-02,  5.9323e-02,  5.8550e-03,  1.1058e-02,  2.1870e-01,\n",
       "         5.9374e-02,  5.4030e-02,  4.4912e-01,  4.0839e-02,  2.2150e-01,\n",
       "         2.8928e-01,  6.1337e-03,  1.3383e-01,  6.0789e-02,  4.7117e-01,\n",
       "         1.8734e-01,  2.9516e-01,  4.3300e-01,  5.6949e-02, -7.9217e-05,\n",
       "         1.5351e-01, -1.7118e-03,  3.2255e-01,  1.3989e-02, -3.3647e-04,\n",
       "         3.1665e-01,  1.7249e-01, -3.8858e-03,  1.8510e-01, -5.6725e-04,\n",
       "         9.6146e-02,  4.8084e-01,  4.4225e-01,  5.2506e-02,  3.0166e-01,\n",
       "         5.9592e-03,  2.6451e-01,  7.8514e-02,  1.8359e-02,  4.1893e-02,\n",
       "         8.6697e-02,  1.6510e-02, -1.5133e-03,  8.2302e-02,  4.3029e-01,\n",
       "         1.9416e-01, -1.0982e-01], device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs2 - probs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ling acc: 0.9539930555555556\n",
      "surface acc: 0.6663995726495726\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_json('/share/data/datasets/msgs/syntactic_category_lexical_content_the/test.jsonl', lines=True)\n",
    "\n",
    "ling_accs, surface_accs = [], []\n",
    "# get accuracy on test data\n",
    "for batch_idx in range(len(test_data) // batch_size):\n",
    "    inputs = test_data['sentence'][batch_idx * batch_size:(batch_idx + 1) * batch_size].tolist()\n",
    "    ling_labels = test_data['linguistic_feature_label'][batch_idx * batch_size:(batch_idx + 1) * batch_size].tolist()\n",
    "    surface_labels = test_data['surface_feature_label'][batch_idx * batch_size:(batch_idx + 1) * batch_size].tolist()\n",
    "\n",
    "    with model.invoke(inputs) as invoker:\n",
    "        hidden_states = model.gpt_neox.layers[-3].output[0].save()\n",
    "    \n",
    "    with t.no_grad():\n",
    "        preds = probe(hidden_states.value)\n",
    "        ling_acc = (preds.round() == t.Tensor(ling_labels).to('cuda:0')).float().mean()\n",
    "        surface_acc = (preds.round() == t.Tensor(surface_labels).to('cuda:0')).float().mean()\n",
    "        ling_accs.append(ling_acc.item())\n",
    "        surface_accs.append(surface_acc.item())\n",
    "\n",
    "print('ling acc:', sum(ling_accs) / len(ling_accs))\n",
    "print('surface acc:', sum(surface_accs) / len(surface_accs))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attribution import patching_effect\n",
    "from dictionary_learning.dictionary import AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4021, 0.0954], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean = \"All grandsons do resemble the print and Debra is an organized child.\"\n",
    "patch = \"All grandsons do resemble a print and Debra is an banana child.\"\n",
    "\n",
    "with model.invoke([clean, patch]) as invoker:\n",
    "    hidden_states = model.gpt_neox.layers[-3].output[0].save()\n",
    "\n",
    "with t.no_grad():\n",
    "    preds = probe(hidden_states.value)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_fn(model):\n",
    "    return probe(model.gpt_neox.layers[-3].output[0])\n",
    "\n",
    "submodules = [\n",
    "    model.gpt_neox.layers[i] for i in range(4)\n",
    "] + [\n",
    "    model.gpt_neox.layers[i].mlp for i in range(4)\n",
    "]\n",
    "dictionaries = []\n",
    "for i in range(4):\n",
    "    dictionary = AutoEncoder(512, 64 * 512).to(device)\n",
    "    dictionary.load_state_dict(t.load(f'/share/projects/dictionary_circuits/autoencoders/pythia-70m-deduped/resid_out_layer{i}/0_32768/ae.pt'))\n",
    "    dictionaries.append(dictionary)\n",
    "for i in range(4):\n",
    "    dictionary = AutoEncoder(512, 64 * 512).to(device)\n",
    "    dictionary.load_state_dict(t.load(f'/share/projects/dictionary_circuits/autoencoders/pythia-70m-deduped/mlp_out_layer{i}/1_32768/ae.pt'))\n",
    "    dictionaries.append(dictionary)\n",
    "\n",
    "out = patching_effect(\n",
    "    clean,\n",
    "    patch,\n",
    "    model,\n",
    "    submodules,\n",
    "    dictionaries,\n",
    "    metric_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total effect: tensor([-0.7627], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Layer 0:\n",
      "    Multindex: (0, 6, 23084), Value: -0.31019383668899536\n",
      "    Multindex: (0, 6, 29115), Value: -0.2026602178812027\n",
      "    Multindex: (0, 12, 9247), Value: 0.23665377497673035\n",
      "    Multindex: (0, 12, 19133), Value: -0.13597454130649567\n",
      "    Multindex: (0, 13, 1147), Value: 0.10391081869602203\n",
      "    Multindex: (0, 13, 1256), Value: -0.12887312471866608\n",
      "    Multindex: (0, 13, 3385), Value: -0.34761813282966614\n",
      "    Multindex: (0, 13, 3613), Value: -0.11544839292764664\n",
      "    Multindex: (0, 13, 5702), Value: -11.003226280212402\n",
      "    Multindex: (0, 13, 5962), Value: -0.18838448822498322\n",
      "    Multindex: (0, 13, 6959), Value: -0.9820340871810913\n",
      "    Multindex: (0, 13, 15146), Value: -0.12887312471866608\n",
      "    Multindex: (0, 13, 25951), Value: -0.24457168579101562\n",
      "    Multindex: (0, 13, 26640), Value: -0.25634047389030457\n",
      "    Multindex: (0, 13, 27692), Value: -0.12028089165687561\n",
      "    Multindex: (0, 13, 31525), Value: -0.12461315840482712\n",
      "    Multindex: (0, 13, 32650), Value: -0.16245757043361664\n",
      "Layer 1:\n",
      "    Multindex: (0, 6, 588), Value: -0.16466379165649414\n",
      "    Multindex: (0, 13, 245), Value: -0.1918429285287857\n",
      "    Multindex: (0, 13, 303), Value: -0.9617262482643127\n",
      "    Multindex: (0, 13, 614), Value: -0.4203931391239166\n",
      "    Multindex: (0, 13, 639), Value: -0.1067311018705368\n",
      "    Multindex: (0, 13, 1322), Value: -0.11249381303787231\n",
      "    Multindex: (0, 13, 2223), Value: -0.1590975821018219\n",
      "    Multindex: (0, 13, 4875), Value: -0.22198885679244995\n",
      "    Multindex: (0, 13, 7079), Value: -0.1313270777463913\n",
      "    Multindex: (0, 13, 7541), Value: 0.21306736767292023\n",
      "    Multindex: (0, 13, 8150), Value: -0.1918429285287857\n",
      "    Multindex: (0, 13, 9908), Value: -0.13814546167850494\n",
      "    Multindex: (0, 13, 16024), Value: -0.17133969068527222\n",
      "    Multindex: (0, 13, 20331), Value: -1.2441682815551758\n",
      "    Multindex: (0, 13, 23013), Value: -0.18849270045757294\n",
      "    Multindex: (0, 13, 23657), Value: -0.12274052947759628\n",
      "    Multindex: (0, 13, 25848), Value: -0.3059385120868683\n",
      "    Multindex: (0, 13, 29820), Value: -0.1918429285287857\n",
      "    Multindex: (0, 13, 30623), Value: 0.18556568026542664\n",
      "    Multindex: (0, 13, 31453), Value: -0.3648081123828888\n",
      "Layer 2:\n",
      "    Multindex: (0, 13, 5121), Value: -0.16367250680923462\n",
      "    Multindex: (0, 13, 7140), Value: -0.178150936961174\n",
      "    Multindex: (0, 13, 8271), Value: -0.16367250680923462\n",
      "    Multindex: (0, 13, 17322), Value: -0.6946452260017395\n",
      "    Multindex: (0, 13, 19819), Value: -0.10428597778081894\n",
      "    Multindex: (0, 13, 20536), Value: -0.16367250680923462\n",
      "    Multindex: (0, 13, 23880), Value: 0.10106822103261948\n",
      "    Multindex: (0, 13, 24445), Value: -0.15478013455867767\n",
      "    Multindex: (0, 13, 25117), Value: -0.3223865330219269\n",
      "    Multindex: (0, 13, 26060), Value: 0.19325946271419525\n",
      "    Multindex: (0, 13, 28532), Value: -0.5902589559555054\n",
      "Layer 3:\n",
      "    Multindex: (0, 13, 1196), Value: -0.14947494864463806\n",
      "    Multindex: (0, 13, 6716), Value: 0.15161971747875214\n",
      "    Multindex: (0, 13, 11808), Value: -0.11084704846143723\n",
      "    Multindex: (0, 13, 18212), Value: -0.1517772376537323\n",
      "    Multindex: (0, 13, 27249), Value: -0.10479556024074554\n",
      "    Multindex: (0, 13, 27731), Value: 0.16034795343875885\n",
      "    Multindex: (0, 13, 28315), Value: -0.24731893837451935\n",
      "    Multindex: (0, 13, 28765), Value: -0.15697763860225677\n",
      "    Multindex: (0, 13, 31665), Value: -0.19916509091854095\n",
      "Layer 4:\n",
      "    Multindex: (0, 6, 11587), Value: -0.5147899985313416\n",
      "    Multindex: (0, 6, 25244), Value: -2.2249109745025635\n",
      "    Multindex: (0, 13, 11113), Value: -0.12203294783830643\n",
      "    Multindex: (0, 13, 22504), Value: -0.1502879410982132\n",
      "    Multindex: (0, 13, 23022), Value: -0.873496949672699\n",
      "    Multindex: (0, 13, 28362), Value: -0.30438944697380066\n",
      "    Multindex: (0, 13, 29638), Value: -0.2694805860519409\n",
      "    Multindex: (0, 13, 30628), Value: -13.042832374572754\n",
      "    Multindex: (0, 13, 32682), Value: -0.17399896681308746\n",
      "Layer 5:\n",
      "    Multindex: (0, 13, 10300), Value: -0.1266690045595169\n",
      "    Multindex: (0, 13, 17180), Value: -0.29111361503601074\n",
      "    Multindex: (0, 13, 19002), Value: -0.29111361503601074\n",
      "    Multindex: (0, 13, 20568), Value: -0.4593691825866699\n",
      "    Multindex: (0, 13, 29711), Value: -0.10204741358757019\n",
      "    Multindex: (0, 13, 32550), Value: -0.10581643134355545\n",
      "Layer 6:\n",
      "    Multindex: (0, 13, 11804), Value: 0.2141132354736328\n",
      "Layer 7:\n"
     ]
    }
   ],
   "source": [
    "effects, total_effect = out\n",
    "print(f\"Total effect: {total_effect}\")\n",
    "for layer, submodule in enumerate(submodules):\n",
    "    print(f\"Layer {layer}:\")\n",
    "    effect = effects[submodule]\n",
    "    for feature_idx in t.nonzero(effect):\n",
    "        value = effect[tuple(feature_idx)]\n",
    "        if value.abs() > 0.1:\n",
    "            print(f\"    Multindex: {tuple(feature_idx.tolist())}, Value: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: All\n",
      "1:  gr\n",
      "2: ands\n",
      "3: ons\n",
      "4:  do\n",
      "5:  resemble\n",
      "6:  the\n",
      "7:  print\n",
      "8:  and\n",
      "9:  De\n",
      "10: bra\n",
      "11:  is\n",
      "12:  an\n",
      "13:  organized\n",
      "14:  child\n",
      "15: .\n"
     ]
    }
   ],
   "source": [
    "for i, tok in enumerate(invoker.input.input_ids[0]):\n",
    "    print(f\"{i}: {model.tokenizer.decode([tok])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_ablate = {\n",
    "#     submodules[0] : [\n",
    "#         5650,\n",
    "#         17126,\n",
    "#         22182,\n",
    "#         25864,\n",
    "#         2655,\n",
    "#         12249,\n",
    "#         12267,\n",
    "#         21248,\n",
    "#         21329,\n",
    "#     ],\n",
    "#     submodules[1] : [\n",
    "#         14465,\n",
    "#         18471,\n",
    "#         22990,\n",
    "#         16629,\n",
    "#         26134,\n",
    "#         32267,\n",
    "#     ],\n",
    "#     submodules[2] : [\n",
    "#         16421,\n",
    "#         22968,\n",
    "#         27888,\n",
    "#         15899,\n",
    "#         28262,\n",
    "#         28306,\n",
    "#         32164,\n",
    "#     ]\n",
    "# }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
