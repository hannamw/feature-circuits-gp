{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/u/smarks/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Matplotlib created a temporary cache directory at /tmp/matplotlib-ws3j_0wx because the default path (/share/u/smarks/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from nnsight import LanguageModel\n",
    "import torch as t\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from attribution import patching_effect\n",
    "from dictionary_learning import AutoEncoder, ActivationBuffer\n",
    "from dictionary_learning.dictionary import IdentityDict\n",
    "from dictionary_learning.interp import examine_dimension\n",
    "from dictionary_learning.utils import zst_to_generator\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "DEBUGGING = False\n",
    "\n",
    "if DEBUGGING:\n",
    "    tracer_kwargs = dict(scan=True, validate=True)\n",
    "else:\n",
    "    tracer_kwargs = dict(scan=False, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# dataset hyperparameters\n",
    "dataset = load_dataset(\"LabHC/bias_in_bios\")\n",
    "profession_dict = {'professor' : 21, 'nurse' : 13}\n",
    "male_prof = 'professor'\n",
    "female_prof = 'nurse'\n",
    "\n",
    "# model hyperparameters\n",
    "DEVICE = 'cuda:0'\n",
    "model = LanguageModel('EleutherAI/pythia-70m-deduped', device_map=DEVICE, dispatch=True)\n",
    "activation_dim = 512\n",
    "layer = 4\n",
    "\n",
    "# dictionary hyperparameters\n",
    "dict_id = 10\n",
    "expansion_factor = 64\n",
    "dictionary_size = expansion_factor * activation_dim\n",
    "\n",
    "# data preparation hyperparameters\n",
    "batch_size = 1024\n",
    "SEED = 42\n",
    "\n",
    "def get_data(train=True, ambiguous=True, batch_size=128, seed=SEED):\n",
    "    if train:\n",
    "        data = dataset['train']\n",
    "    else:\n",
    "        data = dataset['test']\n",
    "    if ambiguous:\n",
    "        neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        n = min([len(neg), len(pos)])\n",
    "        neg, pos = neg[:n], pos[:n]\n",
    "        data = neg + pos\n",
    "        labels = [0]*n + [1]*n\n",
    "        idxs = list(range(2*n))\n",
    "        random.Random(seed).shuffle(idxs)\n",
    "        data, labels = [data[i] for i in idxs], [labels[i] for i in idxs]\n",
    "        true_labels = spurious_labels = labels\n",
    "    else:\n",
    "        neg_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        neg_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 1]\n",
    "        pos_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 0]\n",
    "        pos_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        n = min([len(neg_neg), len(neg_pos), len(pos_neg), len(pos_pos)])\n",
    "        neg_neg, neg_pos, pos_neg, pos_pos = neg_neg[:n], neg_pos[:n], pos_neg[:n], pos_pos[:n]\n",
    "        data = neg_neg + neg_pos + pos_neg + pos_pos\n",
    "        true_labels     = [0]*n + [0]*n + [1]*n + [1]*n\n",
    "        spurious_labels = [0]*n + [1]*n + [0]*n + [1]*n\n",
    "        idxs = list(range(4*n))\n",
    "        random.Random(seed).shuffle(idxs)\n",
    "        data, true_labels, spurious_labels = [data[i] for i in idxs], [true_labels[i] for i in idxs], [spurious_labels[i] for i in idxs]\n",
    "\n",
    "    batches = [\n",
    "        (data[i:i+batch_size], t.tensor(true_labels[i:i+batch_size], device=DEVICE), t.tensor(spurious_labels[i:i+batch_size], device=DEVICE)) for i in range(0, len(data), batch_size)\n",
    "    ]\n",
    "\n",
    "    return batches\n",
    "\n",
    "def get_subgroups(train=True, ambiguous=True, batch_size=128, seed=SEED):\n",
    "    if train:\n",
    "        data = dataset['train']\n",
    "    else:\n",
    "        data = dataset['test']\n",
    "    if ambiguous:\n",
    "        neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        neg_labels, pos_labels = (0, 0), (1, 1)\n",
    "        subgroups = [(neg, neg_labels), (pos, pos_labels)]\n",
    "    else:\n",
    "        neg_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        neg_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 1]\n",
    "        pos_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 0]\n",
    "        pos_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        neg_neg_labels, neg_pos_labels, pos_neg_labels, pos_pos_labels = (0, 0), (0, 1), (1, 0), (1, 1)\n",
    "        subgroups = [(neg_neg, neg_neg_labels), (neg_pos, neg_pos_labels), (pos_neg, pos_neg_labels), (pos_pos, pos_pos_labels)]\n",
    "    \n",
    "    out = {}\n",
    "    for data, label_profile in subgroups:\n",
    "        out[label_profile] = []\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            text = data[i:i+batch_size]\n",
    "            out[label_profile].append(\n",
    "                (\n",
    "                    text,\n",
    "                    t.tensor([label_profile[0]]*len(text), device=DEVICE),\n",
    "                    t.tensor([label_profile[1]]*len(text), device=DEVICE)\n",
    "                )\n",
    "            )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probe training hyperparameters\n",
    "lr = 1e-2\n",
    "epochs = 1\n",
    "\n",
    "class Probe(nn.Module):\n",
    "    def __init__(self, activation_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(activation_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "def train_probe(get_acts, label_idx=0, batches=get_data(), lr=1e-2, epochs=1, seed=SEED):\n",
    "    t.manual_seed(seed)\n",
    "    probe = Probe(512).to('cuda:0')\n",
    "    optimizer = t.optim.AdamW(probe.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for batch in batches:\n",
    "            text = batch[0]\n",
    "            labels = batch[label_idx+1] \n",
    "            acts = get_acts(text)\n",
    "            logits = probe(acts)\n",
    "            loss = criterion(logits, labels.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return probe, losses\n",
    "\n",
    "def test_probe(probe, get_acts, label_idx=0, batches=get_data(train=False), seed=SEED):\n",
    "    with t.no_grad():\n",
    "        corrects = []\n",
    "\n",
    "        for batch in batches:\n",
    "            text = batch[0]\n",
    "            labels = batch[label_idx+1]\n",
    "            acts = get_acts(text)\n",
    "            logits = probe(acts)\n",
    "            preds = (logits > 0.0).long()\n",
    "            corrects.append((preds == labels).float())\n",
    "        return t.cat(corrects).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acts(text):\n",
    "    with model.trace(text, **tracer_kwargs), t.no_grad():\n",
    "        acts = model.gpt_neox.layers[layer].output[0][:,-1,:].save()\n",
    "    return acts.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "probe0, _ = train_probe(get_acts, label_idx=0, batches=get_data(ambiguous=False), lr=lr, epochs=epochs)\n",
    "probe1, _ = train_probe(get_acts, label_idx=1, batches=get_data(ambiguous=False), lr=lr, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probe 0 accuracy: 0.900921642780304\n",
      "Probe 1 accuracy: 0.9688940048217773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6046"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Probe 0 accuracy:', test_probe(probe0, get_acts, batches=batches, label_idx=0))\n",
    "print('Probe 1 accuracy:', test_probe(probe1, get_acts, batches=batches, label_idx=1))\n",
    "del probe0, probe1\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous test accuracy: 0.9915195107460022\n",
      "Ground truth accuracy: 0.6261520981788635\n",
      "Spurious accuracy: 0.8680875301361084\n"
     ]
    }
   ],
   "source": [
    "probe, _ = train_probe(get_acts, label_idx=0, lr=lr, epochs=epochs)\n",
    "print('Ambiguous test accuracy:', test_probe(probe, get_acts, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(probe, get_acts, batches=batches, label_idx=0))\n",
    "print('Spurious accuracy:', test_probe(probe, get_acts, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.9941375851631165\n",
      "Accuracy for (0, 1): 0.13586179912090302\n",
      "Accuracy for (1, 0): 0.38479262590408325\n",
      "Accuracy for (1, 1): 0.9881505370140076\n"
     ]
    }
   ],
   "source": [
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(probe, get_acts, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "submodules = [\n",
    "    model.gpt_neox.layers[i] for i in range(layer + 1)\n",
    "]\n",
    "dictionaries = {}\n",
    "for i in range(layer + 1):\n",
    "    ae = AutoEncoder(512, 64 * 512).to(DEVICE)\n",
    "    ae.load_state_dict(t.load(f'/share/projects/dictionary_circuits/autoencoders/pythia-70m-deduped/resid_out_layer{i}/{dict_id}_{dictionary_size}/ae.pt'))\n",
    "    dictionaries[submodules[i]] = ae\n",
    "\n",
    "def metric_fn(model, labels=None):\n",
    "    return t.where(\n",
    "        labels == 0,\n",
    "        probe(model.gpt_neox.layers[layer].output[0][:,-1,:]),\n",
    "        - probe(model.gpt_neox.layers[layer].output[0][:,-1,:]) # NOTE: 1 - probe if using sigmoid\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:21<00:00,  1.17it/s]\n"
     ]
    }
   ],
   "source": [
    "n_batches = 25\n",
    "batch_size = 4\n",
    "\n",
    "running_total = 0\n",
    "nodes = None\n",
    "\n",
    "for batch_idx, (clean, labels, _) in tqdm(enumerate(get_data(train=True, ambiguous=True, batch_size=batch_size, seed=SEED)), total=n_batches):\n",
    "    if batch_idx == n_batches:\n",
    "        break\n",
    "\n",
    "    effects, _, _, _ = patching_effect(\n",
    "        clean,\n",
    "        None,\n",
    "        model,\n",
    "        submodules,\n",
    "        dictionaries,\n",
    "        metric_fn,\n",
    "        metric_kwargs=dict(labels=labels),\n",
    "        method='ig'\n",
    "    )\n",
    "    with t.no_grad():\n",
    "        if nodes is None:\n",
    "            nodes = {k : len(clean) * v.sum(dim=1).mean(dim=0) for k, v in effects.items()}\n",
    "        else:\n",
    "            for k, v in effects.items():\n",
    "                nodes[k] += len(clean) * v.sum(dim=1).mean(dim=0)\n",
    "        running_total += len(clean)\n",
    "    del effects, _\n",
    "    gc.collect()\n",
    "\n",
    "nodes = {k : v / running_total for k, v in nodes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0:\n",
      "241 -0.10441476851701736\n",
      "1022 0.2669174075126648\n",
      "3122 0.11432410776615143\n",
      "4074 -0.5427650213241577\n",
      "9610 0.2321901023387909\n",
      "9651 0.7651873826980591\n",
      "10060 0.8378416299819946\n",
      "10282 0.14649847149848938\n",
      "18967 0.6124393939971924\n",
      "22084 0.3060828149318695\n",
      "23255 0.2215987890958786\n",
      "23898 0.36252906918525696\n",
      "24418 0.10960207879543304\n",
      "24435 0.2099316418170929\n",
      "26504 0.4111306667327881\n",
      "29626 0.2115512490272522\n",
      "Layer 1:\n",
      "2995 0.17399932444095612\n",
      "4592 0.46434274315834045\n",
      "8920 0.656842052936554\n",
      "9877 0.41685497760772705\n",
      "10115 0.10725012421607971\n",
      "12128 0.5694507360458374\n",
      "14918 0.21164773404598236\n",
      "15017 1.2835333347320557\n",
      "17369 0.22672824561595917\n",
      "18585 0.2379533350467682\n",
      "26476 -0.13248230516910553\n",
      "30248 0.7422191500663757\n",
      "Layer 2:\n",
      "1995 0.5639375448226929\n",
      "8944 0.24482528865337372\n",
      "9128 1.0962494611740112\n",
      "11656 0.1112290546298027\n",
      "14559 0.3551482558250427\n",
      "14638 0.346517413854599\n",
      "17961 -0.11233101785182953\n",
      "21331 0.2790890336036682\n",
      "26413 0.14219459891319275\n",
      "27838 0.11404399573802948\n",
      "29206 0.20579946041107178\n",
      "29295 0.5753335952758789\n",
      "29371 0.2136802077293396\n",
      "30263 0.1464308649301529\n",
      "30811 0.13386905193328857\n",
      "Layer 3:\n",
      "11945 0.18199656903743744\n",
      "13474 0.21070338785648346\n",
      "15246 0.10567651689052582\n",
      "19558 1.5815459489822388\n",
      "19759 0.19629941880702972\n",
      "21220 0.1081535667181015\n",
      "22800 0.1796964555978775\n",
      "23545 0.3288184404373169\n",
      "24484 0.12781114876270294\n",
      "24806 0.14304473996162415\n",
      "27051 0.18473492562770844\n",
      "27334 0.2681363523006439\n",
      "27867 0.2278132438659668\n",
      "31182 0.19007663428783417\n",
      "31453 0.2892701327800751\n",
      "Layer 4:\n",
      "4125 0.31377875804901123\n",
      "4926 0.10600809752941132\n",
      "9766 0.16603520512580872\n",
      "12332 0.13619448244571686\n",
      "12420 2.1251699924468994\n",
      "20708 0.12099238485097885\n",
      "21979 0.1541997343301773\n",
      "30220 0.9966870546340942\n",
      "total features: 66\n"
     ]
    }
   ],
   "source": [
    "n_features = 0\n",
    "for i, effect in zip(range(layer+1), nodes.values()):\n",
    "    print(f\"Layer {i}:\")\n",
    "    for idx in (effect.act.abs() > 0.1).nonzero():\n",
    "        print(idx.item(), effect.act[idx].item())\n",
    "        n_features += 1\n",
    "print(f\"total features: {n_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' leaves', 3.1683294773101807), (' piece', 2.8998165130615234), (' telling', 2.8359341621398926), (' total', 2.607029914855957), (' tells', 2.443530559539795), (' needing', 2.205601930618286), (' notice', 2.185490608215332), (' finish', 2.0870466232299805), ('accept', 1.889017105102539), (' goes', 1.8599400520324707), (' [...]', 1.8255202770233154), (' 1960', 1.7827792167663574), (' donors', 1.685542345046997), (' Lady', 1.6427953243255615), (' Although', 1.6094988584518433), (' needs', 1.589821219444275), ('ret', 1.516148567199707), (' donations', 1.4957566261291504), (' counting', 1.4677677154541016), (' impressed', 1.4677202701568604), (' jumping', 1.4470441341400146), (' Instead', 1.440384864807129), (' jumped', 1.4324177503585815), ('iza', 1.3016449213027954), (' knees', 1.225874900817871), (' career', 1.162060022354126), (' puzzle', 1.1596356630325317), (' participate', 1.0996900796890259), (' inspired', 0.989930272102356), (' mate', 0.9897094964981079)]\n",
      "[(' her', 1.7537654638290405), (' herself', 1.7392916679382324), (' she', 1.6891292333602905), (' She', 1.2257136106491089), ('She', 1.1365330219268799), (' hers', 1.087778091430664), (' Her', 1.0701675415039062), ('Her', 1.0509861707687378), ('she', 0.9448744654655457), (' daughter', 0.8374568223953247), (' blonde', 0.8058038353919983), (' grandmother', 0.777862012386322), (' raped', 0.7526341080665588), (' husbands', 0.7414524555206299), (' actress', 0.7414485216140747), (' postpartum', 0.7349827289581299), (' tears', 0.7345147728919983), (' boyfriend', 0.7180842161178589), (' pregnancy', 0.7068537473678589), (' husband', 0.6929947137832642), (' girl', 0.6817805171012878), (' niece', 0.6765598058700562), (' sisters', 0.6651015281677246), (' Elle', 0.6559014916419983), ('daughter', 0.6539645791053772), (' menstru', 0.6454604268074036), ('her', 0.6400243043899536), (' homem', 0.6366956233978271), (' Aunt', 0.6318170428276062), (' aunt', 0.6264988780021667)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-6925c48e-fe35\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-6925c48e-fe35\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [[\"T\", \"uls\", \"i\", \" G\", \"abb\", \"ard\", \",\", \" 2020\", \" presidential\", \" candidate\", \",\", \" is\", \" closing\", \" in\", \" on\", \" the\", \" number\", \" of\", \" donations\", \" that\", \" she\", \" needs\", \" in\", \" order\", \" to\", \" qualify\", \" to\", \" participate\", \" in\", \" the\", \" Democratic\", \" debates\", \".\", \" As\", \" of\", \" March\", \" 20\", \",\", \" 2019\", \",\", \" she\", \" has\", \" 40\", \",\", \"456\", \" donations\", \",\", \" which\", \" leaves\", \" her\", \" needing\", \" just\", \" a\", \" little\", \" less\", \" than\", \" 25\", \",\", \"000\", \" more\", \" donations\", \" to\", \" participate\", \" in\", \" the\", \" D\", \"NC\", \" debates\", \".\", \" In\", \" total\", \",\"], [\"Ger\", \"da\", \" Gil\", \"bo\", \"e\", \"\\n\", \"\\n\", \"Ger\", \"da\", \" Gil\", \"bo\", \"e\", \" (\", \"5\", \" July\", \" 1914\", \"\\u00a0\", \"\\u2013\", \" 11\", \" April\", \" 2009\", \")\", \" was\", \" a\", \" Danish\", \" actress\", \" and\", \" singer\", \".\", \" She\", \" appeared\", \" in\", \" 18\", \" films\", \" between\", \" 1943\", \" and\", \" 2003\", \".\", \"\\n\", \"\\n\", \"Life\", \" \", \"\\n\", \"Gil\", \"bo\", \"e\", \" was\", \" born\", \" in\", \" 1914\", \".\", \" She\", \" was\", \" the\", \" daughter\", \" of\", \" a\", \" black\", \"smith\", \",\", \" Gil\", \"bo\", \"e\", \" started\", \" her\", \" career\", \" in\", \" musical\", \" theatre\", \" and\", \" oper\", \"as\", \" in\", \" A\", \"ar\", \"hus\", \" before\", \" she\", \" moved\", \" to\", \" Copenhagen\", \" to\", \" work\", \" at\", \" different\", \" theat\", \"res\", \".\", \" Her\", \" national\", \" breakthrough\", \" came\", \",\", \" when\", \" she\", \"  \", \"accept\", \"ed\", \" the\", \" role\", \" as\", \" El\", \"iza\", \" in\", \" My\", \" Fair\", \" Lady\", \" at\", \" Falk\", \"oner\", \" Te\", \"at\", \"ret\", \" at\", \" short\", \" notice\", \" in\", \" 1960\", \".\"], [\"Ger\", \"da\", \" Gil\", \"bo\", \"e\", \"\\n\", \"\\n\", \"Ger\", \"da\", \" Gil\", \"bo\", \"e\", \" (\", \"5\", \" July\", \" 1914\", \"\\u00a0\", \"\\u2013\", \" 11\", \" April\", \" 2009\", \")\", \" was\", \" a\", \" Danish\", \" actress\", \" and\", \" singer\", \".\", \" She\", \" appeared\", \" in\", \" 18\", \" films\", \" between\", \" 1943\", \" and\", \" 2003\", \".\", \"\\n\", \"\\n\", \"Life\", \" \", \"\\n\", \"Gil\", \"bo\", \"e\", \" was\", \" born\", \" in\", \" 1914\", \".\", \" She\", \" was\", \" the\", \" daughter\", \" of\", \" a\", \" black\", \"smith\", \",\", \" Gil\", \"bo\", \"e\", \" started\", \" her\", \" career\", \" in\", \" musical\", \" theatre\", \" and\", \" oper\", \"as\", \" in\", \" A\", \"ar\", \"hus\", \" before\", \" she\", \" moved\", \" to\", \" Copenhagen\", \" to\", \" work\", \" at\", \" different\", \" theat\", \"res\", \".\"], [\"T\", \"uls\", \"i\", \" G\", \"abb\", \"ard\", \",\", \" 2020\", \" presidential\", \" candidate\", \",\", \" is\", \" closing\", \" in\", \" on\", \" the\", \" number\", \" of\", \" donations\", \" that\", \" she\", \" needs\", \" in\", \" order\", \" to\", \" qualify\", \" to\", \" participate\", \" in\", \" the\", \" Democratic\", \" debates\", \".\", \" As\", \" of\", \" March\", \" 20\", \",\", \" 2019\", \",\", \" she\", \" has\", \" 40\", \",\", \"456\", \" donations\", \",\", \" which\", \" leaves\", \" her\", \" needing\", \" just\", \" a\", \" little\", \" less\", \" than\", \" 25\", \",\", \"000\", \" more\", \" donations\", \" to\"], [\"T\", \"uls\", \"i\", \" G\", \"abb\", \"ard\", \",\", \" 2020\", \" presidential\", \" candidate\", \",\", \" is\", \" closing\", \" in\", \" on\", \" the\", \" number\", \" of\", \" donations\", \" that\", \" she\", \" needs\", \" in\", \" order\", \" to\", \" qualify\", \" to\", \" participate\", \" in\", \" the\", \" Democratic\", \" debates\", \".\", \" As\", \" of\", \" March\", \" 20\", \",\", \" 2019\", \",\", \" she\", \" has\", \" 40\", \",\", \"456\", \" donations\", \",\", \" which\"], [\"Ger\", \"da\", \" Gil\", \"bo\", \"e\", \"\\n\", \"\\n\", \"Ger\", \"da\", \" Gil\", \"bo\", \"e\", \" (\", \"5\", \" July\", \" 1914\", \"\\u00a0\", \"\\u2013\", \" 11\", \" April\", \" 2009\", \")\", \" was\", \" a\", \" Danish\", \" actress\", \" and\", \" singer\", \".\", \" She\", \" appeared\", \" in\", \" 18\", \" films\", \" between\", \" 1943\", \" and\", \" 2003\", \".\", \"\\n\", \"\\n\", \"Life\", \" \", \"\\n\", \"Gil\", \"bo\", \"e\", \" was\", \" born\", \" in\", \" 1914\", \".\", \" She\", \" was\", \" the\", \" daughter\", \" of\", \" a\", \" black\", \"smith\", \",\", \" Gil\", \"bo\", \"e\", \" started\", \" her\", \" career\", \" in\", \" musical\", \" theatre\", \" and\", \" oper\", \"as\", \" in\", \" A\", \"ar\", \"hus\", \" before\", \" she\", \" moved\", \" to\", \" Copenhagen\", \" to\", \" work\", \" at\", \" different\", \" theat\", \"res\", \".\", \" Her\", \" national\", \" breakthrough\", \" came\", \",\", \" when\", \" she\", \"  \", \"accept\", \"ed\", \" the\", \" role\", \" as\", \" El\", \"iza\", \" in\", \" My\", \" Fair\", \" Lady\", \" at\", \" Falk\", \"oner\", \" Te\", \"at\", \"ret\", \" at\", \" short\", \" notice\", \" in\", \" 1960\", \".\", \" Although\"], [\"T\", \"uls\", \"i\", \" G\", \"abb\", \"ard\", \",\", \" 2020\", \" presidential\", \" candidate\", \",\", \" is\", \" closing\", \" in\", \" on\", \" the\", \" number\", \" of\", \" donations\", \" that\", \" she\", \" needs\", \" in\", \" order\", \" to\", \" qualify\", \" to\", \" participate\", \" in\", \" the\", \" Democratic\", \" debates\", \".\", \" As\", \" of\", \" March\", \" 20\", \",\", \" 2019\", \",\", \" she\", \" has\", \" 40\", \",\", \"456\", \" donations\", \",\", \" which\", \" leaves\", \" her\", \" needing\", \" just\", \" a\", \" little\", \" less\", \" than\", \" 25\", \",\", \"000\", \" more\", \" donations\", \" to\", \" participate\", \" in\", \" the\", \" D\", \"NC\", \" debates\", \".\", \" In\", \" total\", \",\", \" as\", \" of\", \" March\", \" 20\", \",\", \" she\", \" needs\", \" 24\", \",\", \"544\", \" more\", \" donations\", \" in\"], [\"T\", \"uls\", \"i\", \" G\", \"abb\", \"ard\", \",\", \" 2020\", \" presidential\", \" candidate\", \",\", \" is\", \" closing\", \" in\", \" on\", \" the\", \" number\", \" of\", \" donations\", \" that\", \" she\", \" needs\", \" in\", \" order\", \" to\", \" qualify\", \" to\", \" participate\", \" in\", \" the\", \" Democratic\", \" debates\", \".\", \" As\", \" of\", \" March\", \" 20\", \",\", \" 2019\", \",\", \" she\", \" has\", \" 40\", \",\", \"456\", \" donations\", \",\", \" which\", \" leaves\", \" her\", \" needing\", \" just\", \" a\", \" little\", \" less\", \" than\", \" 25\", \",\", \"000\", \" more\", \" donations\", \" to\", \" participate\", \" in\", \" the\", \" D\", \"NC\", \" debates\", \".\", \" In\", \" total\", \",\", \" as\", \" of\", \" March\", \" 20\", \",\", \" she\", \" needs\", \" 24\", \",\", \"544\", \" more\", \" donations\", \" in\", \" order\", \" to\", \" qualify\", \" for\", \" the\", \" debates\", \".\"], [\"T\", \"uls\", \"i\", \" G\", \"abb\", \"ard\", \",\", \" 2020\", \" presidential\", \" candidate\", \",\", \" is\", \" closing\", \" in\", \" on\", \" the\", \" number\", \" of\", \" donations\", \" that\", \" she\", \" needs\", \" in\", \" order\", \" to\", \" qualify\", \" to\", \" participate\", \" in\", \" the\", \" Democratic\", \" debates\", \".\", \" As\", \" of\", \" March\", \" 20\", \",\", \" 2019\", \",\", \" she\", \" has\", \" 40\", \",\", \"456\", \" donations\", \",\", \" which\", \" leaves\"], [\"Ger\", \"da\", \" Gil\", \"bo\", \"e\", \"\\n\", \"\\n\", \"Ger\", \"da\", \" Gil\", \"bo\", \"e\", \" (\", \"5\", \" July\", \" 1914\", \"\\u00a0\", \"\\u2013\", \" 11\", \" April\", \" 2009\", \")\", \" was\", \" a\", \" Danish\", \" actress\", \" and\", \" singer\", \".\", \" She\", \" appeared\", \" in\", \" 18\", \" films\", \" between\", \" 1943\", \" and\", \" 2003\", \".\", \"\\n\", \"\\n\", \"Life\", \" \", \"\\n\", \"Gil\", \"bo\", \"e\", \" was\", \" born\", \" in\", \" 1914\", \".\", \" She\", \" was\", \" the\", \" daughter\", \" of\", \" a\", \" black\", \"smith\", \",\", \" Gil\", \"bo\", \"e\", \" started\", \" her\", \" career\", \" in\", \" musical\", \" theatre\", \" and\", \" oper\", \"as\", \" in\", \" A\", \"ar\", \"hus\", \" before\", \" she\", \" moved\", \" to\", \" Copenhagen\", \" to\", \" work\", \" at\", \" different\", \" theat\", \"res\", \".\", \" Her\", \" national\", \" breakthrough\", \" came\", \",\", \" when\"], [\"Ger\", \"da\", \" Gil\", \"bo\", \"e\", \"\\n\", \"\\n\", \"Ger\", \"da\", \" Gil\", \"bo\", \"e\", \" (\", \"5\", \" July\", \" 1914\", \"\\u00a0\", \"\\u2013\", \" 11\", \" April\", \" 2009\", \")\", \" was\", \" a\", \" Danish\", \" actress\", \" and\", \" singer\", \".\", \" She\", \" appeared\", \" in\", \" 18\", \" films\", \" between\", \" 1943\", \" and\", \" 2003\", \".\"], [\"Deb\", \"bie\", \" Gregory\", \" D\", \"NP\", \",\", \" RN\", \"\\n\", \"\\n\", \"Dr\", \".\", \" De\", \"bbie\", \" Gregory\", \" is\", \" a\", \" national\", \" leader\", \" in\", \" healthcare\", \" design\", \",\", \" innovation\", \",\", \" and\", \" transformation\", \".\", \" As\", \" a\", \" nurse\", \" executive\", \" and\", \" interior\", \" designer\", \",\", \" Dr\", \".\", \" Gregory\", \" is\", \" passionate\", \" about\", \" \\u201c\", \"Int\", \"entional\", \" Design\", \"\\u201d\", \" that\", \" align\", \"s\", \" People\", \",\", \" Place\", \",\", \" and\", \" Process\", \".\", \" She\", \" creates\", \" and\", \" transforms\", \" environments\", \" into\", \" functional\", \" ecosystems\", \" using\", \" complex\", \" systems\", \" science\", \" and\", \" strategic\", \" thinking\", \".\"], [\"Jean\", \"ette\", \" Saw\", \"yer\", \" Cohen\", \",\", \" PhD\", \",\", \" clinical\", \" assistant\", \" professor\", \" of\", \" psychology\", \" in\", \" pediatric\", \"s\", \" at\", \" We\", \"ill\", \" Cornell\", \" Medical\", \" College\", \" in\", \" New\", \" York\", \" City\", \"\\n\", \"\\n\", \"P\", \"ediatric\", \" Psych\", \"ologist\", \"\\n\", \"\\n\", \"How\", \" to\", \" Te\", \"ach\", \" Independence\", \"?\", \"\\n\", \"\\n\", \"How\", \" can\", \" I\", \" teach\", \" my\", \" toddler\", \" to\", \" do\", \" things\", \" independently\", \"?\", \"\\n\", \"\\n\", \"You\", \"\\u2019\", \"ve\", \" probably\", \" become\", \" more\", \" patient\", \" since\", \" you\", \" started\", \" this\", \" whole\", \" pa\", \"renthood\", \" thing\", \".\", \" And\", \" you\", \"\\u2019\", \"re\", \" going\", \" to\", \" have\", \" to\", \" practice\", \" patience\", \" even\", \" more\", \" as\", \" your\", \" toddler\", \" learns\", \" to\", \" become\", \" more\", \" independent\", \".\", \"\\n\", \"\\n\", \"For\", \" example\", \",\", \" she\", \" tells\", \" you\", \" she\", \" can\", \"\\u2019\", \"t\", \" finish\", \" the\", \" puzzle\", \" she\", \"\\u2019\", \"s\", \" doing\", \".\", \" Instead\", \" of\", \" jumping\", \" right\", \" in\", \" and\", \" telling\", \" her\", \" which\", \" piece\", \" goes\", \" where\", \",\"], [\"T\", \"uls\", \"i\", \" G\", \"abb\", \"ard\", \",\", \" 2020\", \" presidential\", \" candidate\", \",\", \" is\", \" closing\", \" in\", \" on\", \" the\", \" number\", \" of\", \" donations\", \" that\", \" she\", \" needs\", \" in\", \" order\", \" to\", \" qualify\", \" to\", \" participate\", \" in\", \" the\", \" Democratic\", \" debates\", \".\", \" As\", \" of\", \" March\", \" 20\", \",\", \" 2019\", \",\", \" she\", \" has\", \" 40\", \",\", \"456\", \" donations\", \",\", \" which\", \" leaves\", \" her\", \" needing\", \" just\", \" a\", \" little\", \" less\", \" than\", \" 25\", \",\", \"000\", \" more\", \" donations\", \" to\", \" participate\", \" in\", \" the\", \" D\", \"NC\", \" debates\", \".\", \" In\", \" total\", \",\", \" as\", \" of\", \" March\", \" 20\", \",\", \" she\", \" needs\"], [\"T\", \"uls\", \"i\", \" G\", \"abb\", \"ard\", \",\", \" 2020\", \" presidential\", \" candidate\", \",\", \" is\", \" closing\", \" in\", \" on\", \" the\", \" number\", \" of\", \" donations\", \" that\", \" she\", \" needs\", \" in\", \" order\", \" to\", \" qualify\", \" to\", \" participate\", \" in\", \" the\", \" Democratic\", \" debates\", \".\", \" As\", \" of\", \" March\", \" 20\", \",\", \" 2019\", \",\", \" she\", \" has\", \" 40\", \",\", \"456\", \" donations\", \",\", \" which\", \" leaves\", \" her\", \" needing\", \" just\", \" a\", \" little\", \" less\", \" than\", \" 25\", \",\", \"000\", \" more\", \" donations\", \" to\", \" participate\", \" in\", \" the\", \" D\", \"NC\", \" debates\", \".\", \" In\", \" total\", \",\", \" as\", \" of\", \" March\", \" 20\", \",\", \" she\", \" needs\", \" 24\", \",\", \"544\", \" more\", \" donations\", \" in\", \" order\", \" to\", \" qualify\"], [\"Jean\", \"ette\", \" Saw\", \"yer\", \" Cohen\", \",\", \" PhD\", \",\", \" clinical\", \" assistant\", \" professor\", \" of\", \" psychology\", \" in\", \" pediatric\", \"s\", \" at\", \" We\", \"ill\", \" Cornell\", \" Medical\", \" College\", \" in\", \" New\", \" York\", \" City\", \"\\n\", \"\\n\", \"P\", \"ediatric\", \" Psych\", \"ologist\", \"\\n\", \"\\n\", \"How\", \" to\", \" Te\", \"ach\", \" Independence\", \"?\", \"\\n\", \"\\n\", \"How\", \" can\", \" I\", \" teach\", \" my\", \" toddler\", \" to\", \" do\", \" things\", \" independently\", \"?\", \"\\n\", \"\\n\", \"You\", \"\\u2019\", \"ve\", \" probably\", \" become\", \" more\", \" patient\", \" since\", \" you\", \" started\", \" this\", \" whole\", \" pa\", \"renthood\", \" thing\", \".\", \" And\", \" you\", \"\\u2019\", \"re\", \" going\", \" to\", \" have\", \" to\", \" practice\", \" patience\", \" even\", \" more\", \" as\", \" your\", \" toddler\", \" learns\", \" to\", \" become\", \" more\", \" independent\", \".\", \"\\n\", \"\\n\", \"For\", \" example\", \",\", \" she\", \" tells\", \" you\", \" she\", \" can\", \"\\u2019\", \"t\", \" finish\", \" the\", \" puzzle\", \" she\", \"\\u2019\", \"s\", \" doing\", \".\", \" Instead\", \" of\", \" jumping\", \" right\", \" in\", \" and\", \" telling\", \" her\", \" which\", \" piece\"], [\"Jean\", \"ette\", \" Saw\", \"yer\", \" Cohen\", \",\", \" PhD\", \",\", \" clinical\", \" assistant\", \" professor\", \" of\", \" psychology\", \" in\", \" pediatric\", \"s\", \" at\", \" We\", \"ill\", \" Cornell\", \" Medical\", \" College\", \" in\", \" New\", \" York\", \" City\", \"\\n\", \"\\n\", \"P\", \"ediatric\", \" Psych\", \"ologist\", \"\\n\", \"\\n\", \"How\", \" to\", \" Te\", \"ach\", \" Independence\", \"?\", \"\\n\", \"\\n\", \"How\", \" can\", \" I\", \" teach\", \" my\", \" toddler\", \" to\", \" do\", \" things\", \" independently\", \"?\", \"\\n\", \"\\n\", \"You\", \"\\u2019\", \"ve\", \" probably\", \" become\", \" more\", \" patient\", \" since\", \" you\", \" started\", \" this\", \" whole\", \" pa\", \"renthood\", \" thing\", \".\", \" And\", \" you\", \"\\u2019\", \"re\", \" going\", \" to\", \" have\", \" to\", \" practice\", \" patience\", \" even\", \" more\", \" as\", \" your\", \" toddler\", \" learns\", \" to\", \" become\", \" more\", \" independent\", \".\", \"\\n\", \"\\n\", \"For\", \" example\", \",\", \" she\", \" tells\", \" you\", \" she\", \" can\", \"\\u2019\", \"t\", \" finish\", \" the\", \" puzzle\", \" she\", \"\\u2019\", \"s\", \" doing\", \".\", \" Instead\"], [\"Jean\", \"ette\", \" Saw\", \"yer\", \" Cohen\", \",\", \" PhD\", \",\", \" clinical\", \" assistant\", \" professor\", \" of\", \" psychology\", \" in\", \" pediatric\", \"s\", \" at\", \" We\", \"ill\", \" Cornell\", \" Medical\", \" College\", \" in\", \" New\", \" York\", \" City\", \"\\n\", \"\\n\", \"P\", \"ediatric\", \" Psych\", \"ologist\", \"\\n\", \"\\n\", \"How\", \" to\", \" Te\", \"ach\", \" Independence\", \"?\", \"\\n\", \"\\n\", \"How\", \" can\", \" I\", \" teach\", \" my\", \" toddler\", \" to\", \" do\", \" things\", \" independently\", \"?\", \"\\n\", \"\\n\", \"You\", \"\\u2019\", \"ve\", \" probably\", \" become\", \" more\", \" patient\", \" since\", \" you\", \" started\", \" this\", \" whole\", \" pa\", \"renthood\", \" thing\", \".\", \" And\", \" you\", \"\\u2019\", \"re\", \" going\", \" to\", \" have\", \" to\", \" practice\", \" patience\", \" even\", \" more\", \" as\", \" your\", \" toddler\", \" learns\", \" to\", \" become\", \" more\", \" independent\", \".\", \"\\n\", \"\\n\", \"For\", \" example\", \",\", \" she\", \" tells\", \" you\", \" she\", \" can\", \"\\u2019\", \"t\", \" finish\", \" the\", \" puzzle\", \" she\", \"\\u2019\", \"s\", \" doing\", \".\", \" Instead\", \" of\", \" jumping\", \" right\", \" in\", \" and\", \" telling\"], [\"T\", \"uls\", \"i\", \" G\", \"abb\", \"ard\", \",\", \" 2020\", \" presidential\", \" candidate\", \",\", \" is\", \" closing\", \" in\", \" on\", \" the\", \" number\", \" of\", \" donations\", \" that\", \" she\", \" needs\", \" in\", \" order\", \" to\", \" qualify\", \" to\", \" participate\", \" in\", \" the\", \" Democratic\", \" debates\", \".\", \" As\", \" of\", \" March\", \" 20\", \",\", \" 2019\", \",\", \" she\", \" has\", \" 40\", \",\", \"456\", \" donations\", \",\", \" which\", \" leaves\", \" her\", \" needing\", \" just\", \" a\", \" little\", \" less\", \" than\", \" 25\", \",\", \"000\", \" more\", \" donations\", \" to\", \" participate\", \" in\", \" the\", \" D\", \"NC\", \" debates\", \".\"], [\"Jean\", \"ette\", \" Saw\", \"yer\", \" Cohen\", \",\", \" PhD\", \",\", \" clinical\", \" assistant\", \" professor\", \" of\", \" psychology\", \" in\", \" pediatric\", \"s\", \" at\", \" We\", \"ill\", \" Cornell\", \" Medical\", \" College\", \" in\", \" New\", \" York\", \" City\", \"\\n\", \"\\n\", \"P\", \"ediatric\", \" Psych\", \"ologist\", \"\\n\", \"\\n\", \"How\", \" to\", \" Te\", \"ach\", \" Independence\", \"?\", \"\\n\", \"\\n\", \"How\", \" can\", \" I\", \" teach\", \" my\", \" toddler\", \" to\", \" do\", \" things\", \" independently\", \"?\", \"\\n\", \"\\n\", \"You\", \"\\u2019\", \"ve\", \" probably\", \" become\", \" more\", \" patient\", \" since\", \" you\", \" started\", \" this\", \" whole\", \" pa\", \"renthood\", \" thing\", \".\", \" And\", \" you\", \"\\u2019\", \"re\", \" going\", \" to\", \" have\", \" to\", \" practice\", \" patience\", \" even\", \" more\", \" as\", \" your\", \" toddler\", \" learns\", \" to\", \" become\", \" more\", \" independent\", \".\", \"\\n\", \"\\n\", \"For\", \" example\", \",\", \" she\", \" tells\", \" you\", \" she\", \" can\", \"\\u2019\", \"t\", \" finish\", \" the\", \" puzzle\", \" she\", \"\\u2019\", \"s\", \" doing\", \".\", \" Instead\", \" of\"], [\"Jean\", \"ette\", \" Saw\", \"yer\", \" Cohen\", \",\", \" PhD\", \",\", \" clinical\", \" assistant\", \" professor\", \" of\", \" psychology\", \" in\", \" pediatric\", \"s\", \" at\", \" We\", \"ill\", \" Cornell\", \" Medical\", \" College\", \" in\", \" New\", \" York\", \" City\", \"\\n\", \"\\n\", \"P\", \"ediatric\", \" Psych\", \"ologist\", \"\\n\", \"\\n\", \"How\", \" to\", \" Te\", \"ach\", \" Independence\", \"?\", \"\\n\", \"\\n\", \"How\", \" can\", \" I\", \" teach\", \" my\", \" toddler\", \" to\", \" do\", \" things\", \" independently\", \"?\", \"\\n\", \"\\n\", \"You\", \"\\u2019\", \"ve\", \" probably\", \" become\", \" more\", \" patient\", \" since\", \" you\", \" started\", \" this\", \" whole\", \" pa\", \"renthood\", \" thing\", \".\", \" And\", \" you\", \"\\u2019\", \"re\", \" going\", \" to\", \" have\", \" to\", \" practice\", \" patience\", \" even\", \" more\", \" as\", \" your\", \" toddler\", \" learns\", \" to\", \" become\", \" more\", \" independent\", \".\", \"\\n\", \"\\n\", \"For\", \" example\", \",\", \" she\", \" tells\", \" you\", \" she\", \" can\", \"\\u2019\", \"t\", \" finish\", \" the\", \" puzzle\", \" she\", \"\\u2019\", \"s\", \" doing\", \".\"], [\"T\", \"uls\", \"i\", \" G\", \"abb\", \"ard\", \",\", \" 2020\", \" presidential\", \" candidate\", \",\", \" is\", \" closing\", \" in\", \" on\", \" the\", \" number\", \" of\", \" donations\", \" that\", \" she\", \" needs\", \" in\", \" order\", \" to\", \" qualify\", \" to\", \" participate\", \" in\", \" the\", \" Democratic\", \" debates\", \".\", \" As\", \" of\", \" March\", \" 20\", \",\", \" 2019\", \",\", \" she\", \" has\", \" 40\", \",\", \"456\", \" donations\", \",\", \" which\", \" leaves\", \" her\", \" needing\", \" just\", \" a\", \" little\", \" less\", \" than\", \" 25\", \",\", \"000\", \" more\", \" donations\", \" to\", \" participate\", \" in\", \" the\", \" D\", \"NC\", \" debates\", \".\", \" In\", \" total\", \",\", \" as\", \" of\", \" March\", \" 20\", \",\", \" she\", \" needs\", \" 24\", \",\", \"544\", \" more\", \" donations\", \" in\", \" order\", \" to\", \" qualify\", \" for\", \" the\", \" debates\", \".\", \"\\n\", \"\\n\", \"In\", \" a\", \" phone\", \" call\", \" with\"], [\"T\", \"uls\", \"i\", \" G\", \"abb\", \"ard\", \",\", \" 2020\", \" presidential\", \" candidate\", \",\", \" is\", \" closing\", \" in\", \" on\", \" the\", \" number\", \" of\", \" donations\", \" that\", \" she\", \" needs\", \" in\", \" order\", \" to\", \" qualify\", \" to\", \" participate\", \" in\", \" the\", \" Democratic\", \" debates\", \".\", \" As\", \" of\", \" March\", \" 20\", \",\", \" 2019\", \",\", \" she\", \" has\", \" 40\", \",\", \"456\", \" donations\", \",\", \" which\", \" leaves\", \" her\", \" needing\", \" just\", \" a\", \" little\", \" less\", \" than\", \" 25\", \",\", \"000\", \" more\", \" donations\", \" to\", \" participate\", \" in\", \" the\", \" D\", \"NC\", \" debates\", \".\", \" In\", \" total\", \",\", \" as\", \" of\", \" March\", \" 20\", \",\", \" she\", \" needs\", \" 24\", \",\", \"544\", \" more\", \" donations\", \" in\", \" order\", \" to\", \" qualify\", \" for\", \" the\", \" debates\", \".\", \"\\n\", \"\\n\"], [\"T\", \"uls\", \"i\", \" G\", \"abb\", \"ard\", \",\", \" 2020\", \" presidential\", \" candidate\", \",\", \" is\", \" closing\", \" in\", \" on\", \" the\", \" number\", \" of\", \" donations\", \" that\", \" she\", \" needs\", \" in\", \" order\", \" to\", \" qualify\", \" to\", \" participate\", \" in\", \" the\", \" Democratic\", \" debates\", \".\", \" As\", \" of\", \" March\", \" 20\", \",\", \" 2019\", \",\", \" she\", \" has\", \" 40\", \",\", \"456\", \" donations\", \",\", \" which\", \" leaves\", \" her\", \" needing\", \" just\", \" a\", \" little\", \" less\", \" than\", \" 25\", \",\", \"000\", \" more\", \" donations\", \" to\", \" participate\", \" in\", \" the\", \" D\", \"NC\", \" debates\", \".\", \" In\", \" total\", \",\", \" as\"], [\"T\", \"uls\", \"i\", \" G\", \"abb\", \"ard\", \",\", \" 2020\", \" presidential\", \" candidate\", \",\", \" is\", \" closing\", \" in\", \" on\", \" the\", \" number\", \" of\", \" donations\", \" that\", \" she\", \" needs\", \" in\", \" order\", \" to\", \" qualify\", \" to\", \" participate\", \" in\", \" the\", \" Democratic\", \" debates\", \".\", \" As\", \" of\", \" March\", \" 20\", \",\", \" 2019\", \",\", \" she\", \" has\", \" 40\", \",\", \"456\", \" donations\", \",\", \" which\", \" leaves\", \" her\", \" needing\", \" just\", \" a\", \" little\", \" less\", \" than\", \" 25\", \",\", \"000\", \" more\", \" donations\", \" to\", \" participate\", \" in\", \" the\", \" D\", \"NC\", \" debates\", \".\", \" In\", \" total\", \",\", \" as\", \" of\", \" March\", \" 20\", \",\", \" she\", \" needs\", \" 24\", \",\", \"544\", \" more\", \" donations\", \" in\", \" order\", \" to\", \" qualify\", \" for\", \" the\", \" debates\", \".\", \"\\n\", \"\\n\", \"In\", \" a\", \" phone\", \" call\", \" with\", \" volunteers\", \" on\", \" March\", \" 18\", \",\"], [\"T\", \"uls\", \"i\", \" G\", \"abb\", \"ard\", \",\", \" 2020\", \" presidential\", \" candidate\", \",\", \" is\", \" closing\", \" in\", \" on\", \" the\", \" number\", \" of\", \" donations\", \" that\", \" she\", \" needs\", \" in\", \" order\", \" to\", \" qualify\", \" to\", \" participate\", \" in\", \" the\", \" Democratic\", \" debates\", \".\", \" As\", \" of\", \" March\", \" 20\", \",\", \" 2019\", \",\", \" she\", \" has\", \" 40\", \",\", \"456\", \" donations\", \",\", \" which\", \" leaves\", \" her\", \" needing\", \" just\", \" a\", \" little\", \" less\", \" than\", \" 25\", \",\", \"000\", \" more\", \" donations\", \" to\", \" participate\", \" in\", \" the\", \" D\", \"NC\", \" debates\", \".\", \" In\", \" total\"], [\"T\", \"uls\", \"i\", \" G\", \"abb\", \"ard\", \",\", \" 2020\", \" presidential\", \" candidate\", \",\", \" is\", \" closing\", \" in\", \" on\", \" the\", \" number\", \" of\", \" donations\", \" that\", \" she\", \" needs\", \" in\", \" order\", \" to\", \" qualify\", \" to\", \" participate\", \" in\", \" the\", \" Democratic\", \" debates\", \".\", \" As\", \" of\", \" March\", \" 20\", \",\", \" 2019\", \",\", \" she\", \" has\", \" 40\", \",\", \"456\", \" donations\", \",\", \" which\", \" leaves\", \" her\", \" needing\", \" just\", \" a\", \" little\", \" less\", \" than\", \" 25\", \",\", \"000\", \" more\", \" donations\"], [\"Jean\", \"ette\", \" Saw\", \"yer\", \" Cohen\", \",\", \" PhD\", \",\", \" clinical\", \" assistant\", \" professor\", \" of\", \" psychology\", \" in\", \" pediatric\", \"s\", \" at\", \" We\", \"ill\", \" Cornell\", \" Medical\", \" College\", \" in\", \" New\", \" York\", \" City\", \"\\n\", \"\\n\", \"P\", \"ediatric\", \" Psych\", \"ologist\", \"\\n\", \"\\n\", \"How\", \" to\", \" Te\", \"ach\", \" Independence\", \"?\", \"\\n\", \"\\n\", \"How\", \" can\", \" I\", \" teach\", \" my\", \" toddler\", \" to\", \" do\", \" things\", \" independently\", \"?\", \"\\n\", \"\\n\", \"You\", \"\\u2019\", \"ve\", \" probably\", \" become\", \" more\", \" patient\", \" since\", \" you\", \" started\", \" this\", \" whole\", \" pa\", \"renthood\", \" thing\", \".\", \" And\", \" you\", \"\\u2019\", \"re\", \" going\", \" to\", \" have\", \" to\", \" practice\", \" patience\", \" even\", \" more\", \" as\", \" your\", \" toddler\", \" learns\", \" to\", \" become\", \" more\", \" independent\", \".\", \"\\n\", \"\\n\", \"For\", \" example\", \",\", \" she\", \" tells\"], [\"T\", \"uls\", \"i\", \" G\", \"abb\", \"ard\", \",\", \" 2020\", \" presidential\", \" candidate\", \",\", \" is\", \" closing\", \" in\", \" on\", \" the\", \" number\", \" of\", \" donations\", \" that\", \" she\", \" needs\", \" in\", \" order\", \" to\", \" qualify\", \" to\", \" participate\", \" in\", \" the\", \" Democratic\", \" debates\", \".\", \" As\", \" of\", \" March\", \" 20\", \",\", \" 2019\", \",\", \" she\", \" has\", \" 40\", \",\", \"456\", \" donations\", \",\", \" which\", \" leaves\", \" her\", \" needing\", \" just\", \" a\", \" little\", \" less\", \" than\", \" 25\", \",\", \"000\", \" more\", \" donations\", \" to\", \" participate\", \" in\", \" the\", \" D\", \"NC\", \" debates\", \".\", \" In\", \" total\", \",\", \" as\", \" of\", \" March\", \" 20\", \",\", \" she\", \" needs\", \" 24\", \",\", \"544\", \" more\", \" donations\", \" in\", \" order\", \" to\", \" qualify\", \" for\", \" the\", \" debates\", \".\", \"\\n\", \"\\n\", \"In\", \" a\", \" phone\", \" call\"], [\"Jean\", \"ette\", \" Saw\", \"yer\", \" Cohen\", \",\", \" PhD\", \",\", \" clinical\", \" assistant\", \" professor\", \" of\", \" psychology\", \" in\", \" pediatric\", \"s\", \" at\", \" We\", \"ill\", \" Cornell\", \" Medical\", \" College\", \" in\", \" New\", \" York\", \" City\", \"\\n\", \"\\n\", \"P\", \"ediatric\", \" Psych\", \"ologist\", \"\\n\", \"\\n\", \"How\", \" to\", \" Te\", \"ach\", \" Independence\", \"?\", \"\\n\", \"\\n\", \"How\", \" can\", \" I\", \" teach\", \" my\", \" toddler\", \" to\", \" do\", \" things\", \" independently\", \"?\", \"\\n\", \"\\n\", \"You\", \"\\u2019\", \"ve\", \" probably\", \" become\", \" more\", \" patient\", \" since\", \" you\", \" started\", \" this\", \" whole\", \" pa\", \"renthood\", \" thing\", \".\", \" And\", \" you\", \"\\u2019\", \"re\", \" going\", \" to\", \" have\", \" to\", \" practice\", \" patience\", \" even\", \" more\", \" as\", \" your\", \" toddler\", \" learns\", \" to\", \" become\", \" more\", \" independent\", \".\", \"\\n\", \"\\n\", \"For\", \" example\", \",\", \" she\", \" tells\", \" you\", \" she\", \" can\", \"\\u2019\", \"t\", \" finish\", \" the\", \" puzzle\", \" she\", \"\\u2019\", \"s\", \" doing\", \".\", \" Instead\", \" of\", \" jumping\", \" right\", \" in\", \" and\", \" telling\", \" her\", \" which\", \" piece\", \" goes\", \" where\"]], \"activations\": [[[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.06347006559371948]], [[0.0]], [[1.802854299545288]], [[1.8749361038208008]], [[0.4218263626098633]], [[1.3782384395599365]], [[1.7601149082183838]], [[1.2460565567016602]], [[1.0286014080047607]], [[2.0453052520751953]], [[0.2444269061088562]], [[0.0]], [[0.5851564407348633]], [[0.9765392541885376]], [[1.5294780731201172]], [[1.2176717519760132]], [[0.5375726222991943]], [[0.9212256669998169]], [[1.4400856494903564]], [[0.9198911190032959]], [[2.0164506435394287]], [[0.6767697334289551]], [[1.611403465270996]], [[0.689254641532898]], [[0.9989840984344482]], [[0.7477866411209106]], [[1.4163498878479004]], [[2.2330617904663086]], [[3.3266074657440186]], [[3.1683294773101807]], [[1.0645406246185303]], [[2.205601930618286]], [[1.479196548461914]], [[0.8417621850967407]], [[0.37722790241241455]], [[0.5752818584442139]], [[1.6664834022521973]], [[0.044951021671295166]], [[0.6719822883605957]], [[0.5143978595733643]], [[0.5707625150680542]], [[2.452650308609009]], [[3.3323323726654053]], [[1.6675353050231934]], [[1.6783554553985596]], [[0.7006189823150635]], [[0.0]], [[0.3579052686691284]], [[1.0309221744537354]], [[2.766800880432129]], [[2.347296714782715]], [[2.607029914855957]], [[3.462661027908325]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.19252455234527588]], [[1.5357298851013184]], [[0.11711913347244263]], [[0.7301456928253174]], [[0.6620317697525024]], [[1.6288032531738281]], [[1.7002463340759277]], [[0.33100950717926025]], [[1.5263159275054932]], [[2.2165164947509766]], [[0.5544388294219971]], [[0.16506034135818481]], [[1.2183122634887695]], [[3.0259435176849365]], [[2.038299322128296]], [[1.5671467781066895]], [[0.29698920249938965]], [[0.35701608657836914]], [[1.1949892044067383]], [[0.0]], [[0.0]], [[0.0]], [[0.6630306243896484]], [[0.1992517113685608]], [[0.5563148260116577]], [[0.18784064054489136]], [[1.6920790672302246]], [[0.5608751773834229]], [[0.7957826852798462]], [[0.8173302412033081]], [[0.6153416633605957]], [[1.5441951751708984]], [[0.48517048358917236]], [[0.0]], [[0.9063986539840698]], [[1.1922430992126465]], [[0.0]], [[0.0]], [[0.1374339461326599]], [[1.4789707660675049]], [[0.7796556949615479]], [[1.162060022354126]], [[1.072366714477539]], [[0.49346923828125]], [[0.5654232501983643]], [[1.9741647243499756]], [[0.0]], [[0.818589448928833]], [[1.0906678438186646]], [[0.0]], [[0.0]], [[0.009081423282623291]], [[2.237544536590576]], [[0.7942969799041748]], [[1.4353389739990234]], [[1.2263027429580688]], [[0.8989136219024658]], [[1.5806267261505127]], [[1.7955222129821777]], [[1.2940897941589355]], [[0.4295459985733032]], [[0.04721909761428833]], [[1.2235300540924072]], [[3.381244421005249]], [[1.433750867843628]], [[0.9269771575927734]], [[1.2032521963119507]], [[1.4076807498931885]], [[2.23384690284729]], [[3.0321080684661865]], [[0.7829387187957764]], [[1.6960620880126953]], [[1.889017105102539]], [[2.3741543292999268]], [[1.2542074918746948]], [[1.637669324874878]], [[1.648871898651123]], [[0.0]], [[1.3016449213027954]], [[2.3480308055877686]], [[0.4468945264816284]], [[0.0]], [[1.6427953243255615]], [[1.5494005680084229]], [[0.29579657316207886]], [[1.6793525218963623]], [[0.13978898525238037]], [[0.04718589782714844]], [[1.516148567199707]], [[1.508683681488037]], [[0.44605910778045654]], [[2.185490608215332]], [[1.9890217781066895]], [[1.7827792167663574]], [[3.429654836654663]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.19252455234527588]], [[1.5357298851013184]], [[0.11711913347244263]], [[0.7301456928253174]], [[0.6620317697525024]], [[1.6288032531738281]], [[1.7002463340759277]], [[0.33100950717926025]], [[1.5263159275054932]], [[2.2165164947509766]], [[0.5544388294219971]], [[0.16506034135818481]], [[1.2183122634887695]], [[3.0259435176849365]], [[2.038299322128296]], [[1.5671467781066895]], [[0.29698920249938965]], [[0.35701608657836914]], [[1.1949892044067383]], [[0.0]], [[0.0]], [[0.0]], [[0.6630306243896484]], [[0.1992517113685608]], [[0.5563148260116577]], [[0.18784064054489136]], [[1.6920790672302246]], [[0.5608751773834229]], [[0.7957826852798462]], [[0.8173302412033081]], [[0.6153416633605957]], [[1.5441951751708984]], [[0.48517048358917236]], [[0.0]], [[0.9063986539840698]], [[1.1922430992126465]], [[0.0]], [[0.0]], [[0.1374339461326599]], [[1.4789707660675049]], [[0.7796556949615479]], [[1.162060022354126]], [[1.072366714477539]], [[0.49346923828125]], [[0.5654232501983643]], [[1.9741647243499756]], [[0.0]], [[0.818589448928833]], [[1.0906678438186646]], [[0.0]], [[0.0]], [[0.009081423282623291]], [[2.237544536590576]], [[0.7942969799041748]], [[1.4353389739990234]], [[1.2263027429580688]], [[0.8989136219024658]], [[1.5806267261505127]], [[1.7955222129821777]], [[1.2940897941589355]], [[0.4295459985733032]], [[0.04721909761428833]], [[1.2235300540924072]], [[3.381244421005249]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.06347006559371948]], [[0.0]], [[1.802854299545288]], [[1.8749361038208008]], [[0.4218263626098633]], [[1.3782384395599365]], [[1.7601149082183838]], [[1.2460565567016602]], [[1.0286014080047607]], [[2.0453052520751953]], [[0.2444269061088562]], [[0.0]], [[0.5851564407348633]], [[0.9765392541885376]], [[1.5294780731201172]], [[1.2176717519760132]], [[0.5375726222991943]], [[0.9212256669998169]], [[1.4400856494903564]], [[0.9198911190032959]], [[2.0164506435394287]], [[0.6767697334289551]], [[1.611403465270996]], [[0.689254641532898]], [[0.9989840984344482]], [[0.7477866411209106]], [[1.4163498878479004]], [[2.2330617904663086]], [[3.3266074657440186]], [[3.1683294773101807]], [[1.0645406246185303]], [[2.205601930618286]], [[1.479196548461914]], [[0.8417621850967407]], [[0.37722790241241455]], [[0.5752818584442139]], [[1.6664834022521973]], [[0.044951021671295166]], [[0.6719822883605957]], [[0.5143978595733643]], [[0.5707625150680542]], [[2.452650308609009]], [[3.3323323726654053]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.06347006559371948]], [[0.0]], [[1.802854299545288]], [[1.8749361038208008]], [[0.4218263626098633]], [[1.3782384395599365]], [[1.7601149082183838]], [[1.2460565567016602]], [[1.0286014080047607]], [[2.0453052520751953]], [[0.2444269061088562]], [[0.0]], [[0.5851564407348633]], [[0.9765392541885376]], [[1.5294780731201172]], [[1.2176717519760132]], [[0.5375726222991943]], [[0.9212256669998169]], [[1.4400856494903564]], [[0.9198911190032959]], [[2.0164506435394287]], [[0.6767697334289551]], [[1.611403465270996]], [[0.689254641532898]], [[0.9989840984344482]], [[0.7477866411209106]], [[1.4163498878479004]], [[2.2330617904663086]], [[3.3266074657440186]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.19252455234527588]], [[1.5357298851013184]], [[0.11711913347244263]], [[0.7301456928253174]], [[0.6620317697525024]], [[1.6288032531738281]], [[1.7002463340759277]], [[0.33100950717926025]], [[1.5263159275054932]], [[2.2165164947509766]], [[0.5544388294219971]], [[0.16506034135818481]], [[1.2183122634887695]], [[3.0259435176849365]], [[2.038299322128296]], [[1.5671467781066895]], [[0.29698920249938965]], [[0.35701608657836914]], [[1.1949892044067383]], [[0.0]], [[0.0]], [[0.0]], [[0.6630306243896484]], [[0.1992517113685608]], [[0.5563148260116577]], [[0.18784064054489136]], [[1.6920790672302246]], [[0.5608751773834229]], [[0.7957826852798462]], [[0.8173302412033081]], [[0.6153416633605957]], [[1.5441951751708984]], [[0.48517048358917236]], [[0.0]], [[0.9063986539840698]], [[1.1922430992126465]], [[0.0]], [[0.0]], [[0.1374339461326599]], [[1.4789707660675049]], [[0.7796556949615479]], [[1.162060022354126]], [[1.072366714477539]], [[0.49346923828125]], [[0.5654232501983643]], [[1.9741647243499756]], [[0.0]], [[0.818589448928833]], [[1.0906678438186646]], [[0.0]], [[0.0]], [[0.009081423282623291]], [[2.237544536590576]], [[0.7942969799041748]], [[1.4353389739990234]], [[1.2263027429580688]], [[0.8989136219024658]], [[1.5806267261505127]], [[1.7955222129821777]], [[1.2940897941589355]], [[0.4295459985733032]], [[0.04721909761428833]], [[1.2235300540924072]], [[3.381244421005249]], [[1.433750867843628]], [[0.9269771575927734]], [[1.2032521963119507]], [[1.4076807498931885]], [[2.23384690284729]], [[3.0321080684661865]], [[0.7829387187957764]], [[1.6960620880126953]], [[1.889017105102539]], [[2.3741543292999268]], [[1.2542074918746948]], [[1.637669324874878]], [[1.648871898651123]], [[0.0]], [[1.3016449213027954]], [[2.3480308055877686]], [[0.4468945264816284]], [[0.0]], [[1.6427953243255615]], [[1.5494005680084229]], [[0.29579657316207886]], [[1.6793525218963623]], [[0.13978898525238037]], [[0.04718589782714844]], [[1.516148567199707]], [[1.508683681488037]], [[0.44605910778045654]], [[2.185490608215332]], [[1.9890217781066895]], [[1.7827792167663574]], [[3.429654836654663]], [[3.2189977169036865]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.06347006559371948]], [[0.0]], [[1.802854299545288]], [[1.8749361038208008]], [[0.4218263626098633]], [[1.3782384395599365]], [[1.7601149082183838]], [[1.2460565567016602]], [[1.0286014080047607]], [[2.0453052520751953]], [[0.2444269061088562]], [[0.0]], [[0.5851564407348633]], [[0.9765392541885376]], [[1.5294780731201172]], [[1.2176717519760132]], [[0.5375726222991943]], [[0.9212256669998169]], [[1.4400856494903564]], [[0.9198911190032959]], [[2.0164506435394287]], [[0.6767697334289551]], [[1.611403465270996]], [[0.689254641532898]], [[0.9989840984344482]], [[0.7477866411209106]], [[1.4163498878479004]], [[2.2330617904663086]], [[3.3266074657440186]], [[3.1683294773101807]], [[1.0645406246185303]], [[2.205601930618286]], [[1.479196548461914]], [[0.8417621850967407]], [[0.37722790241241455]], [[0.5752818584442139]], [[1.6664834022521973]], [[0.044951021671295166]], [[0.6719822883605957]], [[0.5143978595733643]], [[0.5707625150680542]], [[2.452650308609009]], [[3.3323323726654053]], [[1.6675353050231934]], [[1.6783554553985596]], [[0.7006189823150635]], [[0.0]], [[0.3579052686691284]], [[1.0309221744537354]], [[2.766800880432129]], [[2.347296714782715]], [[2.607029914855957]], [[3.462661027908325]], [[2.6269211769104004]], [[1.1981770992279053]], [[0.0]], [[1.785571813583374]], [[1.6312081813812256]], [[1.7764215469360352]], [[2.966609239578247]], [[1.575878620147705]], [[0.5335711240768433]], [[0.6071090698242188]], [[1.0149058103561401]], [[2.1140263080596924]], [[3.190859794616699]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.06347006559371948]], [[0.0]], [[1.802854299545288]], [[1.8749361038208008]], [[0.4218263626098633]], [[1.3782384395599365]], [[1.7601149082183838]], [[1.2460565567016602]], [[1.0286014080047607]], [[2.0453052520751953]], [[0.2444269061088562]], [[0.0]], [[0.5851564407348633]], [[0.9765392541885376]], [[1.5294780731201172]], [[1.2176717519760132]], [[0.5375726222991943]], [[0.9212256669998169]], [[1.4400856494903564]], [[0.9198911190032959]], [[2.0164506435394287]], [[0.6767697334289551]], [[1.611403465270996]], [[0.689254641532898]], [[0.9989840984344482]], [[0.7477866411209106]], [[1.4163498878479004]], [[2.2330617904663086]], [[3.3266074657440186]], [[3.1683294773101807]], [[1.0645406246185303]], [[2.205601930618286]], [[1.479196548461914]], [[0.8417621850967407]], [[0.37722790241241455]], [[0.5752818584442139]], [[1.6664834022521973]], [[0.044951021671295166]], [[0.6719822883605957]], [[0.5143978595733643]], [[0.5707625150680542]], [[2.452650308609009]], [[3.3323323726654053]], [[1.6675353050231934]], [[1.6783554553985596]], [[0.7006189823150635]], [[0.0]], [[0.3579052686691284]], [[1.0309221744537354]], [[2.766800880432129]], [[2.347296714782715]], [[2.607029914855957]], [[3.462661027908325]], [[2.6269211769104004]], [[1.1981770992279053]], [[0.0]], [[1.785571813583374]], [[1.6312081813812256]], [[1.7764215469360352]], [[2.966609239578247]], [[1.575878620147705]], [[0.5335711240768433]], [[0.6071090698242188]], [[1.0149058103561401]], [[2.1140263080596924]], [[3.190859794616699]], [[0.3823971748352051]], [[1.530935525894165]], [[2.917895793914795]], [[2.081772804260254]], [[1.1767274141311646]], [[1.7527835369110107]], [[3.1707708835601807]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.06347006559371948]], [[0.0]], [[1.802854299545288]], [[1.8749361038208008]], [[0.4218263626098633]], [[1.3782384395599365]], [[1.7601149082183838]], [[1.2460565567016602]], [[1.0286014080047607]], [[2.0453052520751953]], [[0.2444269061088562]], [[0.0]], [[0.5851564407348633]], [[0.9765392541885376]], [[1.5294780731201172]], [[1.2176717519760132]], [[0.5375726222991943]], [[0.9212256669998169]], [[1.4400856494903564]], [[0.9198911190032959]], [[2.0164506435394287]], [[0.6767697334289551]], [[1.611403465270996]], [[0.689254641532898]], [[0.9989840984344482]], [[0.7477866411209106]], [[1.4163498878479004]], [[2.2330617904663086]], [[3.3266074657440186]], [[3.1683294773101807]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.19252455234527588]], [[1.5357298851013184]], [[0.11711913347244263]], [[0.7301456928253174]], [[0.6620317697525024]], [[1.6288032531738281]], [[1.7002463340759277]], [[0.33100950717926025]], [[1.5263159275054932]], [[2.2165164947509766]], [[0.5544388294219971]], [[0.16506034135818481]], [[1.2183122634887695]], [[3.0259435176849365]], [[2.038299322128296]], [[1.5671467781066895]], [[0.29698920249938965]], [[0.35701608657836914]], [[1.1949892044067383]], [[0.0]], [[0.0]], [[0.0]], [[0.6630306243896484]], [[0.1992517113685608]], [[0.5563148260116577]], [[0.18784064054489136]], [[1.6920790672302246]], [[0.5608751773834229]], [[0.7957826852798462]], [[0.8173302412033081]], [[0.6153416633605957]], [[1.5441951751708984]], [[0.48517048358917236]], [[0.0]], [[0.9063986539840698]], [[1.1922430992126465]], [[0.0]], [[0.0]], [[0.1374339461326599]], [[1.4789707660675049]], [[0.7796556949615479]], [[1.162060022354126]], [[1.072366714477539]], [[0.49346923828125]], [[0.5654232501983643]], [[1.9741647243499756]], [[0.0]], [[0.818589448928833]], [[1.0906678438186646]], [[0.0]], [[0.0]], [[0.009081423282623291]], [[2.237544536590576]], [[0.7942969799041748]], [[1.4353389739990234]], [[1.2263027429580688]], [[0.8989136219024658]], [[1.5806267261505127]], [[1.7955222129821777]], [[1.2940897941589355]], [[0.4295459985733032]], [[0.04721909761428833]], [[1.2235300540924072]], [[3.381244421005249]], [[1.433750867843628]], [[0.9269771575927734]], [[1.2032521963119507]], [[1.4076807498931885]], [[2.23384690284729]], [[3.0321080684661865]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.19252455234527588]], [[1.5357298851013184]], [[0.11711913347244263]], [[0.7301456928253174]], [[0.6620317697525024]], [[1.6288032531738281]], [[1.7002463340759277]], [[0.33100950717926025]], [[1.5263159275054932]], [[2.2165164947509766]], [[0.5544388294219971]], [[0.16506034135818481]], [[1.2183122634887695]], [[3.0259435176849365]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.11318778991699219]], [[0.0]], [[0.14594411849975586]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.45492684841156006]], [[0.2932857275009155]], [[0.36901211738586426]], [[0.0]], [[0.4095134735107422]], [[0.0]], [[0.0629931092262268]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.9563347101211548]], [[0.8930401802062988]], [[0.22011590003967285]], [[0.5122741460800171]], [[0.7176159620285034]], [[1.1254106760025024]], [[0.0]], [[0.4252878427505493]], [[1.1200542449951172]], [[0.0]], [[0.3610774278640747]], [[0.06579703092575073]], [[0.7249469757080078]], [[0.39098072052001953]], [[0.9034920930862427]], [[0.13902240991592407]], [[0.0]], [[0.0]], [[0.0]], [[0.254102885723114]], [[0.3051215410232544]], [[0.5698531866073608]], [[0.449138879776001]], [[0.0]], [[0.0]], [[0.0]], [[0.25574564933776855]], [[0.0]], [[0.0]], [[0.4365602731704712]], [[0.4073362350463867]], [[0.91033935546875]], [[0.4250760078430176]], [[0.789321780204773]], [[0.4907146692276001]], [[0.6711746454238892]], [[0.0]], [[0.6012779474258423]], [[1.5645813941955566]], [[0.029314637184143066]], [[0.5427138805389404]], [[0.770331859588623]], [[1.4657483100891113]], [[0.11434590816497803]], [[0.8383995294570923]], [[3.002575397491455]]], [[[0.0]], [[0.15370863676071167]], [[0.4303387403488159]], [[0.7042974233627319]], [[0.0]], [[0.2045365571975708]], [[0.0]], [[0.15135014057159424]], [[0.047507286071777344]], [[0.3375891447067261]], [[0.3439897298812866]], [[0.477217435836792]], [[0.2047526240348816]], [[0.8729926347732544]], [[0.0]], [[0.4485386610031128]], [[0.07710707187652588]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.23235493898391724]], [[0.0]], [[0.0]], [[0.4675610065460205]], [[0.02217763662338257]], [[0.2881569266319275]], [[0.2720351219177246]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.17077898979187012]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.03359323740005493]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.1973302960395813]], [[0.0]], [[0.027334213256835938]], [[0.1089557409286499]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.39060115814208984]], [[0.20470088720321655]], [[0.3241773843765259]], [[0.0]], [[0.03818768262863159]], [[0.010790586471557617]], [[0.0]], [[0.0]], [[0.08270227909088135]], [[0.1549307107925415]], [[0.11284643411636353]], [[0.22594016790390015]], [[2.443530559539795]], [[1.0439453125]], [[0.6924594640731812]], [[1.3123409748077393]], [[0.06080728769302368]], [[1.4182922840118408]], [[2.0870466232299805]], [[0.49309539794921875]], [[1.1596356630325317]], [[1.559216022491455]], [[0.8559755086898804]], [[0.9577215909957886]], [[1.2363677024841309]], [[2.6973049640655518]], [[2.880769729614258]], [[2.762855291366577]], [[1.4470441341400146]], [[0.8177244663238525]], [[0.7882870435714722]], [[1.922593593597412]], [[2.8359341621398926]], [[1.509955883026123]], [[2.0163373947143555]], [[2.8998165130615234]], [[1.8599400520324707]], [[2.381211996078491]], [[2.9821057319641113]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.06347006559371948]], [[0.0]], [[1.802854299545288]], [[1.8749361038208008]], [[0.4218263626098633]], [[1.3782384395599365]], [[1.7601149082183838]], [[1.2460565567016602]], [[1.0286014080047607]], [[2.0453052520751953]], [[0.2444269061088562]], [[0.0]], [[0.5851564407348633]], [[0.9765392541885376]], [[1.5294780731201172]], [[1.2176717519760132]], [[0.5375726222991943]], [[0.9212256669998169]], [[1.4400856494903564]], [[0.9198911190032959]], [[2.0164506435394287]], [[0.6767697334289551]], [[1.611403465270996]], [[0.689254641532898]], [[0.9989840984344482]], [[0.7477866411209106]], [[1.4163498878479004]], [[2.2330617904663086]], [[3.3266074657440186]], [[3.1683294773101807]], [[1.0645406246185303]], [[2.205601930618286]], [[1.479196548461914]], [[0.8417621850967407]], [[0.37722790241241455]], [[0.5752818584442139]], [[1.6664834022521973]], [[0.044951021671295166]], [[0.6719822883605957]], [[0.5143978595733643]], [[0.5707625150680542]], [[2.452650308609009]], [[3.3323323726654053]], [[1.6675353050231934]], [[1.6783554553985596]], [[0.7006189823150635]], [[0.0]], [[0.3579052686691284]], [[1.0309221744537354]], [[2.766800880432129]], [[2.347296714782715]], [[2.607029914855957]], [[3.462661027908325]], [[2.6269211769104004]], [[1.1981770992279053]], [[0.0]], [[1.785571813583374]], [[1.6312081813812256]], [[1.7764215469360352]], [[2.966609239578247]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.06347006559371948]], [[0.0]], [[1.802854299545288]], [[1.8749361038208008]], [[0.4218263626098633]], [[1.3782384395599365]], [[1.7601149082183838]], [[1.2460565567016602]], [[1.0286014080047607]], [[2.0453052520751953]], [[0.2444269061088562]], [[0.0]], [[0.5851564407348633]], [[0.9765392541885376]], [[1.5294780731201172]], [[1.2176717519760132]], [[0.5375726222991943]], [[0.9212256669998169]], [[1.4400856494903564]], [[0.9198911190032959]], [[2.0164506435394287]], [[0.6767697334289551]], [[1.611403465270996]], [[0.689254641532898]], [[0.9989840984344482]], [[0.7477866411209106]], [[1.4163498878479004]], [[2.2330617904663086]], [[3.3266074657440186]], [[3.1683294773101807]], [[1.0645406246185303]], [[2.205601930618286]], [[1.479196548461914]], [[0.8417621850967407]], [[0.37722790241241455]], [[0.5752818584442139]], [[1.6664834022521973]], [[0.044951021671295166]], [[0.6719822883605957]], [[0.5143978595733643]], [[0.5707625150680542]], [[2.452650308609009]], [[3.3323323726654053]], [[1.6675353050231934]], [[1.6783554553985596]], [[0.7006189823150635]], [[0.0]], [[0.3579052686691284]], [[1.0309221744537354]], [[2.766800880432129]], [[2.347296714782715]], [[2.607029914855957]], [[3.462661027908325]], [[2.6269211769104004]], [[1.1981770992279053]], [[0.0]], [[1.785571813583374]], [[1.6312081813812256]], [[1.7764215469360352]], [[2.966609239578247]], [[1.575878620147705]], [[0.5335711240768433]], [[0.6071090698242188]], [[1.0149058103561401]], [[2.1140263080596924]], [[3.190859794616699]], [[0.3823971748352051]], [[1.530935525894165]], [[2.917895793914795]]], [[[0.0]], [[0.15370863676071167]], [[0.4303387403488159]], [[0.7042974233627319]], [[0.0]], [[0.2045365571975708]], [[0.0]], [[0.15135014057159424]], [[0.047507286071777344]], [[0.3375891447067261]], [[0.3439897298812866]], [[0.477217435836792]], [[0.2047526240348816]], [[0.8729926347732544]], [[0.0]], [[0.4485386610031128]], [[0.07710707187652588]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.23235493898391724]], [[0.0]], [[0.0]], [[0.4675610065460205]], [[0.02217763662338257]], [[0.2881569266319275]], [[0.2720351219177246]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.17077898979187012]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.03359323740005493]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.1973302960395813]], [[0.0]], [[0.027334213256835938]], [[0.1089557409286499]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.39060115814208984]], [[0.20470088720321655]], [[0.3241773843765259]], [[0.0]], [[0.03818768262863159]], [[0.010790586471557617]], [[0.0]], [[0.0]], [[0.08270227909088135]], [[0.1549307107925415]], [[0.11284643411636353]], [[0.22594016790390015]], [[2.443530559539795]], [[1.0439453125]], [[0.6924594640731812]], [[1.3123409748077393]], [[0.06080728769302368]], [[1.4182922840118408]], [[2.0870466232299805]], [[0.49309539794921875]], [[1.1596356630325317]], [[1.559216022491455]], [[0.8559755086898804]], [[0.9577215909957886]], [[1.2363677024841309]], [[2.6973049640655518]], [[2.880769729614258]], [[2.762855291366577]], [[1.4470441341400146]], [[0.8177244663238525]], [[0.7882870435714722]], [[1.922593593597412]], [[2.8359341621398926]], [[1.509955883026123]], [[2.0163373947143555]], [[2.8998165130615234]]], [[[0.0]], [[0.15370863676071167]], [[0.4303387403488159]], [[0.7042974233627319]], [[0.0]], [[0.2045365571975708]], [[0.0]], [[0.15135014057159424]], [[0.047507286071777344]], [[0.3375891447067261]], [[0.3439897298812866]], [[0.477217435836792]], [[0.2047526240348816]], [[0.8729926347732544]], [[0.0]], [[0.4485386610031128]], [[0.07710707187652588]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.23235493898391724]], [[0.0]], [[0.0]], [[0.4675610065460205]], [[0.02217763662338257]], [[0.2881569266319275]], [[0.2720351219177246]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.17077898979187012]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.03359323740005493]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.1973302960395813]], [[0.0]], [[0.027334213256835938]], [[0.1089557409286499]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.39060115814208984]], [[0.20470088720321655]], [[0.3241773843765259]], [[0.0]], [[0.03818768262863159]], [[0.010790586471557617]], [[0.0]], [[0.0]], [[0.08270227909088135]], [[0.1549307107925415]], [[0.11284643411636353]], [[0.22594016790390015]], [[2.443530559539795]], [[1.0439453125]], [[0.6924594640731812]], [[1.3123409748077393]], [[0.06080728769302368]], [[1.4182922840118408]], [[2.0870466232299805]], [[0.49309539794921875]], [[1.1596356630325317]], [[1.559216022491455]], [[0.8559755086898804]], [[0.9577215909957886]], [[1.2363677024841309]], [[2.6973049640655518]], [[2.880769729614258]]], [[[0.0]], [[0.15370863676071167]], [[0.4303387403488159]], [[0.7042974233627319]], [[0.0]], [[0.2045365571975708]], [[0.0]], [[0.15135014057159424]], [[0.047507286071777344]], [[0.3375891447067261]], [[0.3439897298812866]], [[0.477217435836792]], [[0.2047526240348816]], [[0.8729926347732544]], [[0.0]], [[0.4485386610031128]], [[0.07710707187652588]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.23235493898391724]], [[0.0]], [[0.0]], [[0.4675610065460205]], [[0.02217763662338257]], [[0.2881569266319275]], [[0.2720351219177246]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.17077898979187012]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.03359323740005493]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.1973302960395813]], [[0.0]], [[0.027334213256835938]], [[0.1089557409286499]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.39060115814208984]], [[0.20470088720321655]], [[0.3241773843765259]], [[0.0]], [[0.03818768262863159]], [[0.010790586471557617]], [[0.0]], [[0.0]], [[0.08270227909088135]], [[0.1549307107925415]], [[0.11284643411636353]], [[0.22594016790390015]], [[2.443530559539795]], [[1.0439453125]], [[0.6924594640731812]], [[1.3123409748077393]], [[0.06080728769302368]], [[1.4182922840118408]], [[2.0870466232299805]], [[0.49309539794921875]], [[1.1596356630325317]], [[1.559216022491455]], [[0.8559755086898804]], [[0.9577215909957886]], [[1.2363677024841309]], [[2.6973049640655518]], [[2.880769729614258]], [[2.762855291366577]], [[1.4470441341400146]], [[0.8177244663238525]], [[0.7882870435714722]], [[1.922593593597412]], [[2.8359341621398926]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.06347006559371948]], [[0.0]], [[1.802854299545288]], [[1.8749361038208008]], [[0.4218263626098633]], [[1.3782384395599365]], [[1.7601149082183838]], [[1.2460565567016602]], [[1.0286014080047607]], [[2.0453052520751953]], [[0.2444269061088562]], [[0.0]], [[0.5851564407348633]], [[0.9765392541885376]], [[1.5294780731201172]], [[1.2176717519760132]], [[0.5375726222991943]], [[0.9212256669998169]], [[1.4400856494903564]], [[0.9198911190032959]], [[2.0164506435394287]], [[0.6767697334289551]], [[1.611403465270996]], [[0.689254641532898]], [[0.9989840984344482]], [[0.7477866411209106]], [[1.4163498878479004]], [[2.2330617904663086]], [[3.3266074657440186]], [[3.1683294773101807]], [[1.0645406246185303]], [[2.205601930618286]], [[1.479196548461914]], [[0.8417621850967407]], [[0.37722790241241455]], [[0.5752818584442139]], [[1.6664834022521973]], [[0.044951021671295166]], [[0.6719822883605957]], [[0.5143978595733643]], [[0.5707625150680542]], [[2.452650308609009]], [[3.3323323726654053]], [[1.6675353050231934]], [[1.6783554553985596]], [[0.7006189823150635]], [[0.0]], [[0.3579052686691284]], [[1.0309221744537354]], [[2.766800880432129]]], [[[0.0]], [[0.15370863676071167]], [[0.4303387403488159]], [[0.7042974233627319]], [[0.0]], [[0.2045365571975708]], [[0.0]], [[0.15135014057159424]], [[0.047507286071777344]], [[0.3375891447067261]], [[0.3439897298812866]], [[0.477217435836792]], [[0.2047526240348816]], [[0.8729926347732544]], [[0.0]], [[0.4485386610031128]], [[0.07710707187652588]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.23235493898391724]], [[0.0]], [[0.0]], [[0.4675610065460205]], [[0.02217763662338257]], [[0.2881569266319275]], [[0.2720351219177246]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.17077898979187012]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.03359323740005493]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.1973302960395813]], [[0.0]], [[0.027334213256835938]], [[0.1089557409286499]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.39060115814208984]], [[0.20470088720321655]], [[0.3241773843765259]], [[0.0]], [[0.03818768262863159]], [[0.010790586471557617]], [[0.0]], [[0.0]], [[0.08270227909088135]], [[0.1549307107925415]], [[0.11284643411636353]], [[0.22594016790390015]], [[2.443530559539795]], [[1.0439453125]], [[0.6924594640731812]], [[1.3123409748077393]], [[0.06080728769302368]], [[1.4182922840118408]], [[2.0870466232299805]], [[0.49309539794921875]], [[1.1596356630325317]], [[1.559216022491455]], [[0.8559755086898804]], [[0.9577215909957886]], [[1.2363677024841309]], [[2.6973049640655518]], [[2.880769729614258]], [[2.762855291366577]]], [[[0.0]], [[0.15370863676071167]], [[0.4303387403488159]], [[0.7042974233627319]], [[0.0]], [[0.2045365571975708]], [[0.0]], [[0.15135014057159424]], [[0.047507286071777344]], [[0.3375891447067261]], [[0.3439897298812866]], [[0.477217435836792]], [[0.2047526240348816]], [[0.8729926347732544]], [[0.0]], [[0.4485386610031128]], [[0.07710707187652588]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.23235493898391724]], [[0.0]], [[0.0]], [[0.4675610065460205]], [[0.02217763662338257]], [[0.2881569266319275]], [[0.2720351219177246]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.17077898979187012]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.03359323740005493]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.1973302960395813]], [[0.0]], [[0.027334213256835938]], [[0.1089557409286499]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.39060115814208984]], [[0.20470088720321655]], [[0.3241773843765259]], [[0.0]], [[0.03818768262863159]], [[0.010790586471557617]], [[0.0]], [[0.0]], [[0.08270227909088135]], [[0.1549307107925415]], [[0.11284643411636353]], [[0.22594016790390015]], [[2.443530559539795]], [[1.0439453125]], [[0.6924594640731812]], [[1.3123409748077393]], [[0.06080728769302368]], [[1.4182922840118408]], [[2.0870466232299805]], [[0.49309539794921875]], [[1.1596356630325317]], [[1.559216022491455]], [[0.8559755086898804]], [[0.9577215909957886]], [[1.2363677024841309]], [[2.6973049640655518]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.06347006559371948]], [[0.0]], [[1.802854299545288]], [[1.8749361038208008]], [[0.4218263626098633]], [[1.3782384395599365]], [[1.7601149082183838]], [[1.2460565567016602]], [[1.0286014080047607]], [[2.0453052520751953]], [[0.2444269061088562]], [[0.0]], [[0.5851564407348633]], [[0.9765392541885376]], [[1.5294780731201172]], [[1.2176717519760132]], [[0.5375726222991943]], [[0.9212256669998169]], [[1.4400856494903564]], [[0.9198911190032959]], [[2.0164506435394287]], [[0.6767697334289551]], [[1.611403465270996]], [[0.689254641532898]], [[0.9989840984344482]], [[0.7477866411209106]], [[1.4163498878479004]], [[2.2330617904663086]], [[3.3266074657440186]], [[3.1683294773101807]], [[1.0645406246185303]], [[2.205601930618286]], [[1.479196548461914]], [[0.8417621850967407]], [[0.37722790241241455]], [[0.5752818584442139]], [[1.6664834022521973]], [[0.044951021671295166]], [[0.6719822883605957]], [[0.5143978595733643]], [[0.5707625150680542]], [[2.452650308609009]], [[3.3323323726654053]], [[1.6675353050231934]], [[1.6783554553985596]], [[0.7006189823150635]], [[0.0]], [[0.3579052686691284]], [[1.0309221744537354]], [[2.766800880432129]], [[2.347296714782715]], [[2.607029914855957]], [[3.462661027908325]], [[2.6269211769104004]], [[1.1981770992279053]], [[0.0]], [[1.785571813583374]], [[1.6312081813812256]], [[1.7764215469360352]], [[2.966609239578247]], [[1.575878620147705]], [[0.5335711240768433]], [[0.6071090698242188]], [[1.0149058103561401]], [[2.1140263080596924]], [[3.190859794616699]], [[0.3823971748352051]], [[1.530935525894165]], [[2.917895793914795]], [[2.081772804260254]], [[1.1767274141311646]], [[1.7527835369110107]], [[3.1707708835601807]], [[2.051011562347412]], [[2.6565797328948975]], [[1.9720652103424072]], [[0.9408859014511108]], [[0.3527991771697998]], [[2.439871311187744]], [[2.6888978481292725]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.06347006559371948]], [[0.0]], [[1.802854299545288]], [[1.8749361038208008]], [[0.4218263626098633]], [[1.3782384395599365]], [[1.7601149082183838]], [[1.2460565567016602]], [[1.0286014080047607]], [[2.0453052520751953]], [[0.2444269061088562]], [[0.0]], [[0.5851564407348633]], [[0.9765392541885376]], [[1.5294780731201172]], [[1.2176717519760132]], [[0.5375726222991943]], [[0.9212256669998169]], [[1.4400856494903564]], [[0.9198911190032959]], [[2.0164506435394287]], [[0.6767697334289551]], [[1.611403465270996]], [[0.689254641532898]], [[0.9989840984344482]], [[0.7477866411209106]], [[1.4163498878479004]], [[2.2330617904663086]], [[3.3266074657440186]], [[3.1683294773101807]], [[1.0645406246185303]], [[2.205601930618286]], [[1.479196548461914]], [[0.8417621850967407]], [[0.37722790241241455]], [[0.5752818584442139]], [[1.6664834022521973]], [[0.044951021671295166]], [[0.6719822883605957]], [[0.5143978595733643]], [[0.5707625150680542]], [[2.452650308609009]], [[3.3323323726654053]], [[1.6675353050231934]], [[1.6783554553985596]], [[0.7006189823150635]], [[0.0]], [[0.3579052686691284]], [[1.0309221744537354]], [[2.766800880432129]], [[2.347296714782715]], [[2.607029914855957]], [[3.462661027908325]], [[2.6269211769104004]], [[1.1981770992279053]], [[0.0]], [[1.785571813583374]], [[1.6312081813812256]], [[1.7764215469360352]], [[2.966609239578247]], [[1.575878620147705]], [[0.5335711240768433]], [[0.6071090698242188]], [[1.0149058103561401]], [[2.1140263080596924]], [[3.190859794616699]], [[0.3823971748352051]], [[1.530935525894165]], [[2.917895793914795]], [[2.081772804260254]], [[1.1767274141311646]], [[1.7527835369110107]], [[3.1707708835601807]], [[2.051011562347412]], [[2.6565797328948975]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.06347006559371948]], [[0.0]], [[1.802854299545288]], [[1.8749361038208008]], [[0.4218263626098633]], [[1.3782384395599365]], [[1.7601149082183838]], [[1.2460565567016602]], [[1.0286014080047607]], [[2.0453052520751953]], [[0.2444269061088562]], [[0.0]], [[0.5851564407348633]], [[0.9765392541885376]], [[1.5294780731201172]], [[1.2176717519760132]], [[0.5375726222991943]], [[0.9212256669998169]], [[1.4400856494903564]], [[0.9198911190032959]], [[2.0164506435394287]], [[0.6767697334289551]], [[1.611403465270996]], [[0.689254641532898]], [[0.9989840984344482]], [[0.7477866411209106]], [[1.4163498878479004]], [[2.2330617904663086]], [[3.3266074657440186]], [[3.1683294773101807]], [[1.0645406246185303]], [[2.205601930618286]], [[1.479196548461914]], [[0.8417621850967407]], [[0.37722790241241455]], [[0.5752818584442139]], [[1.6664834022521973]], [[0.044951021671295166]], [[0.6719822883605957]], [[0.5143978595733643]], [[0.5707625150680542]], [[2.452650308609009]], [[3.3323323726654053]], [[1.6675353050231934]], [[1.6783554553985596]], [[0.7006189823150635]], [[0.0]], [[0.3579052686691284]], [[1.0309221744537354]], [[2.766800880432129]], [[2.347296714782715]], [[2.607029914855957]], [[3.462661027908325]], [[2.6269211769104004]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.06347006559371948]], [[0.0]], [[1.802854299545288]], [[1.8749361038208008]], [[0.4218263626098633]], [[1.3782384395599365]], [[1.7601149082183838]], [[1.2460565567016602]], [[1.0286014080047607]], [[2.0453052520751953]], [[0.2444269061088562]], [[0.0]], [[0.5851564407348633]], [[0.9765392541885376]], [[1.5294780731201172]], [[1.2176717519760132]], [[0.5375726222991943]], [[0.9212256669998169]], [[1.4400856494903564]], [[0.9198911190032959]], [[2.0164506435394287]], [[0.6767697334289551]], [[1.611403465270996]], [[0.689254641532898]], [[0.9989840984344482]], [[0.7477866411209106]], [[1.4163498878479004]], [[2.2330617904663086]], [[3.3266074657440186]], [[3.1683294773101807]], [[1.0645406246185303]], [[2.205601930618286]], [[1.479196548461914]], [[0.8417621850967407]], [[0.37722790241241455]], [[0.5752818584442139]], [[1.6664834022521973]], [[0.044951021671295166]], [[0.6719822883605957]], [[0.5143978595733643]], [[0.5707625150680542]], [[2.452650308609009]], [[3.3323323726654053]], [[1.6675353050231934]], [[1.6783554553985596]], [[0.7006189823150635]], [[0.0]], [[0.3579052686691284]], [[1.0309221744537354]], [[2.766800880432129]], [[2.347296714782715]], [[2.607029914855957]], [[3.462661027908325]], [[2.6269211769104004]], [[1.1981770992279053]], [[0.0]], [[1.785571813583374]], [[1.6312081813812256]], [[1.7764215469360352]], [[2.966609239578247]], [[1.575878620147705]], [[0.5335711240768433]], [[0.6071090698242188]], [[1.0149058103561401]], [[2.1140263080596924]], [[3.190859794616699]], [[0.3823971748352051]], [[1.530935525894165]], [[2.917895793914795]], [[2.081772804260254]], [[1.1767274141311646]], [[1.7527835369110107]], [[3.1707708835601807]], [[2.051011562347412]], [[2.6565797328948975]], [[1.9720652103424072]], [[0.9408859014511108]], [[0.3527991771697998]], [[2.439871311187744]], [[2.6888978481292725]], [[1.4930927753448486]], [[1.616208553314209]], [[0.0]], [[1.2653350830078125]], [[2.6206161975860596]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.06347006559371948]], [[0.0]], [[1.802854299545288]], [[1.8749361038208008]], [[0.4218263626098633]], [[1.3782384395599365]], [[1.7601149082183838]], [[1.2460565567016602]], [[1.0286014080047607]], [[2.0453052520751953]], [[0.2444269061088562]], [[0.0]], [[0.5851564407348633]], [[0.9765392541885376]], [[1.5294780731201172]], [[1.2176717519760132]], [[0.5375726222991943]], [[0.9212256669998169]], [[1.4400856494903564]], [[0.9198911190032959]], [[2.0164506435394287]], [[0.6767697334289551]], [[1.611403465270996]], [[0.689254641532898]], [[0.9989840984344482]], [[0.7477866411209106]], [[1.4163498878479004]], [[2.2330617904663086]], [[3.3266074657440186]], [[3.1683294773101807]], [[1.0645406246185303]], [[2.205601930618286]], [[1.479196548461914]], [[0.8417621850967407]], [[0.37722790241241455]], [[0.5752818584442139]], [[1.6664834022521973]], [[0.044951021671295166]], [[0.6719822883605957]], [[0.5143978595733643]], [[0.5707625150680542]], [[2.452650308609009]], [[3.3323323726654053]], [[1.6675353050231934]], [[1.6783554553985596]], [[0.7006189823150635]], [[0.0]], [[0.3579052686691284]], [[1.0309221744537354]], [[2.766800880432129]], [[2.347296714782715]], [[2.607029914855957]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.06347006559371948]], [[0.0]], [[1.802854299545288]], [[1.8749361038208008]], [[0.4218263626098633]], [[1.3782384395599365]], [[1.7601149082183838]], [[1.2460565567016602]], [[1.0286014080047607]], [[2.0453052520751953]], [[0.2444269061088562]], [[0.0]], [[0.5851564407348633]], [[0.9765392541885376]], [[1.5294780731201172]], [[1.2176717519760132]], [[0.5375726222991943]], [[0.9212256669998169]], [[1.4400856494903564]], [[0.9198911190032959]], [[2.0164506435394287]], [[0.6767697334289551]], [[1.611403465270996]], [[0.689254641532898]], [[0.9989840984344482]], [[0.7477866411209106]], [[1.4163498878479004]], [[2.2330617904663086]], [[3.3266074657440186]], [[3.1683294773101807]], [[1.0645406246185303]], [[2.205601930618286]], [[1.479196548461914]], [[0.8417621850967407]], [[0.37722790241241455]], [[0.5752818584442139]], [[1.6664834022521973]], [[0.044951021671295166]], [[0.6719822883605957]], [[0.5143978595733643]], [[0.5707625150680542]], [[2.452650308609009]]], [[[0.0]], [[0.15370863676071167]], [[0.4303387403488159]], [[0.7042974233627319]], [[0.0]], [[0.2045365571975708]], [[0.0]], [[0.15135014057159424]], [[0.047507286071777344]], [[0.3375891447067261]], [[0.3439897298812866]], [[0.477217435836792]], [[0.2047526240348816]], [[0.8729926347732544]], [[0.0]], [[0.4485386610031128]], [[0.07710707187652588]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.23235493898391724]], [[0.0]], [[0.0]], [[0.4675610065460205]], [[0.02217763662338257]], [[0.2881569266319275]], [[0.2720351219177246]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.17077898979187012]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.03359323740005493]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.1973302960395813]], [[0.0]], [[0.027334213256835938]], [[0.1089557409286499]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.39060115814208984]], [[0.20470088720321655]], [[0.3241773843765259]], [[0.0]], [[0.03818768262863159]], [[0.010790586471557617]], [[0.0]], [[0.0]], [[0.08270227909088135]], [[0.1549307107925415]], [[0.11284643411636353]], [[0.22594016790390015]], [[2.443530559539795]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.06347006559371948]], [[0.0]], [[1.802854299545288]], [[1.8749361038208008]], [[0.4218263626098633]], [[1.3782384395599365]], [[1.7601149082183838]], [[1.2460565567016602]], [[1.0286014080047607]], [[2.0453052520751953]], [[0.2444269061088562]], [[0.0]], [[0.5851564407348633]], [[0.9765392541885376]], [[1.5294780731201172]], [[1.2176717519760132]], [[0.5375726222991943]], [[0.9212256669998169]], [[1.4400856494903564]], [[0.9198911190032959]], [[2.0164506435394287]], [[0.6767697334289551]], [[1.611403465270996]], [[0.689254641532898]], [[0.9989840984344482]], [[0.7477866411209106]], [[1.4163498878479004]], [[2.2330617904663086]], [[3.3266074657440186]], [[3.1683294773101807]], [[1.0645406246185303]], [[2.205601930618286]], [[1.479196548461914]], [[0.8417621850967407]], [[0.37722790241241455]], [[0.5752818584442139]], [[1.6664834022521973]], [[0.044951021671295166]], [[0.6719822883605957]], [[0.5143978595733643]], [[0.5707625150680542]], [[2.452650308609009]], [[3.3323323726654053]], [[1.6675353050231934]], [[1.6783554553985596]], [[0.7006189823150635]], [[0.0]], [[0.3579052686691284]], [[1.0309221744537354]], [[2.766800880432129]], [[2.347296714782715]], [[2.607029914855957]], [[3.462661027908325]], [[2.6269211769104004]], [[1.1981770992279053]], [[0.0]], [[1.785571813583374]], [[1.6312081813812256]], [[1.7764215469360352]], [[2.966609239578247]], [[1.575878620147705]], [[0.5335711240768433]], [[0.6071090698242188]], [[1.0149058103561401]], [[2.1140263080596924]], [[3.190859794616699]], [[0.3823971748352051]], [[1.530935525894165]], [[2.917895793914795]], [[2.081772804260254]], [[1.1767274141311646]], [[1.7527835369110107]], [[3.1707708835601807]], [[2.051011562347412]], [[2.6565797328948975]], [[1.9720652103424072]], [[0.9408859014511108]], [[0.3527991771697998]], [[2.439871311187744]]], [[[0.0]], [[0.15370863676071167]], [[0.4303387403488159]], [[0.7042974233627319]], [[0.0]], [[0.2045365571975708]], [[0.0]], [[0.15135014057159424]], [[0.047507286071777344]], [[0.3375891447067261]], [[0.3439897298812866]], [[0.477217435836792]], [[0.2047526240348816]], [[0.8729926347732544]], [[0.0]], [[0.4485386610031128]], [[0.07710707187652588]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.23235493898391724]], [[0.0]], [[0.0]], [[0.4675610065460205]], [[0.02217763662338257]], [[0.2881569266319275]], [[0.2720351219177246]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.17077898979187012]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.03359323740005493]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.1973302960395813]], [[0.0]], [[0.027334213256835938]], [[0.1089557409286499]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.39060115814208984]], [[0.20470088720321655]], [[0.3241773843765259]], [[0.0]], [[0.03818768262863159]], [[0.010790586471557617]], [[0.0]], [[0.0]], [[0.08270227909088135]], [[0.1549307107925415]], [[0.11284643411636353]], [[0.22594016790390015]], [[2.443530559539795]], [[1.0439453125]], [[0.6924594640731812]], [[1.3123409748077393]], [[0.06080728769302368]], [[1.4182922840118408]], [[2.0870466232299805]], [[0.49309539794921875]], [[1.1596356630325317]], [[1.559216022491455]], [[0.8559755086898804]], [[0.9577215909957886]], [[1.2363677024841309]], [[2.6973049640655518]], [[2.880769729614258]], [[2.762855291366577]], [[1.4470441341400146]], [[0.8177244663238525]], [[0.7882870435714722]], [[1.922593593597412]], [[2.8359341621398926]], [[1.509955883026123]], [[2.0163373947143555]], [[2.8998165130615234]], [[1.8599400520324707]], [[2.381211996078491]]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7fe9e2700290>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "component_idx = 4\n",
    "feat_idx = 12420\n",
    "\n",
    "submodule = submodules[component_idx]\n",
    "dictionary = dictionaries[submodule]\n",
    "\n",
    "# interpret some features\n",
    "data = zst_to_generator('/share/data/datasets/pile/the-eye.eu/public/AI/pile/train/00.jsonl.zst')\n",
    "buffer = ActivationBuffer(\n",
    "    data,\n",
    "    model,\n",
    "    submodule,\n",
    "    out_feats=512,\n",
    "    in_batch_size=128,\n",
    "    n_ctxs=512,\n",
    ")\n",
    "\n",
    "out = examine_dimension(\n",
    "    model,\n",
    "    submodule,\n",
    "    buffer,\n",
    "    dictionary,\n",
    "    dim_idx=feat_idx,\n",
    "    n_inputs=256\n",
    ")\n",
    "print(out.top_tokens)\n",
    "print(out.top_affected)\n",
    "out.top_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features to ablate: 38\n"
     ]
    }
   ],
   "source": [
    "feats_to_ablate = {\n",
    "    submodules[0] : [\n",
    "        # 4074, # predicts code\n",
    "        # 18775, # certain periods, unclear\n",
    "        18967, # 'He'\n",
    "        22084, # 'he'\n",
    "        29626, # 'his'\n",
    "        # 951, # 'with'\n",
    "        1022, # 'she'\n",
    "        # 1692, # 'or'\n",
    "        2079, # 'Woman' or 'Ladies'\n",
    "        # 2493, # 'nurse(s)'\n",
    "        3122, # 'Her'\n",
    "        # 5648, # 'care' in medical context\n",
    "        # 5950, # 'to'\n",
    "        # 9610, # certain periods, unclear\n",
    "        9651, # female names\n",
    "        10060, # 'She'\n",
    "        # 11778, # 'phone(s)'\n",
    "        # 13675, # certain verbs\n",
    "        # 15032, # unclear\n",
    "        # 23666, # 'and'\n",
    "        # 24418, # unclear,\n",
    "        26504, # 'her'\n",
    "        # 26586, # unclear\n",
    "        # 31201, # 'nursing'\n",
    "    ],\n",
    "    submodules[1] : [\n",
    "        2995, # 'He'\n",
    "        8920, # 'he'\n",
    "        12128, # 'his'\n",
    "        # 12436, # gendered pronouns?\n",
    "        4592, # 'her'\n",
    "        9877, # female names\n",
    "        # 12882, # 'share'\n",
    "        # 14918, # certain periods, unclear\n",
    "        15017, # 'she'\n",
    "        # 17369, # predicts phone numbers\n",
    "        26204, # 'Her'\n",
    "        # 26476, # certain periods, unclear\n",
    "        # 26969, # related to nursing\n",
    "        # 28145, # 'medical' \n",
    "        30248, # female names\n",
    "    ],\n",
    "    submodules[2] : [\n",
    "        4433, # promotes male-associated words\n",
    "        4539, # gendered pronouns\n",
    "        8944, # capitalized gendered pronouns\n",
    "        # 10700, # unclear\n",
    "        11656, # promotes male-associated words\n",
    "        # 14559, # periods in biographies, might promote male words a bit\n",
    "        # 15225, # something about dates?\n",
    "        # 15938, # unclear, might promote male words a bit\n",
    "        # 19014, # certain periods, unclear\n",
    "        # 27803, # unclear\n",
    "        29206, # gendered pronouns\n",
    "        30263, # gendered pronouns\n",
    "        # 277, # spammy advertisements\n",
    "        1995, # promotes female-associated words\n",
    "        # 4770, # unclear\n",
    "        9128, # female pronouns\n",
    "        10635, # female-associated word\n",
    "        # 10757, # unclear\n",
    "        12440, # promotes female-associated words\n",
    "        # 14638, # related to contact information?\n",
    "        # 17774, # related to terms and conditions?\n",
    "        # 21331, # certain periods, unclear\n",
    "        # 26413, # fires on periods in ?blog posts/first-person writing?\n",
    "        # 27838, # promotes parts of urls\n",
    "        29295, # female names\n",
    "        # 29371, # certain periods, unclear\n",
    "        # 31098, # nursing-related words\n",
    "    ],\n",
    "    submodules[3] : [\n",
    "        # 93, # periods in medical contexts\n",
    "        # 2751, # words related to research\n",
    "        # 13474, # active in bios of medical professionals\n",
    "        # 15246, # promotes words about art\n",
    "        # 24661, # periods in bios\n",
    "        27334, # promotes male-associated words\n",
    "        # 27867, # periods in bios, might promote male words a bit\n",
    "        # 7539, # unclear \n",
    "        # 10295, # certain periods\n",
    "        # 13542, # promotes advertising words\n",
    "        # 14401, # unclear\n",
    "        19558, # promotes female-associated words\n",
    "        # 20526, # promotes medical words\n",
    "        22152, # promotes female-associated words\n",
    "        # 23375, # addresses\n",
    "        23545, # 'she'\n",
    "        # 24484, # promotes ?names of diseases?\n",
    "        24806, # 'her'\n",
    "        # 27051, # promotes capitalized words\n",
    "        30802, # 'woman'/'women'\n",
    "        # 31182, # contact information\n",
    "        # 31751, # accommodation words\n",
    "    ],\n",
    "    submodules[4] : [\n",
    "        # 4125, # periods in bios\n",
    "        # 11987, # promotes verbs in bios\n",
    "        # 12332, # promotes words related to academia\n",
    "        # 14658, # periods\n",
    "        30220, # promotes male pronouns\n",
    "        # 1804, # words related to RSVPing\n",
    "        # 4926, # certain periods\n",
    "        # 5731, # promotes medicinal words\n",
    "        # 6869, # promotes verbs about phone calls\n",
    "        9766, # promotes female-associated words\n",
    "        12420, # promotes female pronouns\n",
    "        # 20708, # contact info\n",
    "        # 21979, # promotes capitalized words\n",
    "        23207, # promotes gendered words, especial female\n",
    "        # 30612, # fundraising\n",
    "        # 31282, # capitalized words about contacting\n",
    "    ]      \n",
    "}\n",
    "\n",
    "print(f\"Number of features to ablate: {sum(len(v) for v in feats_to_ablate.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acts_abl(\n",
    "    text,\n",
    "    model,\n",
    "    submodules,\n",
    "    dictionaries,\n",
    "    to_ablate\n",
    "):\n",
    "    with model.trace(\"test\"), t.no_grad():\n",
    "        is_tuple = {}\n",
    "        for submodule in submodules:\n",
    "            is_tuple[submodule] = type(submodule.output.shape) == tuple\n",
    "\n",
    "    with model.trace(text, **tracer_kwargs), t.no_grad():\n",
    "        for submodule in submodules:\n",
    "            dictionary = dictionaries[submodule]\n",
    "            feat_idxs = to_ablate[submodule]\n",
    "            x = submodule.output\n",
    "            if is_tuple[submodule]:\n",
    "                x = x[0]\n",
    "            x_hat, f = dictionary(x, output_features=True)\n",
    "            res = x - x_hat\n",
    "            for idx in feat_idxs:\n",
    "                f[..., idx] = 0.\n",
    "            if is_tuple[submodule]:\n",
    "                submodule.output[0][:] = dictionary.decode(f) + res\n",
    "            else:\n",
    "                submodule.output = dictionary.decode(f) + res\n",
    "        out = model.gpt_neox.layers[layer].output[0][:,-1,:].save()\n",
    "    return out.value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy after ablating features deemed irrelevant by human annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous test accuracy: 0.8661710023880005\n",
      "Ground truth accuracy: 0.8254608511924744\n",
      "Spurious accuracy: 0.5547235012054443\n"
     ]
    }
   ],
   "source": [
    "print('Ambiguous test accuracy:', test_probe(probe, lambda text : get_acts_abl(text, model, submodules, dictionaries, feats_to_ablate), label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(probe, lambda text : get_acts_abl(text, model, submodules, dictionaries, feats_to_ablate), batches=batches, label_idx=0))\n",
    "print('Spurious accuracy:', test_probe(probe, lambda text : get_acts_abl(text, model, submodules, dictionaries, feats_to_ablate), batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.9879666566848755\n",
      "Accuracy for (0, 1): 0.9644010066986084\n",
      "Accuracy for (1, 0): 0.559907853603363\n",
      "Accuracy for (1, 1): 0.7453531622886658\n"
     ]
    }
   ],
   "source": [
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(probe, lambda text : get_acts_abl(text, model, submodules, dictionaries, feats_to_ablate), batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get skyline neuron performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:11<00:00,  2.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# get neurons which are most influential for giving gender label\n",
    "neuron_dicts = {\n",
    "    submodule : IdentityDict(activation_dim).to(DEVICE) for submodule in submodules\n",
    "}\n",
    "\n",
    "n_batches = 25\n",
    "batch_size = 4\n",
    "\n",
    "running_total = 0\n",
    "running_nodes = None\n",
    "\n",
    "for batch_idx, (clean, _, labels) in tqdm(enumerate(get_data(train=True, ambiguous=False, batch_size=batch_size, seed=SEED)), total=n_batches):\n",
    "    if batch_idx == n_batches:\n",
    "        break\n",
    "\n",
    "    effects, _, _, _ = patching_effect(\n",
    "        clean,\n",
    "        None,\n",
    "        model,\n",
    "        submodules,\n",
    "        neuron_dicts,\n",
    "        metric_fn,\n",
    "        metric_kwargs=dict(labels=labels),\n",
    "        method='ig'\n",
    "    )\n",
    "    with t.no_grad():\n",
    "        if running_nodes is None:\n",
    "            running_nodes = {k : len(clean) * v.sum(dim=1).mean(dim=0) for k, v in effects.items()}\n",
    "        else:\n",
    "            for k, v in effects.items():\n",
    "                running_nodes[k] += len(clean) * v.sum(dim=1).mean(dim=0)\n",
    "        running_total += len(clean)\n",
    "\n",
    "nodes = {k : v / running_total for k, v in running_nodes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0:\n",
      "111 0.552754282951355\n",
      "156 0.9466100335121155\n",
      "165 -0.33742231130599976\n",
      "410 0.5108632445335388\n",
      "Layer 1:\n",
      "14 0.3330404460430145\n",
      "23 -1.9429857730865479\n",
      "56 0.534831166267395\n",
      "111 1.3249824047088623\n",
      "148 -0.4178657829761505\n",
      "156 -1.1101523637771606\n",
      "248 -0.3936334252357483\n",
      "258 0.5677260756492615\n",
      "369 0.5060036182403564\n",
      "376 0.7261002063751221\n",
      "410 0.5521716475486755\n",
      "478 0.4259152114391327\n",
      "503 0.41798722743988037\n",
      "Layer 2:\n",
      "17 -0.643139123916626\n",
      "23 0.5534395575523376\n",
      "47 0.8207612633705139\n",
      "63 0.31175464391708374\n",
      "111 -1.3454006910324097\n",
      "129 -0.4927549958229065\n",
      "136 0.37165170907974243\n",
      "137 -0.4441964328289032\n",
      "156 -4.553019046783447\n",
      "186 0.38707372546195984\n",
      "258 0.38416942954063416\n",
      "271 -0.9130510687828064\n",
      "365 0.3123455047607422\n",
      "369 0.6829093098640442\n",
      "390 0.31727081537246704\n",
      "416 0.44325557351112366\n",
      "478 0.3030056357383728\n",
      "Layer 3:\n",
      "111 -0.5690432190895081\n",
      "136 -0.40042388439178467\n",
      "156 -4.656957149505615\n",
      "172 -0.49626481533050537\n",
      "271 -0.4498133063316345\n",
      "376 0.3341180980205536\n",
      "478 0.36204877495765686\n",
      "Layer 4:\n",
      "total neurons: 41\n"
     ]
    }
   ],
   "source": [
    "neurons_to_ablate = {}\n",
    "for i, effect in zip(range(layer+1), nodes.values()):\n",
    "    neurons_to_ablate[submodules[i]] = []\n",
    "    print(f\"Layer {i}:\")\n",
    "    for idx in (effect.act.abs() > 0.3).nonzero():\n",
    "        neurons_to_ablate[submodules[i]].append(idx.item())\n",
    "        print(idx.item(), effect.act[idx].item())\n",
    "print(f\"total neurons: {sum(len(v) for v in neurons_to_ablate.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous test accuracy: 0.8055297136306763\n",
      "Ground truth accuracy: 0.5673962831497192\n",
      "Spurious accuracy: 0.7344470024108887\n"
     ]
    }
   ],
   "source": [
    "print('Ambiguous test accuracy:', test_probe(probe, lambda text : get_acts_abl(text, model, submodules, neuron_dicts, neurons_to_ablate), label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(probe, lambda text : get_acts_abl(text, model, submodules, neuron_dicts, neurons_to_ablate), batches=batches, label_idx=0))\n",
    "print('Spurious accuracy:', test_probe(probe, lambda text : get_acts_abl(text, model, submodules, neuron_dicts, neurons_to_ablate), batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.8897870779037476\n",
      "Accuracy for (0, 1): 0.4113405644893646\n",
      "Accuracy for (1, 0): 0.2142857164144516\n",
      "Accuracy for (1, 1): 0.7265334725379944\n"
     ]
    }
   ],
   "source": [
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(probe, lambda text : get_acts_abl(text, model, submodules, neuron_dicts, neurons_to_ablate), batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get skyline feature performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:16<00:00,  1.48it/s]\n"
     ]
    }
   ],
   "source": [
    "n_batches = 25\n",
    "batch_size = 4\n",
    "\n",
    "running_total = 0\n",
    "running_nodes = None\n",
    "\n",
    "for batch_idx, (clean, _, labels) in tqdm(enumerate(get_data(train=True, ambiguous=False, batch_size=batch_size, seed=SEED)), total=n_batches):\n",
    "    if batch_idx == n_batches:\n",
    "        break\n",
    "\n",
    "    effects, _, _, _ = patching_effect(\n",
    "        clean,\n",
    "        None,\n",
    "        model,\n",
    "        submodules,\n",
    "        dictionaries,\n",
    "        metric_fn,\n",
    "        metric_kwargs=dict(labels=labels),\n",
    "        method='ig'\n",
    "    )\n",
    "    with t.no_grad():\n",
    "        if running_nodes is None:\n",
    "            running_nodes = {k : len(clean) * v.sum(dim=1).mean(dim=0) for k, v in effects.items()}\n",
    "        else:\n",
    "            for k, v in effects.items():\n",
    "                running_nodes[k] += len(clean) * v.sum(dim=1).mean(dim=0)\n",
    "        running_total += len(clean)\n",
    "\n",
    "nodes = {k : v / running_total for k, v in running_nodes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0:\n",
      "1022 0.43988922238349915\n",
      "3122 0.259931743144989\n",
      "4074 -0.24184541404247284\n",
      "9610 0.19874531030654907\n",
      "9651 0.7201375961303711\n",
      "10060 1.4157994985580444\n",
      "18967 0.40968555212020874\n",
      "22084 0.2687976062297821\n",
      "23255 0.18106205761432648\n",
      "23898 0.18152987957000732\n",
      "24435 0.16508552432060242\n",
      "26504 0.18438325822353363\n",
      "29626 0.24421903491020203\n",
      "Layer 1:\n",
      "2995 -0.1729012280702591\n",
      "4592 0.25175902247428894\n",
      "8920 0.508174479007721\n",
      "9877 0.2943287491798401\n",
      "12128 0.4129084646701813\n",
      "14918 0.17536771297454834\n",
      "15017 2.1895370483398438\n",
      "18585 0.23559540510177612\n",
      "26204 0.15615183115005493\n",
      "30248 0.41855838894844055\n",
      "Layer 2:\n",
      "1995 0.7555692195892334\n",
      "9128 1.641681432723999\n",
      "14559 0.29228782653808594\n",
      "21331 0.19326896965503693\n",
      "29295 0.32329061627388\n",
      "29371 0.20549967885017395\n",
      "Layer 3:\n",
      "19558 1.900686264038086\n",
      "23545 0.6123898029327393\n",
      "24806 0.19349133968353271\n",
      "27334 0.24706138670444489\n",
      "27867 0.18328824639320374\n",
      "Layer 4:\n",
      "4125 0.19962818920612335\n",
      "9766 0.21931304037570953\n",
      "12420 2.674450397491455\n",
      "30220 0.9574550986289978\n",
      "total features: 38\n"
     ]
    }
   ],
   "source": [
    "top_feats_to_ablate = {}\n",
    "for i, effect in zip(range(layer+1), nodes.values()):\n",
    "    top_feats_to_ablate[submodules[i]] = []\n",
    "    print(f\"Layer {i}:\")\n",
    "    for idx in (effect.act.abs() > 0.15).nonzero():\n",
    "        top_feats_to_ablate[submodules[i]].append(idx.item())\n",
    "        print(idx.item(), effect.act[idx].item())\n",
    "print(f\"total features: {sum(len(v) for v in top_feats_to_ablate.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous test accuracy: 0.8877788186073303\n",
      "Ground truth accuracy: 0.8381336331367493\n",
      "Spurious accuracy: 0.5604838728904724\n"
     ]
    }
   ],
   "source": [
    "print('Ambiguous test accuracy:', test_probe(probe, lambda text : get_acts_abl(text, model, submodules, dictionaries, top_feats_to_ablate), label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(probe, lambda text : get_acts_abl(text, model, submodules, dictionaries, top_feats_to_ablate), batches=batches, label_idx=0))\n",
    "print('Spurious accuracy:', test_probe(probe, lambda text : get_acts_abl(text, model, submodules, dictionaries, top_feats_to_ablate), batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.9879666566848755\n",
      "Accuracy for (0, 1): 0.9487795233726501\n",
      "Accuracy for (1, 0): 0.6013824939727783\n",
      "Accuracy for (1, 1): 0.7899628281593323\n"
     ]
    }
   ],
   "source": [
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(probe, lambda text : get_acts_abl(text, model, submodules, dictionaries, top_feats_to_ablate), batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retraining probe on activations after ablating features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ReferenceError",
     "evalue": "weakly-referenced object no longer exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mReferenceError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpt_neox\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dictionary-circuits/.conda/lib/python3.11/site-packages/nnsight/envoy.py:239\u001b[0m, in \u001b[0;36mEnvoy.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    236\u001b[0m child_lines \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attribute_name, attribute \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(attribute, Envoy):\n\u001b[1;32m    241\u001b[0m         mod_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrepr\u001b[39m(attribute)\n\u001b[1;32m    242\u001b[0m         mod_str \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39m_addindent(mod_str, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mReferenceError\u001b[0m: weakly-referenced object no longer exists"
     ]
    }
   ],
   "source": [
    "print(model.gpt_neox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [27136, 32768]], which is output 0 of AsStridedBackward0, is at version 4; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m new_probe, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_probe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_acts_abl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubmodules\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdictionaries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeats_to_ablate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAmbiguous test accuracy:\u001b[39m\u001b[38;5;124m'\u001b[39m, test_probe(new_probe, \u001b[38;5;28;01mlambda\u001b[39;00m text: get_acts_abl(text, model, submodules, dictionaries, feats_to_ablate), label_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m      3\u001b[0m batches \u001b[38;5;241m=\u001b[39m get_data(train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, ambiguous\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[3], line 29\u001b[0m, in \u001b[0;36mtrain_probe\u001b[0;34m(get_acts, label_idx, batches, lr, epochs, seed)\u001b[0m\n\u001b[1;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, labels\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     31\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/dictionary-circuits/.conda/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dictionary-circuits/.conda/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [27136, 32768]], which is output 0 of AsStridedBackward0, is at version 4; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "new_probe, _ = train_probe(lambda text : get_acts_abl(text, model, submodules, dictionaries, feats_to_ablate), label_idx=0, lr=lr, epochs=epochs)\n",
    "print('Ambiguous test accuracy:', test_probe(new_probe, lambda text: get_acts_abl(text, model, submodules, dictionaries, feats_to_ablate), label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(new_probe, lambda text : get_acts_abl(text, model, submodules, dictionaries, feats_to_ablate), batches=batches, label_idx=0))\n",
    "print('Spurious accuracy:', test_probe(new_probe, lambda text : get_acts_abl(text, model, submodules, dictionaries, feats_to_ablate), batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.9301449656486511\n",
      "Accuracy for (0, 1): 0.8476905226707458\n",
      "Accuracy for (1, 0): 0.8709677457809448\n",
      "Accuracy for (1, 1): 0.9442378878593445\n"
     ]
    }
   ],
   "source": [
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(new_probe, get_acts_abl, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:16<00:00,  1.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# get neurons which are most influential for giving gender label\n",
    "neuron_dicts = {\n",
    "    submodule : IdentityDict(activation_dim).to(DEVICE) for submodule in submodules\n",
    "}\n",
    "\n",
    "n_batches = 25\n",
    "batch_size = 4\n",
    "\n",
    "running_total = 0\n",
    "running_nodes = None\n",
    "\n",
    "for batch_idx, (clean, _, labels) in tqdm(enumerate(get_data(train=True, ambiguous=False, batch_size=batch_size, seed=SEED)), total=n_batches):\n",
    "    if batch_idx == n_batches:\n",
    "        break\n",
    "\n",
    "    effects, _, _, _ = patching_effect(\n",
    "        clean,\n",
    "        None,\n",
    "        model,\n",
    "        submodules,\n",
    "        neuron_dicts,\n",
    "        metric_fn,\n",
    "        metric_kwargs=dict(labels=labels),\n",
    "        method='ig'\n",
    "    )\n",
    "    with t.no_grad():\n",
    "        if running_nodes is None:\n",
    "            running_nodes = {k : len(clean) * v.sum(dim=1).mean(dim=0) for k, v in effects.items()}\n",
    "        else:\n",
    "            for k, v in effects.items():\n",
    "                running_nodes[k] += len(clean) * v.sum(dim=1).mean(dim=0)\n",
    "        running_total += len(clean)\n",
    "\n",
    "nodes = {k : v / running_total for k, v in running_nodes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0:\n",
      "1022 0.8367617726325989\n",
      "3122 0.5025932192802429\n",
      "4074 -0.41542863845825195\n",
      "9610 0.3709331750869751\n",
      "9651 1.3308827877044678\n",
      "10060 2.6564531326293945\n",
      "18967 0.6820940375328064\n",
      "22084 0.4557006359100342\n",
      "23255 0.3255564570426941\n",
      "24435 0.30812686681747437\n",
      "26504 0.3546774685382843\n",
      "29626 0.4150571823120117\n",
      "Layer 1:\n",
      "2995 -0.3428948223590851\n",
      "4592 0.4850669801235199\n",
      "8920 0.8407686352729797\n",
      "9877 0.5462824702262878\n",
      "12128 0.6813495755195618\n",
      "14918 0.31241682171821594\n",
      "15017 4.105208396911621\n",
      "18585 0.4312697947025299\n",
      "26204 0.30695322155952454\n",
      "30248 0.7768656611442566\n",
      "Layer 2:\n",
      "1995 1.3887615203857422\n",
      "9128 3.060696840286255\n",
      "14559 0.5412063598632812\n",
      "21331 0.3440110981464386\n",
      "29295 0.6148474812507629\n",
      "29371 0.3843125104904175\n",
      "Layer 3:\n",
      "19558 3.5814743041992188\n",
      "23545 1.1425334215164185\n",
      "24806 0.3723568022251129\n",
      "27334 0.40034037828445435\n",
      "27867 0.31886884570121765\n",
      "Layer 4:\n",
      "4125 0.3233107030391693\n",
      "9766 0.3931291997432709\n",
      "12420 5.099288463592529\n",
      "30220 1.5810459852218628\n",
      "total features: 37\n"
     ]
    }
   ],
   "source": [
    "n_features = 0\n",
    "to_ablate = {}\n",
    "for i, effect in zip(range(layer+1), nodes.values()):\n",
    "    to_ablate[submodules[i]] = []\n",
    "    print(f\"Layer {i}:\")\n",
    "    for idx in (effect.act.abs() > 0.3).nonzero():\n",
    "        to_ablate[submodules[i]].append(idx.item())\n",
    "        print(idx.item(), effect.act[idx].item())\n",
    "        n_features += 1\n",
    "print(f\"total features: {n_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuit = {}\n",
    "for submod in submodules:\n",
    "    submod_circuit = SparseAct(act=t.zeros(32768, dtype=t.bool), resc=t.zeros(1, dtype=t.bool)).to(DEVICE)\n",
    "    for idx in to_ablate[submod]:\n",
    "        submod_circuit.act[idx] = True\n",
    "    circuit[submod] = submod_circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acts_abl(text):\n",
    "    with model.trace(text), t.no_grad():\n",
    "        for submodule in submodules:\n",
    "            dictionary = dictionaries[submodule] #neuron_dicts[submodule]\n",
    "            submod_nodes = nodes[submodule].clone()\n",
    "            x = submodule.output\n",
    "            is_tuple = type(x.shape) == tuple\n",
    "            if is_tuple:\n",
    "                x = x[0]\n",
    "            f = dictionary.encode(x)\n",
    "            res = x - dictionary(x)\n",
    "\n",
    "            # ablate features\n",
    "            f[...,circuit[submodule].act] = 0.\n",
    "            \n",
    "            if is_tuple:\n",
    "                submodule.output[0][:] = dictionary.decode(f) + res\n",
    "            else:\n",
    "                submodule.output = dictionary.decode(f) + res\n",
    "\n",
    "        out = model.gpt_neox.layers[layer].output[0][:,-1,:].save()\n",
    "    return out.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous test accuracy: 0.9207713603973389\n",
      "Ground truth accuracy: 0.8577188849449158\n",
      "Spurious accuracy: 0.570852518081665\n"
     ]
    }
   ],
   "source": [
    "print('Ambiguous test accuracy:', test_probe(probe, get_acts_abl, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(probe, get_acts_abl, batches=batches, label_idx=0))\n",
    "print('Spurious accuracy:', test_probe(probe, get_acts_abl, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on ambiguous data: 0.4993029832839966\n",
      "Ground truth accuracy 0.4976958632469177\n",
      "Spurious accuracy 0.5011520981788635\n"
     ]
    }
   ],
   "source": [
    "def out_fn(model):\n",
    "    return probe(model.gpt_neox.layers[layer].output[0][:,-1,:])\n",
    "\n",
    "# get accuracy after ablating above features\n",
    "batches = get_data(train=False, ambiguous=True, batch_size=128, seed=SEED)\n",
    "\n",
    "with t.no_grad():\n",
    "    corrects = []\n",
    "    for batch in batches:\n",
    "        text = batch[0]\n",
    "        labels = batch[1]\n",
    "        probs = run_with_ablations(\n",
    "            model,\n",
    "            text,\n",
    "            submodules,\n",
    "            dictionaries,\n",
    "            to_ablate,\n",
    "            out_fn\n",
    "        )\n",
    "        preds = (probs > 0.5).long()\n",
    "        corrects.append((preds == labels).float())\n",
    "    print(\"Accuracy on ambiguous data:\", t.cat(corrects).mean().item())\n",
    "\n",
    "batches = get_data(train=False, ambiguous=False, batch_size=128, seed=SEED)\n",
    "\n",
    "with t.no_grad():\n",
    "    truth_corrects, spurious_corrects = [], []\n",
    "    for batch in batches:\n",
    "        text = batch[0]\n",
    "        true_labels = batch[1]\n",
    "        spurious_labels = batch[2]\n",
    "        probs = run_with_ablations(\n",
    "            model,\n",
    "            text,\n",
    "            submodules,\n",
    "            dictionaries,\n",
    "            to_ablate,\n",
    "            out_fn\n",
    "        )\n",
    "        preds = (probs > 0.5).long()\n",
    "        truth_corrects.append((preds == true_labels).float())\n",
    "        spurious_corrects.append((preds == spurious_labels).float())\n",
    "    print(\"Ground truth accuracy\", t.cat(truth_corrects).mean().item())\n",
    "    print(\"Spurious accuracy\", t.cat(spurious_corrects).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrain probe with ablated model\n",
    "\n",
    "def out_fn(model):\n",
    "    return model.gpt_neox.layers[layer].output[0][:,-1,:]\n",
    "\n",
    "t.manual_seed(SEED)\n",
    "new_probe = Probe(512).to('cuda:0')\n",
    "optimizer = t.optim.AdamW(new_probe.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "batches = get_data(train=True, ambiguous=True, batch_size=64, seed=SEED)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    for batch in batches:\n",
    "        text = batch[0]\n",
    "        labels = batch[1]\n",
    "        acts = run_with_ablations(\n",
    "            model,\n",
    "            text,\n",
    "            submodules,\n",
    "            dictionaries,\n",
    "            to_ablate,\n",
    "            out_fn\n",
    "        ).clone()\n",
    "        probs = new_probe(acts)\n",
    "        loss = criterion(probs, labels.float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
