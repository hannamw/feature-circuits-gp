{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/u/smarks/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Matplotlib created a temporary cache directory at /tmp/matplotlib-a87gknl9 because the default path (/share/u/smarks/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from nnsight import LanguageModel\n",
    "import torch as t\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from attribution import patching_effect\n",
    "from dictionary_learning import AutoEncoder, ActivationBuffer\n",
    "from dictionary_learning.interp import examine_dimension\n",
    "from dictionary_learning.utils import zst_to_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"LabHC/bias_in_bios\")\n",
    "profession_dict = {'professor' : 21, 'nurse' : 13}\n",
    "male_prof = 'professor'\n",
    "female_prof = 'nurse'\n",
    "\n",
    "DEVICE = 'cuda:0'\n",
    "model = LanguageModel('EleutherAI/pythia-70m-deduped', device_map=DEVICE)\n",
    "layer = 4\n",
    "\n",
    "batch_size = 1024\n",
    "SEED = 42\n",
    "\n",
    "def get_data(train=True, ambiguous=True, batch_size=128, seed=SEED):\n",
    "    if train:\n",
    "        data = dataset['train']\n",
    "    else:\n",
    "        data = dataset['test']\n",
    "    if ambiguous:\n",
    "        neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        n = min([len(neg), len(pos)])\n",
    "        neg, pos = neg[:n], pos[:n]\n",
    "        data = neg + pos\n",
    "        labels = [0]*n + [1]*n\n",
    "        idxs = list(range(2*n))\n",
    "        random.Random(seed).shuffle(idxs)\n",
    "        data, labels = [data[i] for i in idxs], [labels[i] for i in idxs]\n",
    "        true_labels = spurious_labels = labels\n",
    "    else:\n",
    "        neg_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        neg_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 1]\n",
    "        pos_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 0]\n",
    "        pos_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        n = min([len(neg_neg), len(neg_pos), len(pos_neg), len(pos_pos)])\n",
    "        neg_neg, neg_pos, pos_neg, pos_pos = neg_neg[:n], neg_pos[:n], pos_neg[:n], pos_pos[:n]\n",
    "        data = neg_neg + neg_pos + pos_neg + pos_pos\n",
    "        true_labels     = [0]*n + [0]*n + [1]*n + [1]*n\n",
    "        spurious_labels = [0]*n + [1]*n + [0]*n + [1]*n\n",
    "        idxs = list(range(4*n))\n",
    "        random.Random(seed).shuffle(idxs)\n",
    "        data, true_labels, spurious_labels = [data[i] for i in idxs], [true_labels[i] for i in idxs], [spurious_labels[i] for i in idxs]\n",
    "\n",
    "    batches = [\n",
    "        (data[i:i+batch_size], t.tensor(true_labels[i:i+batch_size], device=DEVICE), t.tensor(spurious_labels[i:i+batch_size], device=DEVICE)) for i in range(0, len(data), batch_size)\n",
    "    ]\n",
    "\n",
    "    return batches\n",
    "\n",
    "def get_subgroups(train=True, ambiguous=True, batch_size=128, seed=SEED):\n",
    "    if train:\n",
    "        data = dataset['train']\n",
    "    else:\n",
    "        data = dataset['test']\n",
    "    if ambiguous:\n",
    "        neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        neg_labels, pos_labels = (0, 0), (1, 1)\n",
    "        subgroups = [(neg, neg_labels), (pos, pos_labels)]\n",
    "    else:\n",
    "        neg_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        neg_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 1]\n",
    "        pos_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 0]\n",
    "        pos_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        neg_neg_labels, neg_pos_labels, pos_neg_labels, pos_pos_labels = (0, 0), (0, 1), (1, 0), (1, 1)\n",
    "        subgroups = [(neg_neg, neg_neg_labels), (neg_pos, neg_pos_labels), (pos_neg, pos_neg_labels), (pos_pos, pos_pos_labels)]\n",
    "    \n",
    "    out = {}\n",
    "    for data, label_profile in subgroups:\n",
    "        out[label_profile] = []\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            text = data[i:i+batch_size]\n",
    "            out[label_profile].append(\n",
    "                (\n",
    "                    text,\n",
    "                    t.tensor([label_profile[0]]*len(text), device=DEVICE),\n",
    "                    t.tensor([label_profile[1]]*len(text), device=DEVICE)\n",
    "                )\n",
    "            )\n",
    "    return out\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "# def get_batched_data(model, layer, data, true_labels, spurious_labels, batch_size=128):\n",
    "#     batches = []\n",
    "\n",
    "#     for i in range(0, len(data), batch_size):\n",
    "#         text_batch = data[i:i+batch_size]\n",
    "#         with model.invoke(text_batch):\n",
    "#             acts = model.gpt_neox.layers[layer].output[0][:,-1,:].save()\n",
    "#         acts = acts.value.clone()\n",
    "#         true_labels_batch = t.tensor(true_labels[i:i+batch_size]).to(acts.device)\n",
    "#         spurious_labels_batch = t.tensor(spurious_labels[i:i+batch_size]).to(acts.device)\n",
    "#         batches.append((acts, true_labels_batch, spurious_labels_batch))\n",
    "\n",
    "#     return batches\n",
    "\n",
    "# batches = {\n",
    "#     'train' : {\n",
    "#         'ambiguous' : get_batched_data(model, layer, *get_data(train=True, ambiguous=True)), \n",
    "#         'unambiguous' : get_batched_data(model, layer, *get_data(train=True, ambiguous=False))\n",
    "#     },\n",
    "#     'test' : {\n",
    "#         'ambiguous' : get_batched_data(model, layer, *get_data(train=False, ambiguous=True)), \n",
    "#         'unambiguous' : get_batched_data(model, layer, *get_data(train=False, ambiguous=False))\n",
    "#     }\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probe training hyperparameters\n",
    "lr = 1e-2\n",
    "epochs = 1\n",
    "\n",
    "class Probe(nn.Module):\n",
    "    def __init__(self, activation_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(activation_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x).squeeze(-1)\n",
    "        return logits.sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_probe(get_acts, label_idx=0, batches=get_data(), lr=1e-2, epochs=1, seed=SEED):\n",
    "    t.manual_seed(seed)\n",
    "    probe = Probe(512).to('cuda:0')\n",
    "    optimizer = t.optim.AdamW(probe.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for batch in batches:\n",
    "            text = batch[0]\n",
    "            labels = batch[label_idx+1] \n",
    "            acts = get_acts(text)\n",
    "            logits = probe(acts)\n",
    "            loss = criterion(logits, labels.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return probe, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_probe(probe, get_acts, label_idx=0, batches=get_data(train=False), seed=SEED):\n",
    "    with t.no_grad():\n",
    "        corrects = []\n",
    "\n",
    "        for batch in batches:\n",
    "            text = batch[0]\n",
    "            labels = batch[label_idx+1]\n",
    "            acts = get_acts(text)\n",
    "            logits = probe(acts)\n",
    "            preds = (logits > 0.5).long()\n",
    "            corrects.append((preds == labels).float())\n",
    "        return t.cat(corrects).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acts(text):\n",
    "    with model.invoke(text):\n",
    "        acts = model.gpt_neox.layers[layer].output[0][:,-1,:].save()\n",
    "    return acts.value.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe0, _ = train_probe(get_acts, label_idx=0, batches=get_data(ambiguous=False), lr=lr, epochs=epochs)\n",
    "probe1, _ = train_probe(get_acts, label_idx=1, batches=get_data(ambiguous=False), lr=lr, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probe 0 accuracy: 0.900921642780304\n",
      "Probe 1 accuracy: 0.9688940048217773\n"
     ]
    }
   ],
   "source": [
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Probe 0 accuracy:', test_probe(probe0, get_acts, batches=batches, label_idx=0))\n",
    "print('Probe 1 accuracy:', test_probe(probe1, get_acts, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous test accuracy: 0.9915195107460022\n",
      "Ground truth accuracy: 0.6261520981788635\n",
      "Spurious accuracy: 0.8680875301361084\n"
     ]
    }
   ],
   "source": [
    "probe, _ = train_probe(get_acts, label_idx=0, lr=lr, epochs=epochs)\n",
    "print('Ambiguous test accuracy:', test_probe(probe, get_acts, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(probe, get_acts, batches=batches, label_idx=0))\n",
    "print('Spurious accuracy:', test_probe(probe, get_acts, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.9941375851631165\n",
      "Accuracy for (0, 1): 0.13586179912090302\n",
      "Accuracy for (1, 0): 0.38479262590408325\n",
      "Accuracy for (1, 1): 0.9881505370140076\n"
     ]
    }
   ],
   "source": [
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(probe, get_acts, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "submodules = [\n",
    "    model.gpt_neox.layers[i] for i in range(layer + 1)\n",
    "]\n",
    "dictionaries = {}\n",
    "for i in range(layer + 1):\n",
    "    ae = AutoEncoder(512, 64 * 512).to(DEVICE)\n",
    "    ae.load_state_dict(t.load(f'/share/projects/dictionary_circuits/autoencoders/pythia-70m-deduped/resid_out_layer{i}/5_32768/ae.pt'))\n",
    "    dictionaries[submodules[i]] = ae\n",
    "\n",
    "def metric_fn(model):\n",
    "    return probe(model.gpt_neox.layers[layer].output[0][:,-1,:])\n",
    "\n",
    "neg_inputs, pos_inputs = [], []\n",
    "for x in dataset['train']:\n",
    "    if x['profession'] == profession_dict[male_prof] and x['gender'] == 0 and len(neg_inputs) < 16:\n",
    "        neg_inputs.append(x['hard_text'])\n",
    "    if x['profession'] == profession_dict[female_prof] and x['gender'] == 1 and len(pos_inputs) < 16:\n",
    "        pos_inputs.append(x['hard_text'])\n",
    "\n",
    "neg_effects = patching_effect(\n",
    "    neg_inputs,\n",
    "    None,\n",
    "    model,\n",
    "    submodules,\n",
    "    dictionaries,\n",
    "    metric_fn,\n",
    "    method='ig'\n",
    ").effects\n",
    "neg_effects = {k : v.sum(dim=1).mean(dim=0) for k, v in neg_effects.items()}\n",
    "\n",
    "pos_effects = patching_effect(\n",
    "    pos_inputs,\n",
    "    None,\n",
    "    model,\n",
    "    submodules,\n",
    "    dictionaries,\n",
    "    metric_fn,\n",
    "    method='ig'\n",
    ").effects\n",
    "\n",
    "pos_effects = {k : v.sum(dim=1).mean(dim=0) for k, v in pos_effects.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0:\n",
      "    Multindex: (10313,), Value: 0.04134603962302208\n",
      "    Multindex: (17329,), Value: 0.009056453593075275\n",
      "    Multindex: (23553,), Value: 0.0062515512108802795\n",
      "    Multindex: (25794,), Value: 0.010537972673773766\n",
      "Layer 1:\n",
      "    Multindex: (11727,), Value: 0.014927267096936703\n",
      "    Multindex: (12955,), Value: 0.006697795353829861\n",
      "    Multindex: (16731,), Value: 0.007774203550070524\n",
      "    Multindex: (18680,), Value: 0.006383746396750212\n",
      "Layer 2:\n",
      "    Multindex: (2859,), Value: 0.00724233640357852\n",
      "    Multindex: (5547,), Value: 0.014884809032082558\n",
      "    Multindex: (12633,), Value: 0.00643932493403554\n",
      "    Multindex: (17324,), Value: 0.009294000454246998\n",
      "Layer 3:\n",
      "    Multindex: (10539,), Value: 0.006092030089348555\n",
      "    Multindex: (16734,), Value: 0.006312336772680283\n",
      "    Multindex: (26887,), Value: 0.00739505747333169\n",
      "    Multindex: (29985,), Value: 0.006092030089348555\n",
      "Layer 4:\n",
      "    Multindex: (8126,), Value: 0.016388770192861557\n",
      "    Multindex: (23954,), Value: 0.008023547008633614\n",
      "    Multindex: (30226,), Value: 0.005598787218332291\n",
      "Layer 0:\n",
      "    Multindex: (2910,), Value: -0.09522536396980286\n",
      "    Multindex: (7187,), Value: -0.005045532248914242\n",
      "    Multindex: (11379,), Value: -0.010326722636818886\n",
      "    Multindex: (11674,), Value: -0.007547666318714619\n",
      "    Multindex: (11909,), Value: -0.03138626366853714\n",
      "    Multindex: (13051,), Value: -0.01052669808268547\n",
      "    Multindex: (13094,), Value: -0.01380863506346941\n",
      "    Multindex: (15628,), Value: -0.017308617010712624\n",
      "    Multindex: (17078,), Value: -0.005972959566861391\n",
      "    Multindex: (22846,), Value: -0.008506332524120808\n",
      "    Multindex: (29183,), Value: -0.005424296483397484\n",
      "    Multindex: (30927,), Value: -0.0076266201213002205\n",
      "    Multindex: (31251,), Value: -0.00982943456619978\n",
      "    Multindex: (32356,), Value: -0.006398848723620176\n",
      "Layer 1:\n",
      "    Multindex: (2578,), Value: -0.012353976257145405\n",
      "    Multindex: (20964,), Value: -0.010346607305109501\n",
      "    Multindex: (22287,), Value: -0.042218971997499466\n",
      "Layer 2:\n",
      "Layer 3:\n",
      "    Multindex: (3216,), Value: -0.006898955442011356\n",
      "    Multindex: (31219,), Value: -0.005365448538213968\n",
      "Layer 4:\n",
      "    Multindex: (7536,), Value: -0.005250738002359867\n",
      "    Multindex: (25160,), Value: -0.032510463148355484\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.005\n",
    "\n",
    "for i, submodule in enumerate(submodules):\n",
    "    print(f\"Layer {i}:\")\n",
    "    effect = neg_effects[submodule]\n",
    "    for feature_idx in t.nonzero(effect):\n",
    "        value = effect[tuple(feature_idx)]\n",
    "        if value > threshold:\n",
    "            print(f\"    Multindex: {tuple(feature_idx.tolist())}, Value: {value}\")\n",
    "\n",
    "for i, submodule in enumerate(submodules):\n",
    "    print(f\"Layer {i}:\")\n",
    "    effect = pos_effects[submodule]\n",
    "    for feature_idx in t.nonzero(effect):\n",
    "        value = effect[tuple(feature_idx)]\n",
    "        if -value > threshold:\n",
    "            print(f\"    Multindex: {tuple(feature_idx.tolist())}, Value: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' →', 3.498818874359131), (')?', 0.7700076103210449), ('!', 0.6087144613265991), ('%).', 0.6013126373291016), ('.', 0.5607401132583618), (').', 0.19541782140731812), ('.', 0.16909795999526978), ('?', 0.15509594976902008), ('com', 0.13155770301818848), ('…', 0.05967291444540024), ('...', 0.0465102344751358), (' -', 0.02337704598903656), (']', 0.022350026294589043), (' available', 0.02159261703491211), ('References', 0.002975702518597245), (':', 0.0013361757155507803), (',', 0.0005869610467925668), ('It', 0.0), (' is', 0.0), (' done', 0.0), (' and', 0.0), (' submitted', 0.0), (' You', 0.0), (' can', 0.0), (' play', 0.0), (' “', 0.0), ('Sur', 0.0), ('vival', 0.0), (' of', 0.0), (' the', 0.0)]\n",
      "[('UPDATE', 1.3970803022384644), ('#####', 1.3345152139663696), ('fefefe', 1.3260353803634644), ('Docket', 1.3101134300231934), ('EOF', 1.300095558166504), ('txt', 1.2410948276519775), ('<?', 1.2343525886535645), ('\"\"\"', 1.2244322299957275), ('################################', 1.2139382362365723), ('######', 1.2122739553451538), ('ADVERTISEMENT', 1.212050437927246), ('blogspot', 1.2039122581481934), (' Please', 1.2027281522750854), (' Otherwise', 1.1962543725967407), (' Register', 1.193593144416809), ('INSERT', 1.1921528577804565), ('![](', 1.1909116506576538), ('<%', 1.189198613166809), ('TIM', 1.1795634031295776), ('##', 1.1786516904830933), ('msgstr', 1.1760640144348145), (']]>', 1.1738381385803223), (' <!--', 1.1644632816314697), ('<!--', 1.1638243198394775), ('Bye', 1.1629129648208618), ('IRQHandler', 1.1624082326889038), ('HIV', 1.158225417137146), ('IFN', 1.157684087753296), ('****************************************************************************', 1.1564675569534302), ('LANG', 1.1543476581573486)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-390ea45d-231b\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-390ea45d-231b\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [[\"No\", \" other\", \" appliance\", \" company\", \" has\", \" a\", \" wider\", \" scope\", \" of\", \" solutions\", \",\", \" nor\", \" the\", \" experience\", \" to\", \" back\", \" them\", \" up\", \",\", \" than\", \" Elect\", \"rol\", \"ux\", \".\", \" Our\", \" long\", \" presence\", \" in\", \" people\", \"\\u2019\", \"s\", \" homes\", \" around\", \" the\", \" world\", \" means\", \" that\", \" no\", \" other\", \" appliance\", \" company\", \"...\", \"\\n\", \"Read\", \" more\", \"\\n\", \"\\n\", \"...\", \" Easy\", \"-\", \"F\", \"lo\", \" vacu\", \"ums\", \" including\", \" parts\", \" and\", \" bags\", \".\", \" Find\", \"lay\", \"'s\", \" also\", \" offers\", \" sales\", \" and\", \" service\", \" for\", \" all\", \" makes\", \" and\", \" models\", \" of\", \" sewing\", \" machines\", \" and\", \" vacu\", \"ums\", \".\", \" Please\", \" contact\", \" us\", \" for\", \" more\", \" information\", \" about\", \" our\", \" products\", \" and\", \" services\", \".\"], [\"Account\", \"ing\", \"\\n\", \"\\n\", \"Sur\", \"f\", \" Works\", \" offer\", \" a\", \" range\", \" of\", \" accounting\", \" services\", \" suitable\", \" for\", \" all\", \" types\", \" of\", \" business\", \".\", \" Below\", \",\", \" we\", \" have\", \" listed\", \" packages\", \" suitable\", \" for\", \" sole\", \" traders\", \",\", \" partnerships\", \" and\", \" limited\", \" companies\", \".\", \" The\", \" packages\", \" can\", \" be\", \" fully\", \" tailored\", \" to\", \" your\", \" requirements\", \" by\", \" adding\", \" extra\", \" services\", \" to\", \" create\", \" the\", \" exact\", \" service\", \" that\", \" you\", \" and\", \" your\", \" business\", \" requires\", \".\", \"\\n\", \"\\n\", \"All\", \" services\", \" are\", \" carried\", \" out\", \" on\", \" time\", \" with\", \" the\", \" minimum\", \" of\", \" fuss\", \" by\", \" our\", \" in\", \" house\", \",\", \" fully\", \" qualified\", \" accountant\", \"\\n\", \"\\n\", \"The\", \" list\", \" of\", \" services\", \" offered\", \" is\", \" not\", \" exhaustive\", \" so\", \" please\", \" let\", \" us\", \" know\", \" if\", \" you\", \" require\", \" a\", \" service\", \" not\", \" listed\", \".\"], [\"A\", \" small\", \" city\", \" in\", \" Iowa\", \" has\", \" taken\", \" an\", \" action\", \" to\", \" save\", \" the\", \" bees\", \" from\", \" extinction\", \".\", \" Ac\", \"res\", \" of\", \" land\", \" were\", \" donated\", \" to\", \" increase\", \" the\", \" local\", \" habitats\", \" of\", \" the\", \" bees\", \".\", \"\\n\", \"\\n\", \"Over\", \" the\", \" past\", \" decade\", \",\", \" bees\", \" are\", \" steadily\", \" disappearing\", \".\", \" Work\", \"er\", \" bees\", \" disappear\", \" and\", \" leaving\", \" behind\", \" the\", \" queen\", \".\", \" With\", \" a\", \" few\", \" nursing\", \" bees\", \" to\", \" take\", \" care\", \" of\", \" the\", \" immature\", \" bees\", \",\", \" a\", \" col\", \"o\", \"\\u2026\", \" Read\", \" More\", \"\\n\", \"\\n\", \"To\", \" stay\", \" updated\", \" with\", \" the\", \" latest\", \" in\", \" the\", \" ap\", \"icult\", \"ure\", \" industry\", \" to\", \" can\", \" visit\", \" our\", \" bee\", \"keeping\", \" latest\", \" news\", \".\", \" On\", \" the\", \" other\", \" hand\", \" if\", \" you\", \" are\", \" starting\", \" bee\", \"keeping\", \" and\", \" would\", \" like\", \" to\", \" begin\", \" professional\", \" bee\", \"keeping\", \" today\", \" download\", \" a\", \" copy\", \" of\", \" our\", \" bee\", \"keeping\", \" for\", \" beg\", \"inners\", \" e\", \"book\", \".\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"The\", \" date\", \" is\", \" fast\", \" approaching\", \" for\", \" our\", \" spring\", \" rally\", \".\", \" I\", \" have\", \" posted\", \" the\", \" reservation\", \" information\", \" in\", \" the\", \" Calendar\", \" section\", \",\", \" I\", \" will\", \" post\", \" more\", \" details\", \" in\", \" the\", \" calendar\", \" section\", \" as\", \" they\", \" become\", \" available\", \".\", \" If\", \" you\", \" have\", \" any\", \" questions\", \" please\", \" e\", \"-\", \"mail\", \" me\", \" at\", \" tx\", \"je\", \"ff\", \"123\", \"@\", \"gmail\", \".\", \"com\", \".\"], [\"Account\", \"ing\", \"\\n\", \"\\n\", \"Sur\", \"f\", \" Works\", \" offer\", \" a\", \" range\", \" of\", \" accounting\", \" services\", \" suitable\", \" for\", \" all\", \" types\", \" of\", \" business\", \".\", \" Below\", \",\", \" we\", \" have\", \" listed\", \" packages\", \" suitable\", \" for\", \" sole\", \" traders\", \",\", \" partnerships\", \" and\", \" limited\", \" companies\", \".\", \" The\", \" packages\", \" can\", \" be\", \" fully\", \" tailored\", \" to\", \" your\", \" requirements\", \" by\", \" adding\", \" extra\", \" services\", \" to\", \" create\", \" the\", \" exact\", \" service\", \" that\", \" you\", \" and\", \" your\", \" business\", \" requires\", \".\", \"\\n\", \"\\n\", \"All\", \" services\", \" are\", \" carried\", \" out\", \" on\", \" time\", \" with\", \" the\", \" minimum\", \" of\", \" fuss\", \" by\", \" our\", \" in\", \" house\", \",\", \" fully\", \" qualified\", \" accountant\", \"\\n\", \"\\n\", \"The\", \" list\", \" of\", \" services\", \" offered\", \" is\", \" not\", \" exhaustive\", \" so\", \" please\", \" let\", \" us\", \" know\", \" if\", \" you\", \" require\", \" a\", \" service\", \" not\", \" listed\", \".\", \" If\", \" you\", \" have\", \" specific\", \" needs\", \" we\", \" can\", \" build\", \" a\", \" bes\", \"p\", \"oke\", \" account\", \"ancy\", \" package\", \" tailored\", \" to\", \" your\", \" exact\", \" requirements\", \".\"], [\"Education\", \" Week\", \" reporter\", \" Ben\", \" Her\", \"old\", \" explores\", \" how\", \" technology\", \" is\", \" shaping\", \" teaching\", \" and\", \" learning\", \" and\", \" the\", \" management\", \" of\", \" schools\", \".\", \" Join\", \" the\", \" discussion\", \" as\", \" he\", \" analy\", \"zes\", \" the\", \" latest\", \" developments\", \".\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"The\", \" date\", \" is\", \" fast\", \" approaching\", \" for\", \" our\", \" spring\", \" rally\", \".\", \" I\", \" have\", \" posted\", \" the\", \" reservation\", \" information\", \" in\", \" the\", \" Calendar\", \" section\", \",\", \" I\", \" will\", \" post\", \" more\", \" details\", \" in\", \" the\", \" calendar\", \" section\", \" as\", \" they\", \" become\", \" available\", \".\"], [\"Account\", \"ing\", \"\\n\", \"\\n\", \"Sur\", \"f\", \" Works\", \" offer\", \" a\", \" range\", \" of\", \" accounting\", \" services\", \" suitable\", \" for\", \" all\", \" types\", \" of\", \" business\", \".\", \" Below\", \",\", \" we\", \" have\", \" listed\", \" packages\", \" suitable\", \" for\", \" sole\", \" traders\", \",\", \" partnerships\", \" and\", \" limited\", \" companies\", \".\", \" The\", \" packages\", \" can\", \" be\", \" fully\", \" tailored\", \" to\", \" your\", \" requirements\", \" by\", \" adding\", \" extra\", \" services\", \" to\", \" create\", \" the\", \" exact\", \" service\", \" that\", \" you\", \" and\", \" your\", \" business\", \" requires\", \".\"], [\"A\", \" small\", \" city\", \" in\", \" Iowa\", \" has\", \" taken\", \" an\", \" action\", \" to\", \" save\", \" the\", \" bees\", \" from\", \" extinction\", \".\", \" Ac\", \"res\", \" of\", \" land\", \" were\", \" donated\", \" to\", \" increase\", \" the\", \" local\", \" habitats\", \" of\", \" the\", \" bees\", \".\", \"\\n\", \"\\n\", \"Over\", \" the\", \" past\", \" decade\", \",\", \" bees\", \" are\", \" steadily\", \" disappearing\", \".\", \" Work\", \"er\", \" bees\", \" disappear\", \" and\", \" leaving\", \" behind\", \" the\", \" queen\", \".\", \" With\", \" a\", \" few\", \" nursing\", \" bees\", \" to\", \" take\", \" care\", \" of\", \" the\", \" immature\", \" bees\", \",\", \" a\", \" col\", \"o\", \"\\u2026\", \" Read\", \" More\", \"\\n\", \"\\n\", \"To\", \" stay\", \" updated\", \" with\", \" the\", \" latest\", \" in\", \" the\", \" ap\", \"icult\", \"ure\", \" industry\", \" to\", \" can\", \" visit\", \" our\", \" bee\", \"keeping\", \" latest\", \" news\", \".\"], [\"Account\", \"ing\", \"\\n\", \"\\n\", \"Sur\", \"f\", \" Works\", \" offer\", \" a\", \" range\", \" of\", \" accounting\", \" services\", \" suitable\", \" for\", \" all\", \" types\", \" of\", \" business\", \".\", \" Below\", \",\", \" we\", \" have\", \" listed\", \" packages\", \" suitable\", \" for\", \" sole\", \" traders\", \",\", \" partnerships\", \" and\", \" limited\", \" companies\", \".\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"B\", \"are\", \"back\", \" BF\", \" Videos\", \" Pay\", \" Pal\", \"\\n\", \"\\n\", \"Get\", \" your\", \" discount\", \" membership\", \" to\", \" B\", \"are\", \"back\", \" BF\", \" Videos\", \" using\", \" the\", \" image\", \" above\", \"\\u2026\", \"or\", \" try\", \" the\", \" free\", \" B\", \"are\", \"back\", \" BF\", \" Videos\", \" log\", \"ins\", \" below\", \",\", \" and\", \" get\", \" member\", \" access\", \" to\", \" B\", \"are\", \"back\", \"bf\", \"v\", \"ideos\", \" without\", \" paying\", \".\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"B\", \"are\", \"back\", \" BF\", \" Videos\", \" Pay\", \" Pal\", \"\\n\", \"\\n\", \"Get\", \" your\", \" discount\", \" membership\", \" to\", \" B\", \"are\", \"back\", \" BF\", \" Videos\", \" using\", \" the\", \" image\", \" above\", \"\\u2026\", \"or\", \" try\", \" the\", \" free\", \" B\", \"are\", \"back\", \" BF\", \" Videos\", \" log\", \"ins\", \" below\", \",\", \" and\", \" get\", \" member\", \" access\", \" to\", \" B\", \"are\", \"back\", \"bf\", \"v\", \"ideos\", \" without\", \" paying\", \".\", \" Download\", \" tons\", \" of\", \" quality\", \" Un\", \"released\", \" Foot\", \"age\", \" and\", \" Really\", \" High\", \" Resolution\", \" P\", \"ics\", \".\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"B\", \"are\", \"back\", \" BF\", \" Videos\", \" Pay\", \" Pal\", \"\\n\", \"\\n\", \"Get\", \" your\", \" discount\", \" membership\", \" to\", \" B\", \"are\", \"back\", \" BF\", \" Videos\", \" using\", \" the\", \" image\", \" above\", \"\\u2026\", \"or\", \" try\", \" the\", \" free\", \" B\", \"are\", \"back\", \" BF\", \" Videos\", \" log\", \"ins\", \" below\", \",\", \" and\", \" get\", \" member\", \" access\", \" to\", \" B\", \"are\", \"back\", \"bf\", \"v\", \"ideos\", \" without\", \" paying\", \".\", \" Download\", \" tons\", \" of\", \" quality\", \" Un\", \"released\", \" Foot\", \"age\", \" and\", \" Really\", \" High\", \" Resolution\", \" P\", \"ics\", \".\", \" This\", \" bare\", \"back\", \"bf\", \"v\", \"ideos\", \".\", \"com\", \" deal\", \" is\", \" a\", \" limited\", \" offer\", \",\", \" Don\", \"\\u2019\", \"t\", \" miss\", \" out\", \"!\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"Tag\", \":\", \" El\", \"oy\", \" Cas\", \"ados\", \"\\n\", \"\\n\", \"Original\", \" US\", \" release\", \" date\", \":\", \" December\", \" 5\", \",\", \" 2008\", \" Production\", \" budget\", \":\", \" $\", \"25\", \",\", \"000\", \",\", \"000\", \" World\", \"wide\", \" gross\", \":\", \" $\", \"27\", \",\", \"426\", \",\", \"335\", \" There\", \" are\", \" timely\", \" films\", \" and\", \" then\", \" there\", \" are\", \" films\", \" that\", \" are\", \" before\", \" their\", \" time\", \".\", \" Ron\", \" Howard\", \" is\", \" probably\", \" seen\", \" by\", \" most\", \" as\", \" a\", \" director\", \" who\", \" frequently\", \" makes\", \" good\", \" or\", \" very\", \" good\", \" films\", \" and\", \" occasionally\", \" makes\", \" a\", \" great\", \" one\", \".\", \" Most\", \" recently\", \",\", \" a\", \" lot\", \"...\", \" Continue\", \" Reading\", \" \\u2192\"], [\"This\", \" article\", \" is\", \" from\", \" the\", \" archive\", \" of\", \" our\", \" partner\", \".\"], [\"P\", \"urchase\", \" either\", \" a\", \" combined\", \" Build\", \"ings\", \" &\", \" Contents\", \" Home\", \" Insurance\", \" policy\", \",\", \" or\", \" separate\", \" Build\", \"ings\", \" or\", \" Contents\", \" Home\", \" Insurance\", \" Policy\", \" online\", \" at\", \" Little\", \"woods\", \".\", \"com\", \" between\", \" 1\", \"st\", \" and\", \" 31\", \"st\", \" August\", \" 2017\", \" to\", \" qualify\", \" for\", \" a\", \" free\", \" Amazon\", \" Echo\", \" Dot\", \".\", \" New\", \" Little\", \"woods\", \" Home\", \" Insurance\", \" customers\", \" only\", \".\", \"\\n\", \"\\n\", \"Prov\", \"ided\", \" your\", \" policy\", \" is\", \" still\", \" active\", \" and\", \" your\", \" premiums\", \" are\", \" up\", \" to\", \" date\", \",\", \" we\", \"'ll\", \" email\", \" you\", \" 4\", \" weeks\", \" post\", \"-\", \"p\", \"urchase\", \" to\", \" explain\", \" how\", \" you\", \" claim\", \" your\", \" free\", \" Amazon\", \" Echo\", \" Dot\", \".\", \"\\n\", \"\\n\", \"If\", \" you\", \" return\", \" your\", \" item\", \" due\", \" to\", \" a\", \" fault\", \",\", \" where\", \" possible\", \",\", \" a\", \" replacement\", \" item\", \" will\", \" be\", \" provided\", \".\"], [\"Q\", \":\", \"\\n\", \"\\n\", \"Class\", \" AB\", \" amplifier\", \"\\n\", \"\\n\", \"What\", \" role\", \" does\", \" R\", \"v\", \" play\", \" in\", \" this\", \" AB\", \" class\", \" amplifier\", \"?\", \"\\n\", \"\\n\", \"A\", \":\", \"\\n\", \"\\n\", \"This\", \" is\", \" a\", \" class\", \" B\", \" amplifier\", \":\", \" -\", \"\\n\", \"\\n\", \"Your\", \" circuit\", \" is\", \" a\", \" class\", \" AB\", \" amplifier\", \":\", \" -\", \"\\n\", \"\\n\", \"R\", \"v\", \" adjust\", \"s\", \" the\", \" bias\", \" point\", \" of\", \" the\", \" two\", \" transistors\", \" so\", \" that\", \" T\", \"1\", \" and\", \" T\", \"2\", \" are\", \" always\", \" conducting\", \" a\", \" little\", \" bit\", \" of\", \" current\", \" -\", \" this\", \" avoids\", \" excessive\", \" cross\", \" over\", \" distortion\", \":\", \" -\", \"\\n\", \"\\n\", \"See\", \" also\", \" this\", \" article\", \",\", \" Cros\", \"sover\", \" Dist\", \"ortion\", \" in\", \" Ampl\", \"ifiers\", \",\", \" for\", \" more\", \" information\", \".\"], [\"San\", \" Francisco\", \" is\", \" a\", \" city\", \" of\", \" rich\", \" history\", \" and\", \" culture\", \",\", \" and\", \" as\", \" anyone\", \" planning\", \" a\", \" visit\", \" to\", \" the\", \" City\", \" by\", \" the\", \" Bay\", \" realizes\", \",\", \" it\", \" can\", \" be\", \" difficult\", \" to\", \" narrow\", \" down\", \" all\", \" the\", \" places\", \" to\", \" visit\", \" and\", \" thing\", \" to\", \" do\", \" while\", \" there\", \".\", \" Aside\", \" from\", \" the\", \" usual\", \" tourist\", \" spots\", \" like\", \" the\", \" Golden\", \" Gate\", \" Bridge\", \",\", \" Al\", \"cat\", \"raz\", \",\", \" and\", \" Fisher\", \"man\", \"'s\", \" Wh\", \"arf\", \",\", \" San\", \" Francisco\", \" also\", \" offers\", \" historic\", \" architecture\", \" on\", \" nearly\", \" every\", \" corner\", \",\", \" a\", \" se\", \"rene\", \" Japanese\", \" Tea\", \" Gardens\", \",\", \" the\", \" glorious\", \" Golden\", \" Gate\", \" Park\", \",\", \" alongside\", \" countless\", \" cultural\", \" and\", \" artistic\", \" institutions\", \".\", \" Need\", \" help\", \" fitting\", \" it\", \" all\", \" into\", \" one\", \" vacation\", \"?\", \" You\", \" might\", \" need\", \" an\", \" app\", \" for\", \" your\", \" smartphone\", \" (\", \"or\", \" tablet\", \")\", \" to\", \" serve\", \" as\", \" your\", \" guide\", \".\"], [\"My\", \" new\", \" global\", \" trends\", \" book\", \" is\", \" out\", \" now\", \":\", \" The\", \" Future\", \" of\", \" Almost\", \" Everything\", \".\", \" But\", \" it\", \" takes\", \" at\", \" least\", \" 20\", \" years\", \" to\", \" evaluate\", \" how\", \" good\", \" a\", \" trends\", \" analyst\", \" was\", \" /\", \" is\", \" \\u2013\", \" so\", \" what\", \" about\", \" forecasts\", \" made\", \" by\", \" me\", \" in\", \" previous\", \" books\", \",\", \" about\", \" what\", \" to\", \" expect\", \" over\", \" the\", \" following\", \" decade\", \" or\", \" two\", \" or\", \" three\", \"?\", \" How\", \" did\", \" those\", \" forecasts\", \" measure\", \" up\", \"?\", \" I\", \" had\", \" to\", \" answer\", \" that\", \" question\", \" for\", \" myself\", \" by\", \" re\", \"-\", \"reading\", \" what\", \" I\", \" wrote\", \" in\", \" the\", \" past\", \" about\", \" the\", \" future\", \",\", \" before\", \" writing\", \" my\", \" latest\", \" book\", \".\", \" Read\", \" FREE\", \" S\", \"AMPLE\", \" of\", \" The\", \" Truth\", \" about\", \" Almost\", \" Everything\", \".\"], [\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\", \"The\", \" date\", \" is\", \" fast\", \" approaching\", \" for\", \" our\", \" spring\", \" rally\", \".\", \" I\", \" have\", \" posted\", \" the\", \" reservation\", \" information\", \" in\", \" the\", \" Calendar\", \" section\", \",\", \" I\", \" will\", \" post\", \" more\", \" details\", \" in\", \" the\", \" calendar\", \" section\", \" as\", \" they\", \" become\", \" available\", \".\", \" If\", \" you\", \" have\", \" any\", \" questions\", \" please\", \" e\", \"-\", \"mail\", \" me\", \" at\", \" tx\", \"je\", \"ff\", \"123\", \"@\", \"gmail\", \".\", \"com\"], [\"Instead\", \" of\", \" attaching\", \" a\", \" complete\", \" file\", \",\", \" could\", \" you\", \" please\", \" create\", \" a\", \" diff\", \" of\", \" your\", \" changes\", \" against\", \" the\", \" original\", \" file\", \"?\", \" If\", \" possible\", \" we\", \"'d\", \" also\", \" prefer\", \" it\", \" submitted\", \" as\", \" a\", \" commit\", \" change\", \" to\", \" our\", \" ger\", \"rit\", \" instance\", \",\", \" see\", \" https\", \"://\", \"wiki\", \".\", \"document\", \"found\", \"ation\", \".\", \"org\", \"/\", \"Development\", \"/\", \"ger\", \"rit\", \" but\", \" that\", \"'s\", \" not\", \" strictly\", \" necessary\", \" if\", \" you\", \"'re\", \" not\", \" familiar\", \" with\", \" git\", \" and\", \" develop\", \" tools\", \" and\", \" such\", \".\", \"\\n\", \"In\", \" any\", \" case\", \" we\", \"'ll\", \" need\", \" your\", \" license\", \" agreement\", \",\", \" apparently\", \" we\", \" don\", \"'t\", \" have\", \" it\", \" on\", \" file\", \",\", \" could\", \" you\", \" please\", \" send\", \" us\", \" a\", \" blanket\", \" statement\", \" that\", \" you\", \" contribute\", \" all\", \" your\", \" past\", \" and\", \" future\", \" patches\", \" under\", \" the\", \" M\", \"PL\", \"v\", \"2\", \" and\", \" LG\", \"PL\", \"v\", \"3\", \"+\", \" licenses\", \"?\"], [\"It\", \" is\", \" done\", \",\", \" and\", \" submitted\", \".\", \" You\", \" can\", \" play\", \" \\u201c\", \"Sur\", \"vival\", \" of\", \" the\", \" T\", \"ast\", \"iest\", \"\\u201d\", \" on\", \" Android\", \",\", \" and\", \" on\", \" the\", \" web\", \".\"], [\"All\", \" Studio\", \" Posts\", \"\\n\", \"\\n\", \"The\", \" upcoming\", \" AES\", \" 54\", \"th\", \" International\", \" Conference\", \"m\", \" focusing\", \" on\", \" audio\", \" fore\", \"ns\", \"ics\", \",\", \" is\", \" set\", \" to\", \" take\", \" place\", \" June\", \" 12\", \"-\", \"14\", \",\", \" 2014\", \",\", \" at\", \" the\", \" Holiday\", \" Inn\", \" Bl\", \"o\", \"oms\", \"bury\", \" in\", \" London\", \".\", \" Ded\", \"icated\", \" to\", \" exploring\", \" techniques\", \",\", \" technologies\", \" and\", \" advance\", \"ments\", \" in\", \" the\", \" field\", \" of\", \" audio\", \" fore\", \"ns\", \"ics\", \",\", \" the\", \" conference\", \" will\", \" provide\", \" a\", \" platform\", \" for\", \" sharing\", \" research\", \" related\", \" to\", \" the\", \" forensic\", \" application\", \" of\", \" speech\", \"/\", \"signal\", \" processing\", \",\", \" ac\", \"oust\", \"ical\", \" analyses\", \",\", \" audio\", \" authentication\", \" and\", \" the\", \" examination\", \" of\", \" methodologies\", \" and\", \" best\", \" practices\", \".\", \" Chair\", \"pers\", \"ons\", \" for\", \" this\", \" conference\", \" are\", \" Mark\", \" H\", \"uck\", \"v\", \"ale\", \" and\", \" Jeff\", \" M\", \".\", \" Smith\", \".\"], [\"In\", \" the\", \" Community\", \"\\n\", \"\\n\", \"Near\", \"by\", \" Schools\", \"\\n\", \"\\n\", \"3\", \"208\", \" Per\", \"dot\", \" Avenue\", \",\", \" Ros\", \"amond\", \",\", \" CA\", \" 9\", \"35\", \"60\", \" (\", \"ML\", \"S\", \"#\", \" SR\", \"167\", \"275\", \"60\", \")\", \" is\", \" a\", \"\\n\", \"Single\", \" Family\", \" property\", \" with\", \" 4\", \" bedrooms\", \",\", \" 2\", \" full\", \" bathrooms\", \" and\", \" 1\", \" partial\", \" bathroom\", \".\", \"\\n\", \"3\", \"208\", \" Per\", \"dot\", \" Avenue\", \" is\", \" currently\", \" listed\", \" for\", \" $\", \"294\", \",\", \"990\", \" and\", \" was\", \" received\", \" on\", \" October\", \" 17\", \",\", \" 2016\", \".\", \"\\n\", \"Want\", \" to\", \" learn\", \" more\", \" about\", \" 3\", \"208\", \" Per\", \"dot\", \" Avenue\", \"?\"], [\"P\", \"urchase\", \" either\", \" a\", \" combined\", \" Build\", \"ings\", \" &\", \" Contents\", \" Home\", \" Insurance\", \" policy\", \",\", \" or\", \" separate\", \" Build\", \"ings\", \" or\", \" Contents\", \" Home\", \" Insurance\", \" Policy\", \" online\", \" at\", \" Little\", \"woods\", \".\", \"com\", \" between\", \" 1\", \"st\", \" and\", \" 31\", \"st\", \" August\", \" 2017\", \" to\", \" qualify\", \" for\", \" a\", \" free\", \" Amazon\", \" Echo\", \" Dot\", \".\", \" New\", \" Little\", \"woods\", \" Home\", \" Insurance\", \" customers\", \" only\", \".\"], [\"#\", \"1\", \"\\n\", \"Free\", \"\\n\", \"Station\", \"ery\", \" Download\", \" Site\", \"\\n\", \"\\n\", \"K\", \"athy\", \"\\n\", \"and\", \" I\", \" would\", \" like\", \" to\", \" formally\", \" welcome\", \" you\", \" to\", \" our\", \" new\", \"\\n\", \"free\", \" holiday\", \" and\", \" special\", \" occasion\", \" station\", \"ery\", \" website\", \".\"], [\"#\", \"1\", \"\\n\", \"Free\", \"\\n\", \"Station\", \"ery\", \" Download\", \" Site\", \"\\n\", \"\\n\", \"K\", \"athy\", \"\\n\", \"and\", \" I\", \" would\", \" like\", \" to\", \" formally\", \" welcome\", \" you\", \" to\", \" our\", \" new\", \"\\n\", \"free\", \" holiday\", \" and\", \" special\", \" occasion\", \" station\", \"ery\", \" website\", \".\", \" We\", \" are\", \" working\", \" hard\", \" to\", \"\\n\", \"add\", \" as\", \" many\", \" new\", \" station\", \"ery\", \" papers\", \" as\", \" we\", \" can\", \" as\", \"\\n\", \"quick\", \"ly\", \" as\", \" we\", \" can\", \".\", \" We\", \" are\", \" adding\", \" new\", \" paper\", \"\\n\", \"design\", \"s\", \" at\", \" least\", \" weekly\", \" when\", \" possible\", \".\"], [\"About\", \" Grand\", \" S\", \"lam\", \" F\", \"ishing\", \" Char\", \"ters\", \"\\n\", \"\\n\", \"As\", \" a\", \" family\", \" owned\", \" business\", \" we\", \" know\", \" how\", \" important\", \" it\", \" is\", \" that\", \" your\", \" trip\", \" becomes\", \" the\", \" best\", \" memory\", \" of\", \" your\", \" vacation\", \",\", \" we\", \" are\", \" proud\", \" of\", \" our\", \" islands\", \",\", \" our\", \" waters\", \" and\", \" our\", \" crew\", \" and\", \" we\", \" are\", \" desperate\", \" show\", \" you\", \" the\", \" best\", \" possible\", \" time\", \" during\", \" your\", \" stay\", \".\", \" We\", \" can\", \" not\", \" guarantee\", \" fish\", \" every\", \" time\", \" but\", \" we\", \" can\", \" guarantee\", \" you\", \" a\", \" great\", \" time\", \"!\"], [\"Over\", \" every\", \" mountain\", \" there\", \" is\", \" a\", \" path\", \",\", \" although\", \" it\", \" may\", \" not\", \" be\", \" seen\", \" from\", \" the\", \" valley\", \"\\n\", \"\\n\", \"I\", \"'m\", \" now\", \" taking\", \" commissions\", \",\", \" if\", \" you\", \"'re\", \" interested\", \" in\", \" some\", \" high\", \" quality\", \" terrain\", \" PM\", \" me\", \".\"], [\"Over\", \" every\", \" mountain\", \" there\", \" is\", \" a\", \" path\", \",\", \" although\", \" it\", \" may\", \" not\", \" be\", \" seen\", \" from\", \" the\", \" valley\", \"\\n\", \"\\n\", \"I\", \"'m\", \" now\", \" taking\", \" commissions\", \",\", \" if\", \" you\", \"'re\", \" interested\", \" in\", \" some\", \" high\", \" quality\", \" terrain\", \" PM\", \" me\", \".\", \"\\n\\n\\n\\n\\n\", \"\\n\", \"World\", \"Pain\", \"ter\", \" (\", \"for\", \" creating\", \" the\", \" map\", \")\", \"\\n\\n\\n\", \"\\n\", \"Chunk\", \"y\", \" (\", \"for\", \" rendering\", \" the\", \" map\", \")\", \"\\n\\n\\n\", \"\\n\", \"Thanks\", \" for\", \" stopping\", \" by\", \",\", \" if\", \" you\", \" enjoyed\", \" this\", \" submission\", \" please\", \" consider\", \" leaving\", \" a\", \" diamond\", \"!\"]], \"activations\": [[[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.2318291664123535]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[7.007639408111572]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.16587162017822266]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.081214427947998]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.246947765350342]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.687774181365967]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.203967094421387]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.223330974578857]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.04318523406982422]], [[4.345119476318359]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.8942694664001465]], [[5.142200946807861]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.16587162017822266]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.081214427947998]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.246947765350342]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.687774181365967]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.9439473152160645]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.83042573928833]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.04318523406982422]], [[4.345119476318359]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.16587162017822266]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.081214427947998]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.246947765350342]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.203967094421387]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.16587162017822266]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.081214427947998]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.2860100269317627]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.504786491394043]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[3.911980628967285]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.2860100269317627]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.504786491394043]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[3.911980628967285]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[3.691863536834717]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.2860100269317627]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.504786491394043]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[3.911980628967285]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[3.691863536834717]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.0105504989624023]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[3.6421141624450684]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.1788582801818848]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.8500847816467285]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.29721784591674805]], [[0.0]], [[0.0]], [[3.498818874359131]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[3.364440441131592]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.8002338409423828]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.6222949028015137]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.632728099822998]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[3.1961655616760254]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[3.04356050491333]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.9454545974731445]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.965817928314209]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.15717315673828125]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.6866636276245117]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.930911064147949]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.04318523406982422]], [[4.345119476318359]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.8942694664001465]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.532343864440918]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.2619752883911133]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.8878822326660156]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.860806941986084]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.349039077758789]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.8693771362304688]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.6910948753356934]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.4517107009887695]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.641991138458252]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.8002338409423828]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.6222949028015137]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.4655508995056152]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.4655508995056152]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.283794403076172]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.367358684539795]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.0343852043151855]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.339905261993408]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.328434944152832]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.328434944152832]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.3032350540161133]]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7fe08c5fd700>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "component_idx = 4\n",
    "feat_idx = 7536\n",
    "\n",
    "submodule = submodules[component_idx]\n",
    "dictionary = dictionaries[submodule]\n",
    "\n",
    "# interpret some features\n",
    "data = zst_to_generator('/share/data/datasets/pile/the-eye.eu/public/AI/pile/train/00.jsonl.zst')\n",
    "buffer = ActivationBuffer(\n",
    "    data,\n",
    "    model,\n",
    "    submodule,\n",
    "    out_feats=512,\n",
    "    in_batch_size=128,\n",
    "    n_ctxs=512,\n",
    ")\n",
    "\n",
    "out = examine_dimension(\n",
    "    model,\n",
    "    submodule,\n",
    "    buffer,\n",
    "    dictionary,\n",
    "    dim_idx=feat_idx,\n",
    "    n_inputs=256\n",
    ")\n",
    "print(out.top_tokens)\n",
    "print(out.top_affected)\n",
    "out.top_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_ablate = {\n",
    "    submodules[0] : [\n",
    "        10313, # He\n",
    "        17329, # he\n",
    "        #23553, # unclear\n",
    "        25794, # his\n",
    "        2910, # she but in very few cases?\n",
    "        7187, # Her + His on two examples?\n",
    "        # 11379, # or \n",
    "        # 11674, # hospital\n",
    "        # 11909, # call\n",
    "        # 13051, # unclear\n",
    "        # 13094, # share \n",
    "        15628, # female names\n",
    "        # 17078, # primary care or health care\n",
    "        # 22846, # unclear\n",
    "        # 29183, # certain verbs\n",
    "        # 30927, # unclear\n",
    "        31251, # her\n",
    "        # 32356, # f\n",
    "    ],\n",
    "    submodules[1] : [\n",
    "        11727, # He\n",
    "        # 12955, # unclear \n",
    "        16731, # his\n",
    "        # 18680, # certain /'s\n",
    "        2578, # female names\n",
    "        20964, # certain instances of her\n",
    "        22287, # she\n",
    "    ],\n",
    "    submodules[2] : [\n",
    "        # 2859, # periods which end sentences\n",
    "        5547, # He\n",
    "        # 12633, # unclear\n",
    "        # 17324, # periods in medical contexts\n",
    "    ],\n",
    "    submodules[3] : [\n",
    "        # 10539, # periods in biographies\n",
    "        # 16734, # unclear\n",
    "        # 26887, # fires a lot in the biography of a particular male professor, unclear\n",
    "        # 29985, # periods in biographies\n",
    "        # 3216, # certain periods\n",
    "        # 31219, # certain periods \n",
    "    ],\n",
    "    submodules[4] : [\n",
    "        # 8126, # periods in biographies\n",
    "        # 23954, # certain periods\n",
    "        # 30226, # same as 26887 above\n",
    "        # 7536, # certain periods\n",
    "        25160, # unclear, but predicts female-associated words\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_ablations(\n",
    "        model,\n",
    "        inputs,\n",
    "        submodules,\n",
    "        dictionaries,\n",
    "        to_ablate,\n",
    "        inference=True,\n",
    "):\n",
    "    with model.invoke(inputs, fwd_args={'inference': inference}):\n",
    "        for submodule in submodules:\n",
    "            dictionary = dictionaries[submodule]\n",
    "            x = submodule.output\n",
    "            is_resid = (type(x.shape) == tuple)\n",
    "            if is_resid:\n",
    "                x = x[0]\n",
    "            x_hat = dictionary(x)\n",
    "            residual = x - x_hat\n",
    "\n",
    "            f = dictionary.encode(x)\n",
    "            ablation_idxs = t.Tensor(to_ablate[submodule]).long()\n",
    "            f[:, :, ablation_idxs] = 0.\n",
    "            x_hat = dictionary.decode(f)\n",
    "            if is_resid:\n",
    "                submodule.output[0][:] = x_hat + residual\n",
    "            else:\n",
    "                submodule.output = x_hat + residual\n",
    "            \n",
    "        acts = model.gpt_neox.layers[layer].output[0][:,-1,:].save()\n",
    "    return acts.value.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acts_abl(text):\n",
    "    return run_with_ablations(\n",
    "        model,\n",
    "        text,\n",
    "        submodules,\n",
    "        dictionaries,\n",
    "        to_ablate,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous test accuracy: 0.9613150358200073\n",
      "Ground truth accuracy: 0.856566846370697\n",
      "Spurious accuracy: 0.6111751198768616\n"
     ]
    }
   ],
   "source": [
    "new_probe, _ = train_probe(get_acts_abl, label_idx=0, lr=lr, epochs=epochs)\n",
    "print('Ambiguous test accuracy:', test_probe(new_probe, get_acts_abl, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(new_probe, get_acts_abl, batches=batches, label_idx=0))\n",
    "print('Spurious accuracy:', test_probe(new_probe, get_acts_abl, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(new_probe, get_acts_abl, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on ambiguous data: 0.4993029832839966\n",
      "Ground truth accuracy 0.4976958632469177\n",
      "Spurious accuracy 0.5011520981788635\n"
     ]
    }
   ],
   "source": [
    "def out_fn(model):\n",
    "    return probe(model.gpt_neox.layers[layer].output[0][:,-1,:])\n",
    "\n",
    "# get accuracy after ablating above features\n",
    "batches = get_data(train=False, ambiguous=True, batch_size=128, seed=SEED)\n",
    "\n",
    "with t.no_grad():\n",
    "    corrects = []\n",
    "    for batch in batches:\n",
    "        text = batch[0]\n",
    "        labels = batch[1]\n",
    "        probs = run_with_ablations(\n",
    "            model,\n",
    "            text,\n",
    "            submodules,\n",
    "            dictionaries,\n",
    "            to_ablate,\n",
    "            out_fn\n",
    "        )\n",
    "        preds = (probs > 0.5).long()\n",
    "        corrects.append((preds == labels).float())\n",
    "    print(\"Accuracy on ambiguous data:\", t.cat(corrects).mean().item())\n",
    "\n",
    "batches = get_data(train=False, ambiguous=False, batch_size=128, seed=SEED)\n",
    "\n",
    "with t.no_grad():\n",
    "    truth_corrects, spurious_corrects = [], []\n",
    "    for batch in batches:\n",
    "        text = batch[0]\n",
    "        true_labels = batch[1]\n",
    "        spurious_labels = batch[2]\n",
    "        probs = run_with_ablations(\n",
    "            model,\n",
    "            text,\n",
    "            submodules,\n",
    "            dictionaries,\n",
    "            to_ablate,\n",
    "            out_fn\n",
    "        )\n",
    "        preds = (probs > 0.5).long()\n",
    "        truth_corrects.append((preds == true_labels).float())\n",
    "        spurious_corrects.append((preds == spurious_labels).float())\n",
    "    print(\"Ground truth accuracy\", t.cat(truth_corrects).mean().item())\n",
    "    print(\"Spurious accuracy\", t.cat(spurious_corrects).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrain probe with ablated model\n",
    "\n",
    "def out_fn(model):\n",
    "    return model.gpt_neox.layers[layer].output[0][:,-1,:]\n",
    "\n",
    "t.manual_seed(SEED)\n",
    "new_probe = Probe(512).to('cuda:0')\n",
    "optimizer = t.optim.AdamW(new_probe.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "batches = get_data(train=True, ambiguous=True, batch_size=64, seed=SEED)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    for batch in batches:\n",
    "        text = batch[0]\n",
    "        labels = batch[1]\n",
    "        acts = run_with_ablations(\n",
    "            model,\n",
    "            text,\n",
    "            submodules,\n",
    "            dictionaries,\n",
    "            to_ablate,\n",
    "            out_fn\n",
    "        ).clone()\n",
    "        probs = new_probe(acts)\n",
    "        loss = criterion(probs, labels.float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
