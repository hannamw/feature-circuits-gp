{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/u/smarks/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/share/u/smarks/.local/lib/python3.11/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Matplotlib created a temporary cache directory at /tmp/matplotlib-8czlsmw3 because the default path (/share/u/smarks/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from nnsight import LanguageModel\n",
    "import torch as t\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from attribution import patching_effect\n",
    "from dictionary_learning import AutoEncoder, ActivationBuffer\n",
    "from dictionary_learning.interp import examine_dimension\n",
    "from dictionary_learning.utils import zst_to_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# dataset hyperparameters\n",
    "dataset = load_dataset(\"LabHC/bias_in_bios\")\n",
    "profession_dict = {'professor' : 21, 'nurse' : 13}\n",
    "male_prof = 'professor'\n",
    "female_prof = 'nurse'\n",
    "\n",
    "# model hyperparameters\n",
    "DEVICE = 'cuda:0'\n",
    "model = LanguageModel('EleutherAI/pythia-70m-deduped', device_map=DEVICE)\n",
    "activation_dim = 512\n",
    "layer = 4\n",
    "\n",
    "# dictionary hyperparameters\n",
    "dict_id = 10\n",
    "expansion_factor = 64\n",
    "dictionary_size = expansion_factor * activation_dim\n",
    "\n",
    "# data preparation hyperparameters\n",
    "batch_size = 1024\n",
    "SEED = 42\n",
    "\n",
    "def get_data(train=True, ambiguous=True, batch_size=128, seed=SEED):\n",
    "    if train:\n",
    "        data = dataset['train']\n",
    "    else:\n",
    "        data = dataset['test']\n",
    "    if ambiguous:\n",
    "        neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        n = min([len(neg), len(pos)])\n",
    "        neg, pos = neg[:n], pos[:n]\n",
    "        data = neg + pos\n",
    "        labels = [0]*n + [1]*n\n",
    "        idxs = list(range(2*n))\n",
    "        random.Random(seed).shuffle(idxs)\n",
    "        data, labels = [data[i] for i in idxs], [labels[i] for i in idxs]\n",
    "        true_labels = spurious_labels = labels\n",
    "    else:\n",
    "        neg_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        neg_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 1]\n",
    "        pos_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 0]\n",
    "        pos_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        n = min([len(neg_neg), len(neg_pos), len(pos_neg), len(pos_pos)])\n",
    "        neg_neg, neg_pos, pos_neg, pos_pos = neg_neg[:n], neg_pos[:n], pos_neg[:n], pos_pos[:n]\n",
    "        data = neg_neg + neg_pos + pos_neg + pos_pos\n",
    "        true_labels     = [0]*n + [0]*n + [1]*n + [1]*n\n",
    "        spurious_labels = [0]*n + [1]*n + [0]*n + [1]*n\n",
    "        idxs = list(range(4*n))\n",
    "        random.Random(seed).shuffle(idxs)\n",
    "        data, true_labels, spurious_labels = [data[i] for i in idxs], [true_labels[i] for i in idxs], [spurious_labels[i] for i in idxs]\n",
    "\n",
    "    batches = [\n",
    "        (data[i:i+batch_size], t.tensor(true_labels[i:i+batch_size], device=DEVICE), t.tensor(spurious_labels[i:i+batch_size], device=DEVICE)) for i in range(0, len(data), batch_size)\n",
    "    ]\n",
    "\n",
    "    return batches\n",
    "\n",
    "def get_subgroups(train=True, ambiguous=True, batch_size=128, seed=SEED):\n",
    "    if train:\n",
    "        data = dataset['train']\n",
    "    else:\n",
    "        data = dataset['test']\n",
    "    if ambiguous:\n",
    "        neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        neg_labels, pos_labels = (0, 0), (1, 1)\n",
    "        subgroups = [(neg, neg_labels), (pos, pos_labels)]\n",
    "    else:\n",
    "        neg_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 0]\n",
    "        neg_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[male_prof] and x['gender'] == 1]\n",
    "        pos_neg = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 0]\n",
    "        pos_pos = [x['hard_text'] for x in data if x['profession'] == profession_dict[female_prof] and x['gender'] == 1]\n",
    "        neg_neg_labels, neg_pos_labels, pos_neg_labels, pos_pos_labels = (0, 0), (0, 1), (1, 0), (1, 1)\n",
    "        subgroups = [(neg_neg, neg_neg_labels), (neg_pos, neg_pos_labels), (pos_neg, pos_neg_labels), (pos_pos, pos_pos_labels)]\n",
    "    \n",
    "    out = {}\n",
    "    for data, label_profile in subgroups:\n",
    "        out[label_profile] = []\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            text = data[i:i+batch_size]\n",
    "            out[label_profile].append(\n",
    "                (\n",
    "                    text,\n",
    "                    t.tensor([label_profile[0]]*len(text), device=DEVICE),\n",
    "                    t.tensor([label_profile[1]]*len(text), device=DEVICE)\n",
    "                )\n",
    "            )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probe training hyperparameters\n",
    "lr = 1e-2\n",
    "epochs = 1\n",
    "\n",
    "class Probe(nn.Module):\n",
    "    def __init__(self, activation_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(activation_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x).squeeze(-1)\n",
    "        return logits.sigmoid()\n",
    "\n",
    "def train_probe(get_acts, label_idx=0, batches=get_data(), lr=1e-2, epochs=1, seed=SEED):\n",
    "    t.manual_seed(seed)\n",
    "    probe = Probe(512).to('cuda:0')\n",
    "    optimizer = t.optim.AdamW(probe.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for batch in batches:\n",
    "            text = batch[0]\n",
    "            labels = batch[label_idx+1] \n",
    "            acts = get_acts(text)\n",
    "            logits = probe(acts)\n",
    "            loss = criterion(logits, labels.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return probe, losses\n",
    "\n",
    "def test_probe(probe, get_acts, label_idx=0, batches=get_data(train=False), seed=SEED):\n",
    "    with t.no_grad():\n",
    "        corrects = []\n",
    "\n",
    "        for batch in batches:\n",
    "            text = batch[0]\n",
    "            labels = batch[label_idx+1]\n",
    "            acts = get_acts(text)\n",
    "            logits = probe(acts)\n",
    "            preds = (logits > 0.5).long()\n",
    "            corrects.append((preds == labels).float())\n",
    "        return t.cat(corrects).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acts(text):\n",
    "    with model.invoke(text):\n",
    "        acts = model.gpt_neox.layers[layer].output[0][:,-1,:].save()\n",
    "    return acts.value.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "probe0, _ = train_probe(get_acts, label_idx=0, batches=get_data(ambiguous=False), lr=lr, epochs=epochs)\n",
    "probe1, _ = train_probe(get_acts, label_idx=1, batches=get_data(ambiguous=False), lr=lr, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probe 0 accuracy: 0.900921642780304\n",
      "Probe 1 accuracy: 0.9688940048217773\n"
     ]
    }
   ],
   "source": [
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Probe 0 accuracy:', test_probe(probe0, get_acts, batches=batches, label_idx=0))\n",
    "print('Probe 1 accuracy:', test_probe(probe1, get_acts, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous test accuracy: 0.9915195107460022\n",
      "Ground truth accuracy: 0.6261520981788635\n",
      "Spurious accuracy: 0.8680875301361084\n"
     ]
    }
   ],
   "source": [
    "probe, _ = train_probe(get_acts, label_idx=0, lr=lr, epochs=epochs)\n",
    "print('Ambiguous test accuracy:', test_probe(probe, get_acts, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(probe, get_acts, batches=batches, label_idx=0))\n",
    "print('Spurious accuracy:', test_probe(probe, get_acts, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.9941375851631165\n",
      "Accuracy for (0, 1): 0.13586179912090302\n",
      "Accuracy for (1, 0): 0.38479262590408325\n",
      "Accuracy for (1, 1): 0.9881505370140076\n"
     ]
    }
   ],
   "source": [
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(probe, get_acts, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "submodules = [\n",
    "    model.gpt_neox.layers[i] for i in range(layer + 1)\n",
    "]\n",
    "dictionaries = {}\n",
    "for i in range(layer + 1):\n",
    "    ae = AutoEncoder(512, 64 * 512).to(DEVICE)\n",
    "    ae.load_state_dict(t.load(f'/share/projects/dictionary_circuits/autoencoders/pythia-70m-deduped/resid_out_layer{i}/{dict_id}_{dictionary_size}/ae.pt'))\n",
    "    dictionaries[submodules[i]] = ae\n",
    "\n",
    "def metric_fn(model):\n",
    "    return probe(model.gpt_neox.layers[layer].output[0][:,-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_inputs, pos_inputs = [], []\n",
    "for x in dataset['train']:\n",
    "    if x['profession'] == profession_dict[male_prof] and x['gender'] == 0 and len(neg_inputs) < 16:\n",
    "        neg_inputs.append(x['hard_text'])\n",
    "    if x['profession'] == profession_dict[female_prof] and x['gender'] == 1 and len(pos_inputs) < 16:\n",
    "        pos_inputs.append(x['hard_text'])\n",
    "\n",
    "neg_effects = patching_effect(\n",
    "    neg_inputs,\n",
    "    None,\n",
    "    model,\n",
    "    submodules,\n",
    "    dictionaries,\n",
    "    metric_fn,\n",
    "    method='ig'\n",
    ").effects\n",
    "neg_effects = {k : v.sum(dim=1).mean(dim=0) for k, v in neg_effects.items()}\n",
    "\n",
    "pos_effects = patching_effect(\n",
    "    pos_inputs,\n",
    "    None,\n",
    "    model,\n",
    "    submodules,\n",
    "    dictionaries,\n",
    "    metric_fn,\n",
    "    method='ig'\n",
    ").effects\n",
    "\n",
    "pos_effects = {k : v.sum(dim=1).mean(dim=0) for k, v in pos_effects.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0:\n",
      "    Multindex: (4074,), Value: 0.007796672638505697\n",
      "    Multindex: (10282,), Value: 0.005953371524810791\n",
      "    Multindex: (14149,), Value: 0.008430274203419685\n",
      "    Multindex: (18775,), Value: 0.016031203791499138\n",
      "    Multindex: (18967,), Value: 0.044523485004901886\n",
      "    Multindex: (22084,), Value: 0.014413665048778057\n",
      "    Multindex: (23255,), Value: 0.006609784439206123\n",
      "    Multindex: (23898,), Value: 0.00813205074518919\n",
      "    Multindex: (24435,), Value: 0.006274200975894928\n",
      "    Multindex: (29626,), Value: 0.012494005262851715\n",
      "    Multindex: (31616,), Value: 0.006371437571942806\n",
      "Layer 1:\n",
      "    Multindex: (2995,), Value: 0.013699337840080261\n",
      "    Multindex: (8920,), Value: 0.029961880296468735\n",
      "    Multindex: (12128,), Value: 0.02386729046702385\n",
      "    Multindex: (12436,), Value: 0.00632051657885313\n",
      "Layer 2:\n",
      "    Multindex: (4433,), Value: 0.014091068878769875\n",
      "    Multindex: (4539,), Value: 0.015132302418351173\n",
      "    Multindex: (8944,), Value: 0.023961298167705536\n",
      "    Multindex: (10700,), Value: 0.008245648816227913\n",
      "    Multindex: (11656,), Value: 0.011116784065961838\n",
      "    Multindex: (14559,), Value: 0.013782883994281292\n",
      "    Multindex: (15225,), Value: 0.005212010350078344\n",
      "    Multindex: (15938,), Value: 0.006554870866239071\n",
      "    Multindex: (19014,), Value: 0.00791632104665041\n",
      "    Multindex: (27803,), Value: 0.009038958698511124\n",
      "    Multindex: (29206,), Value: 0.024784894660115242\n",
      "    Multindex: (30263,), Value: 0.016338728368282318\n",
      "Layer 3:\n",
      "    Multindex: (93,), Value: 0.015708720311522484\n",
      "    Multindex: (2751,), Value: 0.005914686247706413\n",
      "    Multindex: (13474,), Value: 0.024755682796239853\n",
      "    Multindex: (15246,), Value: 0.005424877628684044\n",
      "    Multindex: (24661,), Value: 0.007444471120834351\n",
      "    Multindex: (27334,), Value: 0.019405348226428032\n",
      "    Multindex: (27867,), Value: 0.01861327514052391\n",
      "Layer 4:\n",
      "    Multindex: (4125,), Value: 0.02841648831963539\n",
      "    Multindex: (11987,), Value: 0.013745594769716263\n",
      "    Multindex: (12332,), Value: 0.007290008012205362\n",
      "    Multindex: (14658,), Value: 0.009338128380477428\n",
      "    Multindex: (30220,), Value: 0.06206604838371277\n",
      "Layer 0:\n",
      "    Multindex: (68,), Value: -0.006124074105173349\n",
      "    Multindex: (951,), Value: -0.011823907494544983\n",
      "    Multindex: (1022,), Value: -0.00928553007543087\n",
      "    Multindex: (1692,), Value: -0.018448036164045334\n",
      "    Multindex: (2079,), Value: -0.011885412037372589\n",
      "    Multindex: (2493,), Value: -0.033086180686950684\n",
      "    Multindex: (3122,), Value: -0.01057964377105236\n",
      "    Multindex: (3302,), Value: -0.005454882979393005\n",
      "    Multindex: (5648,), Value: -0.014219814911484718\n",
      "    Multindex: (5950,), Value: -0.01904447004199028\n",
      "    Multindex: (6868,), Value: -0.006659955717623234\n",
      "    Multindex: (7761,), Value: -0.0061271172016859055\n",
      "    Multindex: (7917,), Value: -0.007851650007069111\n",
      "    Multindex: (9610,), Value: -0.009236276149749756\n",
      "    Multindex: (9651,), Value: -0.041233163326978683\n",
      "    Multindex: (10060,), Value: -0.12788894772529602\n",
      "    Multindex: (11229,), Value: -0.008016485720872879\n",
      "    Multindex: (11501,), Value: -0.008496860973536968\n",
      "    Multindex: (11778,), Value: -0.014973497949540615\n",
      "    Multindex: (13149,), Value: -0.008603328838944435\n",
      "    Multindex: (13675,), Value: -0.014579987153410912\n",
      "    Multindex: (15032,), Value: -0.009217903017997742\n",
      "    Multindex: (15075,), Value: -0.007187824696302414\n",
      "    Multindex: (17278,), Value: -0.009036744944751263\n",
      "    Multindex: (19357,), Value: -0.005605224519968033\n",
      "    Multindex: (19423,), Value: -0.0052977451123297215\n",
      "    Multindex: (23666,), Value: -0.01121536921709776\n",
      "    Multindex: (24418,), Value: -0.033275093883275986\n",
      "    Multindex: (26504,), Value: -0.021479854360222816\n",
      "    Multindex: (26586,), Value: -0.019033201038837433\n",
      "    Multindex: (26843,), Value: -0.005413400009274483\n",
      "    Multindex: (28359,), Value: -0.006014706566929817\n",
      "    Multindex: (28850,), Value: -0.006030871532857418\n",
      "    Multindex: (29058,), Value: -0.005045972298830748\n",
      "    Multindex: (31201,), Value: -0.011132286861538887\n",
      "    Multindex: (31219,), Value: -0.00683389138430357\n",
      "Layer 1:\n",
      "    Multindex: (4592,), Value: -0.026672789826989174\n",
      "    Multindex: (7025,), Value: -0.006162094883620739\n",
      "    Multindex: (9877,), Value: -0.025729408487677574\n",
      "    Multindex: (12882,), Value: -0.008831268176436424\n",
      "    Multindex: (14499,), Value: -0.00754063855856657\n",
      "    Multindex: (14918,), Value: -0.011177365668118\n",
      "    Multindex: (15017,), Value: -0.15685386955738068\n",
      "    Multindex: (17369,), Value: -0.037295110523700714\n",
      "    Multindex: (26204,), Value: -0.009701690636575222\n",
      "    Multindex: (26476,), Value: -0.01835453137755394\n",
      "    Multindex: (26969,), Value: -0.007508114445954561\n",
      "    Multindex: (28145,), Value: -0.007259350270032883\n",
      "    Multindex: (30248,), Value: -0.06816384196281433\n",
      "Layer 2:\n",
      "    Multindex: (277,), Value: -0.011803245171904564\n",
      "    Multindex: (1995,), Value: -0.055247072130441666\n",
      "    Multindex: (4770,), Value: -0.005406274925917387\n",
      "    Multindex: (9128,), Value: -0.09329873323440552\n",
      "    Multindex: (10635,), Value: -0.006498144939541817\n",
      "    Multindex: (10757,), Value: -0.005722416564822197\n",
      "    Multindex: (12440,), Value: -0.007464653346687555\n",
      "    Multindex: (14638,), Value: -0.054368987679481506\n",
      "    Multindex: (17774,), Value: -0.008461598306894302\n",
      "    Multindex: (21331,), Value: -0.01286228746175766\n",
      "    Multindex: (26413,), Value: -0.015460341237485409\n",
      "    Multindex: (27838,), Value: -0.008407261222600937\n",
      "    Multindex: (29295,), Value: -0.03418001905083656\n",
      "    Multindex: (29371,), Value: -0.017564712092280388\n",
      "    Multindex: (31098,), Value: -0.00504444120451808\n",
      "Layer 3:\n",
      "    Multindex: (7539,), Value: -0.010630715638399124\n",
      "    Multindex: (10295,), Value: -0.006168850697577\n",
      "    Multindex: (13542,), Value: -0.005549272522330284\n",
      "    Multindex: (14401,), Value: -0.008133084513247013\n",
      "    Multindex: (19558,), Value: -0.20383372902870178\n",
      "    Multindex: (20526,), Value: -0.005753857083618641\n",
      "    Multindex: (22152,), Value: -0.006324305199086666\n",
      "    Multindex: (23375,), Value: -0.010353682562708855\n",
      "    Multindex: (23545,), Value: -0.026318833231925964\n",
      "    Multindex: (24484,), Value: -0.02464858442544937\n",
      "    Multindex: (24806,), Value: -0.02636616677045822\n",
      "    Multindex: (27051,), Value: -0.03148452937602997\n",
      "    Multindex: (30802,), Value: -0.008429186418652534\n",
      "    Multindex: (31182,), Value: -0.03689089044928551\n",
      "    Multindex: (31751,), Value: -0.005460415035486221\n",
      "Layer 4:\n",
      "    Multindex: (1804,), Value: -0.006157426163554192\n",
      "    Multindex: (4926,), Value: -0.02842037007212639\n",
      "    Multindex: (5731,), Value: -0.00602379534393549\n",
      "    Multindex: (6869,), Value: -0.016256462782621384\n",
      "    Multindex: (9766,), Value: -0.00802060030400753\n",
      "    Multindex: (12420,), Value: -0.262023001909256\n",
      "    Multindex: (20708,), Value: -0.01720055565237999\n",
      "    Multindex: (21979,), Value: -0.02583683282136917\n",
      "    Multindex: (23207,), Value: -0.005485158879309893\n",
      "    Multindex: (30612,), Value: -0.008523142896592617\n",
      "    Multindex: (31282,), Value: -0.014792843721807003\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.005\n",
    "\n",
    "for i, submodule in enumerate(submodules):\n",
    "    print(f\"Layer {i}:\")\n",
    "    effect = neg_effects[submodule].act\n",
    "    for feature_idx in t.nonzero(effect):\n",
    "        value = effect[tuple(feature_idx)]\n",
    "        if value > threshold:\n",
    "            print(f\"    Multindex: {tuple(feature_idx.tolist())}, Value: {value}\")\n",
    "\n",
    "for i, submodule in enumerate(submodules):\n",
    "    print(f\"Layer {i}:\")\n",
    "    effect = pos_effects[submodule].act\n",
    "    for feature_idx in t.nonzero(effect):\n",
    "        value = effect[tuple(feature_idx)]\n",
    "        if -value > threshold:\n",
    "            print(f\"    Multindex: {tuple(feature_idx.tolist())}, Value: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' leaves', 3.1683294773101807), (' piece', 2.8998165130615234), (' telling', 2.8359341621398926), (' total', 2.607029914855957), (' tells', 2.443530559539795), (' needing', 2.205601930618286), (' notice', 2.185490608215332), (' finish', 2.0870466232299805), ('accept', 1.889017105102539), (' goes', 1.8599400520324707), (' [...]', 1.8255202770233154), (' 1960', 1.7827792167663574), (' donors', 1.685542345046997), (' Lady', 1.6427953243255615), (' Although', 1.6094988584518433), (' needs', 1.589821219444275), ('ret', 1.516148567199707), (' donations', 1.4957566261291504), (' counting', 1.4677677154541016), (' impressed', 1.4677202701568604), (' jumping', 1.4470441341400146), (' Instead', 1.440384864807129), (' jumped', 1.4324177503585815), ('iza', 1.3016449213027954), (' knees', 1.225874900817871), (' career', 1.162060022354126), (' puzzle', 1.1596356630325317), (' participate', 1.0996900796890259), (' inspired', 0.989930272102356), (' mate', 0.9897094964981079)]\n",
      "[(' her', 1.7537654638290405), (' herself', 1.7392916679382324), (' she', 1.6891292333602905), (' She', 1.2257136106491089), ('She', 1.1365330219268799), (' hers', 1.087778091430664), (' Her', 1.0701675415039062), ('Her', 1.0509861707687378), ('she', 0.9448744654655457), (' daughter', 0.8374568223953247), (' blonde', 0.8058038353919983), (' grandmother', 0.777862012386322), (' raped', 0.7526341080665588), (' husbands', 0.7414524555206299), (' actress', 0.7414485216140747), (' postpartum', 0.7349827289581299), (' tears', 0.7345147728919983), (' boyfriend', 0.7180842161178589), (' pregnancy', 0.7068537473678589), (' husband', 0.6929947137832642), (' girl', 0.6817805171012878), (' niece', 0.6765598058700562), (' sisters', 0.6651015281677246), (' Elle', 0.6559014916419983), ('daughter', 0.6539645791053772), (' menstru', 0.6454604268074036), ('her', 0.6400243043899536), (' homem', 0.6366956233978271), (' Aunt', 0.6318170428276062), (' aunt', 0.6264988780021667)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-6925c48e-fe35\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-6925c48e-fe35\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [[\"T\", \"uls\", \"i\", \" G\", \"abb\", \"ard\", \",\", \" 2020\", \" presidential\", \" candidate\", \",\", \" is\", \" closing\", \" in\", \" on\", \" the\", \" number\", \" of\", \" donations\", \" that\", \" she\", \" needs\", \" in\", \" order\", \" to\", \" qualify\", \" to\", \" participate\", \" in\", \" the\", \" Democratic\", \" debates\", \".\", \" As\", \" of\", \" March\", \" 20\", \",\", \" 2019\", \",\", \" she\", \" has\", \" 40\", \",\", \"456\", \" donations\", \",\", \" which\", \" leaves\", \" her\", \" needing\", \" just\", \" a\", \" little\", \" less\", \" than\", \" 25\", \",\", \"000\", \" more\", \" donations\", \" to\", \" participate\", \" in\", \" the\", \" D\", \"NC\", \" debates\", \".\", \" In\", \" total\", \",\"], [\"Ger\", \"da\", \" Gil\", \"bo\", \"e\", \"\\n\", \"\\n\", \"Ger\", \"da\", \" Gil\", \"bo\", \"e\", \" (\", \"5\", \" July\", \" 1914\", \"\\u00a0\", \"\\u2013\", \" 11\", \" April\", \" 2009\", \")\", \" was\", \" a\", \" Danish\", \" actress\", \" and\", \" singer\", \".\", \" She\", \" appeared\", \" in\", \" 18\", \" films\", \" between\", \" 1943\", \" and\", \" 2003\", \".\", \"\\n\", \"\\n\", \"Life\", \" \", \"\\n\", \"Gil\", \"bo\", \"e\", \" was\", \" born\", \" in\", \" 1914\", \".\", \" She\", \" was\", \" the\", \" daughter\", \" of\", \" a\", \" black\", \"smith\", \",\", \" Gil\", \"bo\", \"e\", \" started\", \" her\", \" career\", \" in\", \" musical\", \" theatre\", \" and\", \" oper\", \"as\", \" in\", \" A\", \"ar\", \"hus\", \" before\", \" she\", \" moved\", \" to\", \" Copenhagen\", \" to\", \" work\", \" at\", \" different\", \" theat\", \"res\", \".\", \" Her\", \" national\", \" breakthrough\", \" came\", \",\", \" when\", \" she\", \"  \", \"accept\", \"ed\", \" the\", \" role\", \" as\", \" El\", \"iza\", \" in\", \" My\", \" Fair\", \" Lady\", \" at\", \" Falk\", \"oner\", \" Te\", \"at\", \"ret\", \" at\", \" short\", \" notice\", \" in\", \" 1960\", \".\"], [\"Ger\", \"da\", \" Gil\", \"bo\", \"e\", \"\\n\", \"\\n\", \"Ger\", \"da\", \" Gil\", \"bo\", \"e\", \" (\", \"5\", \" July\", \" 1914\", \"\\u00a0\", \"\\u2013\", \" 11\", \" April\", \" 2009\", \")\", \" was\", \" a\", \" Danish\", \" actress\", \" and\", \" singer\", \".\", \" She\", \" appeared\", \" in\", \" 18\", \" films\", \" between\", \" 1943\", \" and\", \" 2003\", \".\", \"\\n\", \"\\n\", \"Life\", \" \", \"\\n\", \"Gil\", \"bo\", \"e\", \" was\", \" born\", \" in\", \" 1914\", \".\", \" She\", \" was\", \" the\", \" daughter\", \" of\", \" a\", \" black\", \"smith\", \",\", \" Gil\", \"bo\", \"e\", \" started\", \" her\", \" career\", \" in\", \" musical\", \" theatre\", \" and\", \" oper\", \"as\", \" in\", \" A\", \"ar\", \"hus\", \" before\", \" she\", \" moved\", \" to\", \" Copenhagen\", \" to\", \" work\", \" at\", \" different\", \" theat\", \"res\", \".\"], [\"T\", \"uls\", \"i\", \" G\", \"abb\", \"ard\", \",\", \" 2020\", \" presidential\", \" candidate\", \",\", \" is\", \" closing\", \" in\", \" on\", \" the\", \" number\", \" of\", \" donations\", \" that\", \" she\", \" needs\", \" in\", \" order\", \" to\", \" qualify\", \" to\", \" participate\", \" in\", \" the\", \" Democratic\", \" debates\", \".\", \" As\", \" of\", \" March\", \" 20\", \",\", \" 2019\", \",\", \" she\", \" has\", \" 40\", \",\", \"456\", \" donations\", \",\", \" which\", \" leaves\", \" her\", \" needing\", \" just\", \" a\", \" little\", \" less\", \" than\", \" 25\", \",\", \"000\", \" more\", \" donations\", \" to\"], [\"T\", \"uls\", \"i\", \" G\", \"abb\", \"ard\", \",\", \" 2020\", \" presidential\", \" candidate\", \",\", \" is\", \" closing\", \" in\", \" on\", \" the\", \" number\", \" of\", \" donations\", \" that\", \" she\", \" needs\", \" in\", \" order\", \" to\", \" qualify\", \" to\", \" participate\", \" in\", \" the\", \" Democratic\", \" debates\", \".\", \" As\", \" of\", \" March\", \" 20\", \",\", \" 2019\", \",\", \" she\", \" has\", \" 40\", \",\", \"456\", \" donations\", \",\", \" which\"], [\"Ger\", \"da\", \" Gil\", \"bo\", \"e\", \"\\n\", \"\\n\", \"Ger\", \"da\", \" Gil\", \"bo\", \"e\", \" (\", \"5\", \" July\", \" 1914\", \"\\u00a0\", \"\\u2013\", \" 11\", \" April\", \" 2009\", \")\", \" was\", \" a\", \" Danish\", \" actress\", \" and\", \" singer\", \".\", \" She\", \" appeared\", \" in\", \" 18\", \" films\", \" between\", \" 1943\", \" and\", \" 2003\", \".\", \"\\n\", \"\\n\", \"Life\", \" \", \"\\n\", \"Gil\", \"bo\", \"e\", \" was\", \" born\", \" in\", \" 1914\", \".\", \" She\", \" was\", \" the\", \" daughter\", \" of\", \" a\", \" black\", \"smith\", \",\", \" Gil\", \"bo\", \"e\", \" started\", \" her\", \" career\", \" in\", \" musical\", \" theatre\", \" and\", \" oper\", \"as\", \" in\", \" A\", \"ar\", \"hus\", \" before\", \" she\", \" moved\", \" to\", \" Copenhagen\", \" to\", \" work\", \" at\", \" different\", \" theat\", \"res\", \".\", \" Her\", \" national\", \" breakthrough\", \" came\", \",\", \" when\", \" she\", \"  \", \"accept\", \"ed\", \" the\", \" role\", \" as\", \" El\", \"iza\", \" in\", \" My\", \" Fair\", \" Lady\", \" at\", \" Falk\", \"oner\", \" Te\", \"at\", \"ret\", \" at\", \" short\", \" notice\", \" in\", \" 1960\", \".\", \" Although\"], [\"T\", \"uls\", \"i\", \" G\", \"abb\", \"ard\", \",\", \" 2020\", \" presidential\", \" candidate\", \",\", \" is\", \" closing\", \" in\", \" on\", \" the\", \" number\", \" of\", \" donations\", \" that\", \" she\", \" needs\", \" in\", \" order\", \" to\", \" qualify\", \" to\", \" participate\", \" in\", \" the\", \" Democratic\", \" debates\", \".\", \" As\", \" of\", \" March\", \" 20\", \",\", \" 2019\", \",\", \" she\", \" has\", \" 40\", \",\", \"456\", \" donations\", \",\", \" which\", \" leaves\", \" her\", \" needing\", \" just\", \" a\", \" little\", \" less\", \" than\", \" 25\", \",\", \"000\", \" more\", \" donations\", \" to\", \" participate\", \" in\", \" the\", \" D\", \"NC\", \" debates\", \".\", \" In\", \" total\", \",\", \" as\", \" of\", \" March\", \" 20\", \",\", \" she\", \" needs\", \" 24\", \",\", \"544\", \" more\", \" donations\", \" in\"], [\"T\", \"uls\", \"i\", \" G\", \"abb\", \"ard\", \",\", \" 2020\", \" presidential\", \" candidate\", \",\", \" is\", \" closing\", \" in\", \" on\", \" the\", \" number\", \" of\", \" donations\", \" that\", \" she\", \" needs\", \" in\", \" order\", \" to\", \" qualify\", \" to\", \" participate\", \" in\", \" the\", \" Democratic\", \" debates\", \".\", \" As\", \" of\", \" March\", \" 20\", \",\", \" 2019\", \",\", \" she\", \" has\", \" 40\", \",\", \"456\", \" donations\", \",\", \" which\", \" leaves\", \" her\", \" needing\", \" just\", \" a\", \" little\", \" less\", \" than\", \" 25\", \",\", \"000\", \" more\", \" donations\", \" to\", \" participate\", \" in\", \" the\", \" D\", \"NC\", \" debates\", \".\", \" In\", \" total\", \",\", \" as\", \" of\", \" March\", \" 20\", \",\", \" she\", \" needs\", \" 24\", \",\", \"544\", \" more\", \" donations\", \" in\", \" order\", \" to\", \" qualify\", \" for\", \" the\", \" debates\", \".\"], [\"T\", \"uls\", \"i\", \" G\", \"abb\", \"ard\", \",\", \" 2020\", \" presidential\", \" candidate\", \",\", \" is\", \" closing\", \" in\", \" on\", \" the\", \" number\", \" of\", \" donations\", \" that\", \" she\", \" needs\", \" in\", \" order\", \" to\", \" qualify\", \" to\", \" participate\", \" in\", \" the\", \" Democratic\", \" debates\", \".\", \" As\", \" of\", \" March\", \" 20\", \",\", \" 2019\", \",\", \" she\", \" has\", \" 40\", \",\", \"456\", \" donations\", \",\", \" which\", \" leaves\"], [\"Ger\", \"da\", \" Gil\", \"bo\", \"e\", \"\\n\", \"\\n\", \"Ger\", \"da\", \" Gil\", \"bo\", \"e\", \" (\", \"5\", \" July\", \" 1914\", \"\\u00a0\", \"\\u2013\", \" 11\", \" April\", \" 2009\", \")\", \" was\", \" a\", \" Danish\", \" actress\", \" and\", \" singer\", \".\", \" She\", \" appeared\", \" in\", \" 18\", \" films\", \" between\", \" 1943\", \" and\", \" 2003\", \".\", \"\\n\", \"\\n\", \"Life\", \" \", \"\\n\", \"Gil\", \"bo\", \"e\", \" was\", \" born\", \" in\", \" 1914\", \".\", \" She\", \" was\", \" the\", \" daughter\", \" of\", \" a\", \" black\", \"smith\", \",\", \" Gil\", \"bo\", \"e\", \" started\", \" her\", \" career\", \" in\", \" musical\", \" theatre\", \" and\", \" oper\", \"as\", \" in\", \" A\", \"ar\", \"hus\", \" before\", \" she\", \" moved\", \" to\", \" Copenhagen\", \" to\", \" work\", \" at\", \" different\", \" theat\", \"res\", \".\", \" Her\", \" national\", \" breakthrough\", \" came\", \",\", \" when\"], [\"Ger\", \"da\", \" Gil\", \"bo\", \"e\", \"\\n\", \"\\n\", \"Ger\", \"da\", \" Gil\", \"bo\", \"e\", \" (\", \"5\", \" July\", \" 1914\", \"\\u00a0\", \"\\u2013\", \" 11\", \" April\", \" 2009\", \")\", \" was\", \" a\", \" Danish\", \" actress\", \" and\", \" singer\", \".\", \" She\", \" appeared\", \" in\", \" 18\", \" films\", \" between\", \" 1943\", \" and\", \" 2003\", \".\"], [\"Deb\", \"bie\", \" Gregory\", \" D\", \"NP\", \",\", \" RN\", \"\\n\", \"\\n\", \"Dr\", \".\", \" De\", \"bbie\", \" Gregory\", \" is\", \" a\", \" national\", \" leader\", \" in\", \" healthcare\", \" design\", \",\", \" innovation\", \",\", \" and\", \" transformation\", \".\", \" As\", \" a\", \" nurse\", \" executive\", \" and\", \" interior\", \" designer\", \",\", \" Dr\", \".\", \" Gregory\", \" is\", \" passionate\", \" about\", \" \\u201c\", \"Int\", \"entional\", \" Design\", \"\\u201d\", \" that\", \" align\", \"s\", \" People\", \",\", \" Place\", \",\", \" and\", \" Process\", \".\", \" She\", \" creates\", \" and\", \" transforms\", \" environments\", \" into\", \" functional\", \" ecosystems\", \" using\", \" complex\", \" systems\", \" science\", \" and\", \" strategic\", \" thinking\", \".\"], [\"Jean\", \"ette\", \" Saw\", \"yer\", \" Cohen\", \",\", \" PhD\", \",\", \" clinical\", \" assistant\", \" professor\", \" of\", \" psychology\", \" in\", \" pediatric\", \"s\", \" at\", \" We\", \"ill\", \" Cornell\", \" Medical\", \" College\", \" in\", \" New\", \" York\", \" City\", \"\\n\", \"\\n\", \"P\", \"ediatric\", \" Psych\", \"ologist\", \"\\n\", \"\\n\", \"How\", \" to\", \" Te\", \"ach\", \" Independence\", \"?\", \"\\n\", \"\\n\", \"How\", \" can\", \" I\", \" teach\", \" my\", \" toddler\", \" to\", \" do\", \" things\", \" independently\", \"?\", \"\\n\", \"\\n\", \"You\", \"\\u2019\", \"ve\", \" probably\", \" become\", \" more\", \" patient\", \" since\", \" you\", \" started\", \" this\", \" whole\", \" pa\", \"renthood\", \" thing\", \".\", \" And\", \" you\", \"\\u2019\", \"re\", \" going\", \" to\", \" have\", \" to\", \" practice\", \" patience\", \" even\", \" more\", \" as\", \" your\", \" toddler\", \" learns\", \" to\", \" become\", \" more\", \" independent\", \".\", \"\\n\", \"\\n\", \"For\", \" example\", \",\", \" she\", \" tells\", \" you\", \" she\", \" can\", \"\\u2019\", \"t\", \" finish\", \" the\", \" puzzle\", \" she\", \"\\u2019\", \"s\", \" doing\", \".\", \" Instead\", \" of\", \" jumping\", \" right\", \" in\", \" and\", \" telling\", \" her\", \" which\", \" piece\", \" goes\", \" where\", \",\"], [\"T\", \"uls\", \"i\", \" G\", \"abb\", \"ard\", \",\", \" 2020\", \" presidential\", \" candidate\", \",\", \" is\", \" closing\", \" in\", \" on\", \" the\", \" number\", \" of\", \" donations\", \" that\", \" she\", \" needs\", \" in\", \" order\", \" to\", \" qualify\", \" to\", \" participate\", \" in\", \" the\", \" Democratic\", \" debates\", \".\", \" As\", \" of\", \" March\", \" 20\", \",\", \" 2019\", \",\", \" she\", \" has\", \" 40\", \",\", \"456\", \" donations\", \",\", \" which\", \" leaves\", \" her\", \" needing\", \" just\", \" a\", \" little\", \" less\", \" than\", \" 25\", \",\", \"000\", \" more\", \" donations\", \" to\", \" participate\", \" in\", \" the\", \" D\", \"NC\", \" debates\", \".\", \" In\", \" total\", \",\", \" as\", \" of\", \" March\", \" 20\", \",\", \" she\", \" needs\"], [\"T\", \"uls\", \"i\", \" G\", \"abb\", \"ard\", \",\", \" 2020\", \" presidential\", \" candidate\", \",\", \" is\", \" closing\", \" in\", \" on\", \" the\", \" number\", \" of\", \" donations\", \" that\", \" she\", \" needs\", \" in\", \" order\", \" to\", \" qualify\", \" to\", \" participate\", \" in\", \" the\", \" Democratic\", \" debates\", \".\", \" As\", \" of\", \" March\", \" 20\", \",\", \" 2019\", \",\", \" she\", \" has\", \" 40\", \",\", \"456\", \" donations\", \",\", \" which\", \" leaves\", \" her\", \" needing\", \" just\", \" a\", \" little\", \" less\", \" than\", \" 25\", \",\", \"000\", \" more\", \" donations\", \" to\", \" participate\", \" in\", \" the\", \" D\", \"NC\", \" debates\", \".\", \" In\", \" total\", \",\", \" as\", \" of\", \" March\", \" 20\", \",\", \" she\", \" needs\", \" 24\", \",\", \"544\", \" more\", \" donations\", \" in\", \" order\", \" to\", \" qualify\"], [\"Jean\", \"ette\", \" Saw\", \"yer\", \" Cohen\", \",\", \" PhD\", \",\", \" clinical\", \" assistant\", \" professor\", \" of\", \" psychology\", \" in\", \" pediatric\", \"s\", \" at\", \" We\", \"ill\", \" Cornell\", \" Medical\", \" College\", \" in\", \" New\", \" York\", \" City\", \"\\n\", \"\\n\", \"P\", \"ediatric\", \" Psych\", \"ologist\", \"\\n\", \"\\n\", \"How\", \" to\", \" Te\", \"ach\", \" Independence\", \"?\", \"\\n\", \"\\n\", \"How\", \" can\", \" I\", \" teach\", \" my\", \" toddler\", \" to\", \" do\", \" things\", \" independently\", \"?\", \"\\n\", \"\\n\", \"You\", \"\\u2019\", \"ve\", \" probably\", \" become\", \" more\", \" patient\", \" since\", \" you\", \" started\", \" this\", \" whole\", \" pa\", \"renthood\", \" thing\", \".\", \" And\", \" you\", \"\\u2019\", \"re\", \" going\", \" to\", \" have\", \" to\", \" practice\", \" patience\", \" even\", \" more\", \" as\", \" your\", \" toddler\", \" learns\", \" to\", \" become\", \" more\", \" independent\", \".\", \"\\n\", \"\\n\", \"For\", \" example\", \",\", \" she\", \" tells\", \" you\", \" she\", \" can\", \"\\u2019\", \"t\", \" finish\", \" the\", \" puzzle\", \" she\", \"\\u2019\", \"s\", \" doing\", \".\", \" Instead\", \" of\", \" jumping\", \" right\", \" in\", \" and\", \" telling\", \" her\", \" which\", \" piece\"], [\"Jean\", \"ette\", \" Saw\", \"yer\", \" Cohen\", \",\", \" PhD\", \",\", \" clinical\", \" assistant\", \" professor\", \" of\", \" psychology\", \" in\", \" pediatric\", \"s\", \" at\", \" We\", \"ill\", \" Cornell\", \" Medical\", \" College\", \" in\", \" New\", \" York\", \" City\", \"\\n\", \"\\n\", \"P\", \"ediatric\", \" Psych\", \"ologist\", \"\\n\", \"\\n\", \"How\", \" to\", \" Te\", \"ach\", \" Independence\", \"?\", \"\\n\", \"\\n\", \"How\", \" can\", \" I\", \" teach\", \" my\", \" toddler\", \" to\", \" do\", \" things\", \" independently\", \"?\", \"\\n\", \"\\n\", \"You\", \"\\u2019\", \"ve\", \" probably\", \" become\", \" more\", \" patient\", \" since\", \" you\", \" started\", \" this\", \" whole\", \" pa\", \"renthood\", \" thing\", \".\", \" And\", \" you\", \"\\u2019\", \"re\", \" going\", \" to\", \" have\", \" to\", \" practice\", \" patience\", \" even\", \" more\", \" as\", \" your\", \" toddler\", \" learns\", \" to\", \" become\", \" more\", \" independent\", \".\", \"\\n\", \"\\n\", \"For\", \" example\", \",\", \" she\", \" tells\", \" you\", \" she\", \" can\", \"\\u2019\", \"t\", \" finish\", \" the\", \" puzzle\", \" she\", \"\\u2019\", \"s\", \" doing\", \".\", \" Instead\"], [\"Jean\", \"ette\", \" Saw\", \"yer\", \" Cohen\", \",\", \" PhD\", \",\", \" clinical\", \" assistant\", \" professor\", \" of\", \" psychology\", \" in\", \" pediatric\", \"s\", \" at\", \" We\", \"ill\", \" Cornell\", \" Medical\", \" College\", \" in\", \" New\", \" York\", \" City\", \"\\n\", \"\\n\", \"P\", \"ediatric\", \" Psych\", \"ologist\", \"\\n\", \"\\n\", \"How\", \" to\", \" Te\", \"ach\", \" Independence\", \"?\", \"\\n\", \"\\n\", \"How\", \" can\", \" I\", \" teach\", \" my\", \" toddler\", \" to\", \" do\", \" things\", \" independently\", \"?\", \"\\n\", \"\\n\", \"You\", \"\\u2019\", \"ve\", \" probably\", \" become\", \" more\", \" patient\", \" since\", \" you\", \" started\", \" this\", \" whole\", \" pa\", \"renthood\", \" thing\", \".\", \" And\", \" you\", \"\\u2019\", \"re\", \" going\", \" to\", \" have\", \" to\", \" practice\", \" patience\", \" even\", \" more\", \" as\", \" your\", \" toddler\", \" learns\", \" to\", \" become\", \" more\", \" independent\", \".\", \"\\n\", \"\\n\", \"For\", \" example\", \",\", \" she\", \" tells\", \" you\", \" she\", \" can\", \"\\u2019\", \"t\", \" finish\", \" the\", \" puzzle\", \" she\", \"\\u2019\", \"s\", \" doing\", \".\", \" Instead\", \" of\", \" jumping\", \" right\", \" in\", \" and\", \" telling\"], [\"T\", \"uls\", \"i\", \" G\", \"abb\", \"ard\", \",\", \" 2020\", \" presidential\", \" candidate\", \",\", \" is\", \" closing\", \" in\", \" on\", \" the\", \" number\", \" of\", \" donations\", \" that\", \" she\", \" needs\", \" in\", \" order\", \" to\", \" qualify\", \" to\", \" participate\", \" in\", \" the\", \" Democratic\", \" debates\", \".\", \" As\", \" of\", \" March\", \" 20\", \",\", \" 2019\", \",\", \" she\", \" has\", \" 40\", \",\", \"456\", \" donations\", \",\", \" which\", \" leaves\", \" her\", \" needing\", \" just\", \" a\", \" little\", \" less\", \" than\", \" 25\", \",\", \"000\", \" more\", \" donations\", \" to\", \" participate\", \" in\", \" the\", \" D\", \"NC\", \" debates\", \".\"], [\"Jean\", \"ette\", \" Saw\", \"yer\", \" Cohen\", \",\", \" PhD\", \",\", \" clinical\", \" assistant\", \" professor\", \" of\", \" psychology\", \" in\", \" pediatric\", \"s\", \" at\", \" We\", \"ill\", \" Cornell\", \" Medical\", \" College\", \" in\", \" New\", \" York\", \" City\", \"\\n\", \"\\n\", \"P\", \"ediatric\", \" Psych\", \"ologist\", \"\\n\", \"\\n\", \"How\", \" to\", \" Te\", \"ach\", \" Independence\", \"?\", \"\\n\", \"\\n\", \"How\", \" can\", \" I\", \" teach\", \" my\", \" toddler\", \" to\", \" do\", \" things\", \" independently\", \"?\", \"\\n\", \"\\n\", \"You\", \"\\u2019\", \"ve\", \" probably\", \" become\", \" more\", \" patient\", \" since\", \" you\", \" started\", \" this\", \" whole\", \" pa\", \"renthood\", \" thing\", \".\", \" And\", \" you\", \"\\u2019\", \"re\", \" going\", \" to\", \" have\", \" to\", \" practice\", \" patience\", \" even\", \" more\", \" as\", \" your\", \" toddler\", \" learns\", \" to\", \" become\", \" more\", \" independent\", \".\", \"\\n\", \"\\n\", \"For\", \" example\", \",\", \" she\", \" tells\", \" you\", \" she\", \" can\", \"\\u2019\", \"t\", \" finish\", \" the\", \" puzzle\", \" she\", \"\\u2019\", \"s\", \" doing\", \".\", \" Instead\", \" of\"], [\"Jean\", \"ette\", \" Saw\", \"yer\", \" Cohen\", \",\", \" PhD\", \",\", \" clinical\", \" assistant\", \" professor\", \" of\", \" psychology\", \" in\", \" pediatric\", \"s\", \" at\", \" We\", \"ill\", \" Cornell\", \" Medical\", \" College\", \" in\", \" New\", \" York\", \" City\", \"\\n\", \"\\n\", \"P\", \"ediatric\", \" Psych\", \"ologist\", \"\\n\", \"\\n\", \"How\", \" to\", \" Te\", \"ach\", \" Independence\", \"?\", \"\\n\", \"\\n\", \"How\", \" can\", \" I\", \" teach\", \" my\", \" toddler\", \" to\", \" do\", \" things\", \" independently\", \"?\", \"\\n\", \"\\n\", \"You\", \"\\u2019\", \"ve\", \" probably\", \" become\", \" more\", \" patient\", \" since\", \" you\", \" started\", \" this\", \" whole\", \" pa\", \"renthood\", \" thing\", \".\", \" And\", \" you\", \"\\u2019\", \"re\", \" going\", \" to\", \" have\", \" to\", \" practice\", \" patience\", \" even\", \" more\", \" as\", \" your\", \" toddler\", \" learns\", \" to\", \" become\", \" more\", \" independent\", \".\", \"\\n\", \"\\n\", \"For\", \" example\", \",\", \" she\", \" tells\", \" you\", \" she\", \" can\", \"\\u2019\", \"t\", \" finish\", \" the\", \" puzzle\", \" she\", \"\\u2019\", \"s\", \" doing\", \".\"], [\"T\", \"uls\", \"i\", \" G\", \"abb\", \"ard\", \",\", \" 2020\", \" presidential\", \" candidate\", \",\", \" is\", \" closing\", \" in\", \" on\", \" the\", \" number\", \" of\", \" donations\", \" that\", \" she\", \" needs\", \" in\", \" order\", \" to\", \" qualify\", \" to\", \" participate\", \" in\", \" the\", \" Democratic\", \" debates\", \".\", \" As\", \" of\", \" March\", \" 20\", \",\", \" 2019\", \",\", \" she\", \" has\", \" 40\", \",\", \"456\", \" donations\", \",\", \" which\", \" leaves\", \" her\", \" needing\", \" just\", \" a\", \" little\", \" less\", \" than\", \" 25\", \",\", \"000\", \" more\", \" donations\", \" to\", \" participate\", \" in\", \" the\", \" D\", \"NC\", \" debates\", \".\", \" In\", \" total\", \",\", \" as\", \" of\", \" March\", \" 20\", \",\", \" she\", \" needs\", \" 24\", \",\", \"544\", \" more\", \" donations\", \" in\", \" order\", \" to\", \" qualify\", \" for\", \" the\", \" debates\", \".\", \"\\n\", \"\\n\", \"In\", \" a\", \" phone\", \" call\", \" with\"], [\"T\", \"uls\", \"i\", \" G\", \"abb\", \"ard\", \",\", \" 2020\", \" presidential\", \" candidate\", \",\", \" is\", \" closing\", \" in\", \" on\", \" the\", \" number\", \" of\", \" donations\", \" that\", \" she\", \" needs\", \" in\", \" order\", \" to\", \" qualify\", \" to\", \" participate\", \" in\", \" the\", \" Democratic\", \" debates\", \".\", \" As\", \" of\", \" March\", \" 20\", \",\", \" 2019\", \",\", \" she\", \" has\", \" 40\", \",\", \"456\", \" donations\", \",\", \" which\", \" leaves\", \" her\", \" needing\", \" just\", \" a\", \" little\", \" less\", \" than\", \" 25\", \",\", \"000\", \" more\", \" donations\", \" to\", \" participate\", \" in\", \" the\", \" D\", \"NC\", \" debates\", \".\", \" In\", \" total\", \",\", \" as\", \" of\", \" March\", \" 20\", \",\", \" she\", \" needs\", \" 24\", \",\", \"544\", \" more\", \" donations\", \" in\", \" order\", \" to\", \" qualify\", \" for\", \" the\", \" debates\", \".\", \"\\n\", \"\\n\"], [\"T\", \"uls\", \"i\", \" G\", \"abb\", \"ard\", \",\", \" 2020\", \" presidential\", \" candidate\", \",\", \" is\", \" closing\", \" in\", \" on\", \" the\", \" number\", \" of\", \" donations\", \" that\", \" she\", \" needs\", \" in\", \" order\", \" to\", \" qualify\", \" to\", \" participate\", \" in\", \" the\", \" Democratic\", \" debates\", \".\", \" As\", \" of\", \" March\", \" 20\", \",\", \" 2019\", \",\", \" she\", \" has\", \" 40\", \",\", \"456\", \" donations\", \",\", \" which\", \" leaves\", \" her\", \" needing\", \" just\", \" a\", \" little\", \" less\", \" than\", \" 25\", \",\", \"000\", \" more\", \" donations\", \" to\", \" participate\", \" in\", \" the\", \" D\", \"NC\", \" debates\", \".\", \" In\", \" total\", \",\", \" as\"], [\"T\", \"uls\", \"i\", \" G\", \"abb\", \"ard\", \",\", \" 2020\", \" presidential\", \" candidate\", \",\", \" is\", \" closing\", \" in\", \" on\", \" the\", \" number\", \" of\", \" donations\", \" that\", \" she\", \" needs\", \" in\", \" order\", \" to\", \" qualify\", \" to\", \" participate\", \" in\", \" the\", \" Democratic\", \" debates\", \".\", \" As\", \" of\", \" March\", \" 20\", \",\", \" 2019\", \",\", \" she\", \" has\", \" 40\", \",\", \"456\", \" donations\", \",\", \" which\", \" leaves\", \" her\", \" needing\", \" just\", \" a\", \" little\", \" less\", \" than\", \" 25\", \",\", \"000\", \" more\", \" donations\", \" to\", \" participate\", \" in\", \" the\", \" D\", \"NC\", \" debates\", \".\", \" In\", \" total\", \",\", \" as\", \" of\", \" March\", \" 20\", \",\", \" she\", \" needs\", \" 24\", \",\", \"544\", \" more\", \" donations\", \" in\", \" order\", \" to\", \" qualify\", \" for\", \" the\", \" debates\", \".\", \"\\n\", \"\\n\", \"In\", \" a\", \" phone\", \" call\", \" with\", \" volunteers\", \" on\", \" March\", \" 18\", \",\"], [\"T\", \"uls\", \"i\", \" G\", \"abb\", \"ard\", \",\", \" 2020\", \" presidential\", \" candidate\", \",\", \" is\", \" closing\", \" in\", \" on\", \" the\", \" number\", \" of\", \" donations\", \" that\", \" she\", \" needs\", \" in\", \" order\", \" to\", \" qualify\", \" to\", \" participate\", \" in\", \" the\", \" Democratic\", \" debates\", \".\", \" As\", \" of\", \" March\", \" 20\", \",\", \" 2019\", \",\", \" she\", \" has\", \" 40\", \",\", \"456\", \" donations\", \",\", \" which\", \" leaves\", \" her\", \" needing\", \" just\", \" a\", \" little\", \" less\", \" than\", \" 25\", \",\", \"000\", \" more\", \" donations\", \" to\", \" participate\", \" in\", \" the\", \" D\", \"NC\", \" debates\", \".\", \" In\", \" total\"], [\"T\", \"uls\", \"i\", \" G\", \"abb\", \"ard\", \",\", \" 2020\", \" presidential\", \" candidate\", \",\", \" is\", \" closing\", \" in\", \" on\", \" the\", \" number\", \" of\", \" donations\", \" that\", \" she\", \" needs\", \" in\", \" order\", \" to\", \" qualify\", \" to\", \" participate\", \" in\", \" the\", \" Democratic\", \" debates\", \".\", \" As\", \" of\", \" March\", \" 20\", \",\", \" 2019\", \",\", \" she\", \" has\", \" 40\", \",\", \"456\", \" donations\", \",\", \" which\", \" leaves\", \" her\", \" needing\", \" just\", \" a\", \" little\", \" less\", \" than\", \" 25\", \",\", \"000\", \" more\", \" donations\"], [\"Jean\", \"ette\", \" Saw\", \"yer\", \" Cohen\", \",\", \" PhD\", \",\", \" clinical\", \" assistant\", \" professor\", \" of\", \" psychology\", \" in\", \" pediatric\", \"s\", \" at\", \" We\", \"ill\", \" Cornell\", \" Medical\", \" College\", \" in\", \" New\", \" York\", \" City\", \"\\n\", \"\\n\", \"P\", \"ediatric\", \" Psych\", \"ologist\", \"\\n\", \"\\n\", \"How\", \" to\", \" Te\", \"ach\", \" Independence\", \"?\", \"\\n\", \"\\n\", \"How\", \" can\", \" I\", \" teach\", \" my\", \" toddler\", \" to\", \" do\", \" things\", \" independently\", \"?\", \"\\n\", \"\\n\", \"You\", \"\\u2019\", \"ve\", \" probably\", \" become\", \" more\", \" patient\", \" since\", \" you\", \" started\", \" this\", \" whole\", \" pa\", \"renthood\", \" thing\", \".\", \" And\", \" you\", \"\\u2019\", \"re\", \" going\", \" to\", \" have\", \" to\", \" practice\", \" patience\", \" even\", \" more\", \" as\", \" your\", \" toddler\", \" learns\", \" to\", \" become\", \" more\", \" independent\", \".\", \"\\n\", \"\\n\", \"For\", \" example\", \",\", \" she\", \" tells\"], [\"T\", \"uls\", \"i\", \" G\", \"abb\", \"ard\", \",\", \" 2020\", \" presidential\", \" candidate\", \",\", \" is\", \" closing\", \" in\", \" on\", \" the\", \" number\", \" of\", \" donations\", \" that\", \" she\", \" needs\", \" in\", \" order\", \" to\", \" qualify\", \" to\", \" participate\", \" in\", \" the\", \" Democratic\", \" debates\", \".\", \" As\", \" of\", \" March\", \" 20\", \",\", \" 2019\", \",\", \" she\", \" has\", \" 40\", \",\", \"456\", \" donations\", \",\", \" which\", \" leaves\", \" her\", \" needing\", \" just\", \" a\", \" little\", \" less\", \" than\", \" 25\", \",\", \"000\", \" more\", \" donations\", \" to\", \" participate\", \" in\", \" the\", \" D\", \"NC\", \" debates\", \".\", \" In\", \" total\", \",\", \" as\", \" of\", \" March\", \" 20\", \",\", \" she\", \" needs\", \" 24\", \",\", \"544\", \" more\", \" donations\", \" in\", \" order\", \" to\", \" qualify\", \" for\", \" the\", \" debates\", \".\", \"\\n\", \"\\n\", \"In\", \" a\", \" phone\", \" call\"], [\"Jean\", \"ette\", \" Saw\", \"yer\", \" Cohen\", \",\", \" PhD\", \",\", \" clinical\", \" assistant\", \" professor\", \" of\", \" psychology\", \" in\", \" pediatric\", \"s\", \" at\", \" We\", \"ill\", \" Cornell\", \" Medical\", \" College\", \" in\", \" New\", \" York\", \" City\", \"\\n\", \"\\n\", \"P\", \"ediatric\", \" Psych\", \"ologist\", \"\\n\", \"\\n\", \"How\", \" to\", \" Te\", \"ach\", \" Independence\", \"?\", \"\\n\", \"\\n\", \"How\", \" can\", \" I\", \" teach\", \" my\", \" toddler\", \" to\", \" do\", \" things\", \" independently\", \"?\", \"\\n\", \"\\n\", \"You\", \"\\u2019\", \"ve\", \" probably\", \" become\", \" more\", \" patient\", \" since\", \" you\", \" started\", \" this\", \" whole\", \" pa\", \"renthood\", \" thing\", \".\", \" And\", \" you\", \"\\u2019\", \"re\", \" going\", \" to\", \" have\", \" to\", \" practice\", \" patience\", \" even\", \" more\", \" as\", \" your\", \" toddler\", \" learns\", \" to\", \" become\", \" more\", \" independent\", \".\", \"\\n\", \"\\n\", \"For\", \" example\", \",\", \" she\", \" tells\", \" you\", \" she\", \" can\", \"\\u2019\", \"t\", \" finish\", \" the\", \" puzzle\", \" she\", \"\\u2019\", \"s\", \" doing\", \".\", \" Instead\", \" of\", \" jumping\", \" right\", \" in\", \" and\", \" telling\", \" her\", \" which\", \" piece\", \" goes\", \" where\"]], \"activations\": [[[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.06347006559371948]], [[0.0]], [[1.802854299545288]], [[1.8749361038208008]], [[0.4218263626098633]], [[1.3782384395599365]], [[1.7601149082183838]], [[1.2460565567016602]], [[1.0286014080047607]], [[2.0453052520751953]], [[0.2444269061088562]], [[0.0]], [[0.5851564407348633]], [[0.9765392541885376]], [[1.5294780731201172]], [[1.2176717519760132]], [[0.5375726222991943]], [[0.9212256669998169]], [[1.4400856494903564]], [[0.9198911190032959]], [[2.0164506435394287]], [[0.6767697334289551]], [[1.611403465270996]], [[0.689254641532898]], [[0.9989840984344482]], [[0.7477866411209106]], [[1.4163498878479004]], [[2.2330617904663086]], [[3.3266074657440186]], [[3.1683294773101807]], [[1.0645406246185303]], [[2.205601930618286]], [[1.479196548461914]], [[0.8417621850967407]], [[0.37722790241241455]], [[0.5752818584442139]], [[1.6664834022521973]], [[0.044951021671295166]], [[0.6719822883605957]], [[0.5143978595733643]], [[0.5707625150680542]], [[2.452650308609009]], [[3.3323323726654053]], [[1.6675353050231934]], [[1.6783554553985596]], [[0.7006189823150635]], [[0.0]], [[0.3579052686691284]], [[1.0309221744537354]], [[2.766800880432129]], [[2.347296714782715]], [[2.607029914855957]], [[3.462661027908325]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.19252455234527588]], [[1.5357298851013184]], [[0.11711913347244263]], [[0.7301456928253174]], [[0.6620317697525024]], [[1.6288032531738281]], [[1.7002463340759277]], [[0.33100950717926025]], [[1.5263159275054932]], [[2.2165164947509766]], [[0.5544388294219971]], [[0.16506034135818481]], [[1.2183122634887695]], [[3.0259435176849365]], [[2.038299322128296]], [[1.5671467781066895]], [[0.29698920249938965]], [[0.35701608657836914]], [[1.1949892044067383]], [[0.0]], [[0.0]], [[0.0]], [[0.6630306243896484]], [[0.1992517113685608]], [[0.5563148260116577]], [[0.18784064054489136]], [[1.6920790672302246]], [[0.5608751773834229]], [[0.7957826852798462]], [[0.8173302412033081]], [[0.6153416633605957]], [[1.5441951751708984]], [[0.48517048358917236]], [[0.0]], [[0.9063986539840698]], [[1.1922430992126465]], [[0.0]], [[0.0]], [[0.1374339461326599]], [[1.4789707660675049]], [[0.7796556949615479]], [[1.162060022354126]], [[1.072366714477539]], [[0.49346923828125]], [[0.5654232501983643]], [[1.9741647243499756]], [[0.0]], [[0.818589448928833]], [[1.0906678438186646]], [[0.0]], [[0.0]], [[0.009081423282623291]], [[2.237544536590576]], [[0.7942969799041748]], [[1.4353389739990234]], [[1.2263027429580688]], [[0.8989136219024658]], [[1.5806267261505127]], [[1.7955222129821777]], [[1.2940897941589355]], [[0.4295459985733032]], [[0.04721909761428833]], [[1.2235300540924072]], [[3.381244421005249]], [[1.433750867843628]], [[0.9269771575927734]], [[1.2032521963119507]], [[1.4076807498931885]], [[2.23384690284729]], [[3.0321080684661865]], [[0.7829387187957764]], [[1.6960620880126953]], [[1.889017105102539]], [[2.3741543292999268]], [[1.2542074918746948]], [[1.637669324874878]], [[1.648871898651123]], [[0.0]], [[1.3016449213027954]], [[2.3480308055877686]], [[0.4468945264816284]], [[0.0]], [[1.6427953243255615]], [[1.5494005680084229]], [[0.29579657316207886]], [[1.6793525218963623]], [[0.13978898525238037]], [[0.04718589782714844]], [[1.516148567199707]], [[1.508683681488037]], [[0.44605910778045654]], [[2.185490608215332]], [[1.9890217781066895]], [[1.7827792167663574]], [[3.429654836654663]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.19252455234527588]], [[1.5357298851013184]], [[0.11711913347244263]], [[0.7301456928253174]], [[0.6620317697525024]], [[1.6288032531738281]], [[1.7002463340759277]], [[0.33100950717926025]], [[1.5263159275054932]], [[2.2165164947509766]], [[0.5544388294219971]], [[0.16506034135818481]], [[1.2183122634887695]], [[3.0259435176849365]], [[2.038299322128296]], [[1.5671467781066895]], [[0.29698920249938965]], [[0.35701608657836914]], [[1.1949892044067383]], [[0.0]], [[0.0]], [[0.0]], [[0.6630306243896484]], [[0.1992517113685608]], [[0.5563148260116577]], [[0.18784064054489136]], [[1.6920790672302246]], [[0.5608751773834229]], [[0.7957826852798462]], [[0.8173302412033081]], [[0.6153416633605957]], [[1.5441951751708984]], [[0.48517048358917236]], [[0.0]], [[0.9063986539840698]], [[1.1922430992126465]], [[0.0]], [[0.0]], [[0.1374339461326599]], [[1.4789707660675049]], [[0.7796556949615479]], [[1.162060022354126]], [[1.072366714477539]], [[0.49346923828125]], [[0.5654232501983643]], [[1.9741647243499756]], [[0.0]], [[0.818589448928833]], [[1.0906678438186646]], [[0.0]], [[0.0]], [[0.009081423282623291]], [[2.237544536590576]], [[0.7942969799041748]], [[1.4353389739990234]], [[1.2263027429580688]], [[0.8989136219024658]], [[1.5806267261505127]], [[1.7955222129821777]], [[1.2940897941589355]], [[0.4295459985733032]], [[0.04721909761428833]], [[1.2235300540924072]], [[3.381244421005249]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.06347006559371948]], [[0.0]], [[1.802854299545288]], [[1.8749361038208008]], [[0.4218263626098633]], [[1.3782384395599365]], [[1.7601149082183838]], [[1.2460565567016602]], [[1.0286014080047607]], [[2.0453052520751953]], [[0.2444269061088562]], [[0.0]], [[0.5851564407348633]], [[0.9765392541885376]], [[1.5294780731201172]], [[1.2176717519760132]], [[0.5375726222991943]], [[0.9212256669998169]], [[1.4400856494903564]], [[0.9198911190032959]], [[2.0164506435394287]], [[0.6767697334289551]], [[1.611403465270996]], [[0.689254641532898]], [[0.9989840984344482]], [[0.7477866411209106]], [[1.4163498878479004]], [[2.2330617904663086]], [[3.3266074657440186]], [[3.1683294773101807]], [[1.0645406246185303]], [[2.205601930618286]], [[1.479196548461914]], [[0.8417621850967407]], [[0.37722790241241455]], [[0.5752818584442139]], [[1.6664834022521973]], [[0.044951021671295166]], [[0.6719822883605957]], [[0.5143978595733643]], [[0.5707625150680542]], [[2.452650308609009]], [[3.3323323726654053]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.06347006559371948]], [[0.0]], [[1.802854299545288]], [[1.8749361038208008]], [[0.4218263626098633]], [[1.3782384395599365]], [[1.7601149082183838]], [[1.2460565567016602]], [[1.0286014080047607]], [[2.0453052520751953]], [[0.2444269061088562]], [[0.0]], [[0.5851564407348633]], [[0.9765392541885376]], [[1.5294780731201172]], [[1.2176717519760132]], [[0.5375726222991943]], [[0.9212256669998169]], [[1.4400856494903564]], [[0.9198911190032959]], [[2.0164506435394287]], [[0.6767697334289551]], [[1.611403465270996]], [[0.689254641532898]], [[0.9989840984344482]], [[0.7477866411209106]], [[1.4163498878479004]], [[2.2330617904663086]], [[3.3266074657440186]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.19252455234527588]], [[1.5357298851013184]], [[0.11711913347244263]], [[0.7301456928253174]], [[0.6620317697525024]], [[1.6288032531738281]], [[1.7002463340759277]], [[0.33100950717926025]], [[1.5263159275054932]], [[2.2165164947509766]], [[0.5544388294219971]], [[0.16506034135818481]], [[1.2183122634887695]], [[3.0259435176849365]], [[2.038299322128296]], [[1.5671467781066895]], [[0.29698920249938965]], [[0.35701608657836914]], [[1.1949892044067383]], [[0.0]], [[0.0]], [[0.0]], [[0.6630306243896484]], [[0.1992517113685608]], [[0.5563148260116577]], [[0.18784064054489136]], [[1.6920790672302246]], [[0.5608751773834229]], [[0.7957826852798462]], [[0.8173302412033081]], [[0.6153416633605957]], [[1.5441951751708984]], [[0.48517048358917236]], [[0.0]], [[0.9063986539840698]], [[1.1922430992126465]], [[0.0]], [[0.0]], [[0.1374339461326599]], [[1.4789707660675049]], [[0.7796556949615479]], [[1.162060022354126]], [[1.072366714477539]], [[0.49346923828125]], [[0.5654232501983643]], [[1.9741647243499756]], [[0.0]], [[0.818589448928833]], [[1.0906678438186646]], [[0.0]], [[0.0]], [[0.009081423282623291]], [[2.237544536590576]], [[0.7942969799041748]], [[1.4353389739990234]], [[1.2263027429580688]], [[0.8989136219024658]], [[1.5806267261505127]], [[1.7955222129821777]], [[1.2940897941589355]], [[0.4295459985733032]], [[0.04721909761428833]], [[1.2235300540924072]], [[3.381244421005249]], [[1.433750867843628]], [[0.9269771575927734]], [[1.2032521963119507]], [[1.4076807498931885]], [[2.23384690284729]], [[3.0321080684661865]], [[0.7829387187957764]], [[1.6960620880126953]], [[1.889017105102539]], [[2.3741543292999268]], [[1.2542074918746948]], [[1.637669324874878]], [[1.648871898651123]], [[0.0]], [[1.3016449213027954]], [[2.3480308055877686]], [[0.4468945264816284]], [[0.0]], [[1.6427953243255615]], [[1.5494005680084229]], [[0.29579657316207886]], [[1.6793525218963623]], [[0.13978898525238037]], [[0.04718589782714844]], [[1.516148567199707]], [[1.508683681488037]], [[0.44605910778045654]], [[2.185490608215332]], [[1.9890217781066895]], [[1.7827792167663574]], [[3.429654836654663]], [[3.2189977169036865]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.06347006559371948]], [[0.0]], [[1.802854299545288]], [[1.8749361038208008]], [[0.4218263626098633]], [[1.3782384395599365]], [[1.7601149082183838]], [[1.2460565567016602]], [[1.0286014080047607]], [[2.0453052520751953]], [[0.2444269061088562]], [[0.0]], [[0.5851564407348633]], [[0.9765392541885376]], [[1.5294780731201172]], [[1.2176717519760132]], [[0.5375726222991943]], [[0.9212256669998169]], [[1.4400856494903564]], [[0.9198911190032959]], [[2.0164506435394287]], [[0.6767697334289551]], [[1.611403465270996]], [[0.689254641532898]], [[0.9989840984344482]], [[0.7477866411209106]], [[1.4163498878479004]], [[2.2330617904663086]], [[3.3266074657440186]], [[3.1683294773101807]], [[1.0645406246185303]], [[2.205601930618286]], [[1.479196548461914]], [[0.8417621850967407]], [[0.37722790241241455]], [[0.5752818584442139]], [[1.6664834022521973]], [[0.044951021671295166]], [[0.6719822883605957]], [[0.5143978595733643]], [[0.5707625150680542]], [[2.452650308609009]], [[3.3323323726654053]], [[1.6675353050231934]], [[1.6783554553985596]], [[0.7006189823150635]], [[0.0]], [[0.3579052686691284]], [[1.0309221744537354]], [[2.766800880432129]], [[2.347296714782715]], [[2.607029914855957]], [[3.462661027908325]], [[2.6269211769104004]], [[1.1981770992279053]], [[0.0]], [[1.785571813583374]], [[1.6312081813812256]], [[1.7764215469360352]], [[2.966609239578247]], [[1.575878620147705]], [[0.5335711240768433]], [[0.6071090698242188]], [[1.0149058103561401]], [[2.1140263080596924]], [[3.190859794616699]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.06347006559371948]], [[0.0]], [[1.802854299545288]], [[1.8749361038208008]], [[0.4218263626098633]], [[1.3782384395599365]], [[1.7601149082183838]], [[1.2460565567016602]], [[1.0286014080047607]], [[2.0453052520751953]], [[0.2444269061088562]], [[0.0]], [[0.5851564407348633]], [[0.9765392541885376]], [[1.5294780731201172]], [[1.2176717519760132]], [[0.5375726222991943]], [[0.9212256669998169]], [[1.4400856494903564]], [[0.9198911190032959]], [[2.0164506435394287]], [[0.6767697334289551]], [[1.611403465270996]], [[0.689254641532898]], [[0.9989840984344482]], [[0.7477866411209106]], [[1.4163498878479004]], [[2.2330617904663086]], [[3.3266074657440186]], [[3.1683294773101807]], [[1.0645406246185303]], [[2.205601930618286]], [[1.479196548461914]], [[0.8417621850967407]], [[0.37722790241241455]], [[0.5752818584442139]], [[1.6664834022521973]], [[0.044951021671295166]], [[0.6719822883605957]], [[0.5143978595733643]], [[0.5707625150680542]], [[2.452650308609009]], [[3.3323323726654053]], [[1.6675353050231934]], [[1.6783554553985596]], [[0.7006189823150635]], [[0.0]], [[0.3579052686691284]], [[1.0309221744537354]], [[2.766800880432129]], [[2.347296714782715]], [[2.607029914855957]], [[3.462661027908325]], [[2.6269211769104004]], [[1.1981770992279053]], [[0.0]], [[1.785571813583374]], [[1.6312081813812256]], [[1.7764215469360352]], [[2.966609239578247]], [[1.575878620147705]], [[0.5335711240768433]], [[0.6071090698242188]], [[1.0149058103561401]], [[2.1140263080596924]], [[3.190859794616699]], [[0.3823971748352051]], [[1.530935525894165]], [[2.917895793914795]], [[2.081772804260254]], [[1.1767274141311646]], [[1.7527835369110107]], [[3.1707708835601807]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.06347006559371948]], [[0.0]], [[1.802854299545288]], [[1.8749361038208008]], [[0.4218263626098633]], [[1.3782384395599365]], [[1.7601149082183838]], [[1.2460565567016602]], [[1.0286014080047607]], [[2.0453052520751953]], [[0.2444269061088562]], [[0.0]], [[0.5851564407348633]], [[0.9765392541885376]], [[1.5294780731201172]], [[1.2176717519760132]], [[0.5375726222991943]], [[0.9212256669998169]], [[1.4400856494903564]], [[0.9198911190032959]], [[2.0164506435394287]], [[0.6767697334289551]], [[1.611403465270996]], [[0.689254641532898]], [[0.9989840984344482]], [[0.7477866411209106]], [[1.4163498878479004]], [[2.2330617904663086]], [[3.3266074657440186]], [[3.1683294773101807]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.19252455234527588]], [[1.5357298851013184]], [[0.11711913347244263]], [[0.7301456928253174]], [[0.6620317697525024]], [[1.6288032531738281]], [[1.7002463340759277]], [[0.33100950717926025]], [[1.5263159275054932]], [[2.2165164947509766]], [[0.5544388294219971]], [[0.16506034135818481]], [[1.2183122634887695]], [[3.0259435176849365]], [[2.038299322128296]], [[1.5671467781066895]], [[0.29698920249938965]], [[0.35701608657836914]], [[1.1949892044067383]], [[0.0]], [[0.0]], [[0.0]], [[0.6630306243896484]], [[0.1992517113685608]], [[0.5563148260116577]], [[0.18784064054489136]], [[1.6920790672302246]], [[0.5608751773834229]], [[0.7957826852798462]], [[0.8173302412033081]], [[0.6153416633605957]], [[1.5441951751708984]], [[0.48517048358917236]], [[0.0]], [[0.9063986539840698]], [[1.1922430992126465]], [[0.0]], [[0.0]], [[0.1374339461326599]], [[1.4789707660675049]], [[0.7796556949615479]], [[1.162060022354126]], [[1.072366714477539]], [[0.49346923828125]], [[0.5654232501983643]], [[1.9741647243499756]], [[0.0]], [[0.818589448928833]], [[1.0906678438186646]], [[0.0]], [[0.0]], [[0.009081423282623291]], [[2.237544536590576]], [[0.7942969799041748]], [[1.4353389739990234]], [[1.2263027429580688]], [[0.8989136219024658]], [[1.5806267261505127]], [[1.7955222129821777]], [[1.2940897941589355]], [[0.4295459985733032]], [[0.04721909761428833]], [[1.2235300540924072]], [[3.381244421005249]], [[1.433750867843628]], [[0.9269771575927734]], [[1.2032521963119507]], [[1.4076807498931885]], [[2.23384690284729]], [[3.0321080684661865]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.19252455234527588]], [[1.5357298851013184]], [[0.11711913347244263]], [[0.7301456928253174]], [[0.6620317697525024]], [[1.6288032531738281]], [[1.7002463340759277]], [[0.33100950717926025]], [[1.5263159275054932]], [[2.2165164947509766]], [[0.5544388294219971]], [[0.16506034135818481]], [[1.2183122634887695]], [[3.0259435176849365]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.11318778991699219]], [[0.0]], [[0.14594411849975586]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.45492684841156006]], [[0.2932857275009155]], [[0.36901211738586426]], [[0.0]], [[0.4095134735107422]], [[0.0]], [[0.0629931092262268]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.9563347101211548]], [[0.8930401802062988]], [[0.22011590003967285]], [[0.5122741460800171]], [[0.7176159620285034]], [[1.1254106760025024]], [[0.0]], [[0.4252878427505493]], [[1.1200542449951172]], [[0.0]], [[0.3610774278640747]], [[0.06579703092575073]], [[0.7249469757080078]], [[0.39098072052001953]], [[0.9034920930862427]], [[0.13902240991592407]], [[0.0]], [[0.0]], [[0.0]], [[0.254102885723114]], [[0.3051215410232544]], [[0.5698531866073608]], [[0.449138879776001]], [[0.0]], [[0.0]], [[0.0]], [[0.25574564933776855]], [[0.0]], [[0.0]], [[0.4365602731704712]], [[0.4073362350463867]], [[0.91033935546875]], [[0.4250760078430176]], [[0.789321780204773]], [[0.4907146692276001]], [[0.6711746454238892]], [[0.0]], [[0.6012779474258423]], [[1.5645813941955566]], [[0.029314637184143066]], [[0.5427138805389404]], [[0.770331859588623]], [[1.4657483100891113]], [[0.11434590816497803]], [[0.8383995294570923]], [[3.002575397491455]]], [[[0.0]], [[0.15370863676071167]], [[0.4303387403488159]], [[0.7042974233627319]], [[0.0]], [[0.2045365571975708]], [[0.0]], [[0.15135014057159424]], [[0.047507286071777344]], [[0.3375891447067261]], [[0.3439897298812866]], [[0.477217435836792]], [[0.2047526240348816]], [[0.8729926347732544]], [[0.0]], [[0.4485386610031128]], [[0.07710707187652588]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.23235493898391724]], [[0.0]], [[0.0]], [[0.4675610065460205]], [[0.02217763662338257]], [[0.2881569266319275]], [[0.2720351219177246]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.17077898979187012]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.03359323740005493]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.1973302960395813]], [[0.0]], [[0.027334213256835938]], [[0.1089557409286499]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.39060115814208984]], [[0.20470088720321655]], [[0.3241773843765259]], [[0.0]], [[0.03818768262863159]], [[0.010790586471557617]], [[0.0]], [[0.0]], [[0.08270227909088135]], [[0.1549307107925415]], [[0.11284643411636353]], [[0.22594016790390015]], [[2.443530559539795]], [[1.0439453125]], [[0.6924594640731812]], [[1.3123409748077393]], [[0.06080728769302368]], [[1.4182922840118408]], [[2.0870466232299805]], [[0.49309539794921875]], [[1.1596356630325317]], [[1.559216022491455]], [[0.8559755086898804]], [[0.9577215909957886]], [[1.2363677024841309]], [[2.6973049640655518]], [[2.880769729614258]], [[2.762855291366577]], [[1.4470441341400146]], [[0.8177244663238525]], [[0.7882870435714722]], [[1.922593593597412]], [[2.8359341621398926]], [[1.509955883026123]], [[2.0163373947143555]], [[2.8998165130615234]], [[1.8599400520324707]], [[2.381211996078491]], [[2.9821057319641113]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.06347006559371948]], [[0.0]], [[1.802854299545288]], [[1.8749361038208008]], [[0.4218263626098633]], [[1.3782384395599365]], [[1.7601149082183838]], [[1.2460565567016602]], [[1.0286014080047607]], [[2.0453052520751953]], [[0.2444269061088562]], [[0.0]], [[0.5851564407348633]], [[0.9765392541885376]], [[1.5294780731201172]], [[1.2176717519760132]], [[0.5375726222991943]], [[0.9212256669998169]], [[1.4400856494903564]], [[0.9198911190032959]], [[2.0164506435394287]], [[0.6767697334289551]], [[1.611403465270996]], [[0.689254641532898]], [[0.9989840984344482]], [[0.7477866411209106]], [[1.4163498878479004]], [[2.2330617904663086]], [[3.3266074657440186]], [[3.1683294773101807]], [[1.0645406246185303]], [[2.205601930618286]], [[1.479196548461914]], [[0.8417621850967407]], [[0.37722790241241455]], [[0.5752818584442139]], [[1.6664834022521973]], [[0.044951021671295166]], [[0.6719822883605957]], [[0.5143978595733643]], [[0.5707625150680542]], [[2.452650308609009]], [[3.3323323726654053]], [[1.6675353050231934]], [[1.6783554553985596]], [[0.7006189823150635]], [[0.0]], [[0.3579052686691284]], [[1.0309221744537354]], [[2.766800880432129]], [[2.347296714782715]], [[2.607029914855957]], [[3.462661027908325]], [[2.6269211769104004]], [[1.1981770992279053]], [[0.0]], [[1.785571813583374]], [[1.6312081813812256]], [[1.7764215469360352]], [[2.966609239578247]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.06347006559371948]], [[0.0]], [[1.802854299545288]], [[1.8749361038208008]], [[0.4218263626098633]], [[1.3782384395599365]], [[1.7601149082183838]], [[1.2460565567016602]], [[1.0286014080047607]], [[2.0453052520751953]], [[0.2444269061088562]], [[0.0]], [[0.5851564407348633]], [[0.9765392541885376]], [[1.5294780731201172]], [[1.2176717519760132]], [[0.5375726222991943]], [[0.9212256669998169]], [[1.4400856494903564]], [[0.9198911190032959]], [[2.0164506435394287]], [[0.6767697334289551]], [[1.611403465270996]], [[0.689254641532898]], [[0.9989840984344482]], [[0.7477866411209106]], [[1.4163498878479004]], [[2.2330617904663086]], [[3.3266074657440186]], [[3.1683294773101807]], [[1.0645406246185303]], [[2.205601930618286]], [[1.479196548461914]], [[0.8417621850967407]], [[0.37722790241241455]], [[0.5752818584442139]], [[1.6664834022521973]], [[0.044951021671295166]], [[0.6719822883605957]], [[0.5143978595733643]], [[0.5707625150680542]], [[2.452650308609009]], [[3.3323323726654053]], [[1.6675353050231934]], [[1.6783554553985596]], [[0.7006189823150635]], [[0.0]], [[0.3579052686691284]], [[1.0309221744537354]], [[2.766800880432129]], [[2.347296714782715]], [[2.607029914855957]], [[3.462661027908325]], [[2.6269211769104004]], [[1.1981770992279053]], [[0.0]], [[1.785571813583374]], [[1.6312081813812256]], [[1.7764215469360352]], [[2.966609239578247]], [[1.575878620147705]], [[0.5335711240768433]], [[0.6071090698242188]], [[1.0149058103561401]], [[2.1140263080596924]], [[3.190859794616699]], [[0.3823971748352051]], [[1.530935525894165]], [[2.917895793914795]]], [[[0.0]], [[0.15370863676071167]], [[0.4303387403488159]], [[0.7042974233627319]], [[0.0]], [[0.2045365571975708]], [[0.0]], [[0.15135014057159424]], [[0.047507286071777344]], [[0.3375891447067261]], [[0.3439897298812866]], [[0.477217435836792]], [[0.2047526240348816]], [[0.8729926347732544]], [[0.0]], [[0.4485386610031128]], [[0.07710707187652588]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.23235493898391724]], [[0.0]], [[0.0]], [[0.4675610065460205]], [[0.02217763662338257]], [[0.2881569266319275]], [[0.2720351219177246]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.17077898979187012]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.03359323740005493]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.1973302960395813]], [[0.0]], [[0.027334213256835938]], [[0.1089557409286499]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.39060115814208984]], [[0.20470088720321655]], [[0.3241773843765259]], [[0.0]], [[0.03818768262863159]], [[0.010790586471557617]], [[0.0]], [[0.0]], [[0.08270227909088135]], [[0.1549307107925415]], [[0.11284643411636353]], [[0.22594016790390015]], [[2.443530559539795]], [[1.0439453125]], [[0.6924594640731812]], [[1.3123409748077393]], [[0.06080728769302368]], [[1.4182922840118408]], [[2.0870466232299805]], [[0.49309539794921875]], [[1.1596356630325317]], [[1.559216022491455]], [[0.8559755086898804]], [[0.9577215909957886]], [[1.2363677024841309]], [[2.6973049640655518]], [[2.880769729614258]], [[2.762855291366577]], [[1.4470441341400146]], [[0.8177244663238525]], [[0.7882870435714722]], [[1.922593593597412]], [[2.8359341621398926]], [[1.509955883026123]], [[2.0163373947143555]], [[2.8998165130615234]]], [[[0.0]], [[0.15370863676071167]], [[0.4303387403488159]], [[0.7042974233627319]], [[0.0]], [[0.2045365571975708]], [[0.0]], [[0.15135014057159424]], [[0.047507286071777344]], [[0.3375891447067261]], [[0.3439897298812866]], [[0.477217435836792]], [[0.2047526240348816]], [[0.8729926347732544]], [[0.0]], [[0.4485386610031128]], [[0.07710707187652588]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.23235493898391724]], [[0.0]], [[0.0]], [[0.4675610065460205]], [[0.02217763662338257]], [[0.2881569266319275]], [[0.2720351219177246]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.17077898979187012]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.03359323740005493]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.1973302960395813]], [[0.0]], [[0.027334213256835938]], [[0.1089557409286499]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.39060115814208984]], [[0.20470088720321655]], [[0.3241773843765259]], [[0.0]], [[0.03818768262863159]], [[0.010790586471557617]], [[0.0]], [[0.0]], [[0.08270227909088135]], [[0.1549307107925415]], [[0.11284643411636353]], [[0.22594016790390015]], [[2.443530559539795]], [[1.0439453125]], [[0.6924594640731812]], [[1.3123409748077393]], [[0.06080728769302368]], [[1.4182922840118408]], [[2.0870466232299805]], [[0.49309539794921875]], [[1.1596356630325317]], [[1.559216022491455]], [[0.8559755086898804]], [[0.9577215909957886]], [[1.2363677024841309]], [[2.6973049640655518]], [[2.880769729614258]]], [[[0.0]], [[0.15370863676071167]], [[0.4303387403488159]], [[0.7042974233627319]], [[0.0]], [[0.2045365571975708]], [[0.0]], [[0.15135014057159424]], [[0.047507286071777344]], [[0.3375891447067261]], [[0.3439897298812866]], [[0.477217435836792]], [[0.2047526240348816]], [[0.8729926347732544]], [[0.0]], [[0.4485386610031128]], [[0.07710707187652588]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.23235493898391724]], [[0.0]], [[0.0]], [[0.4675610065460205]], [[0.02217763662338257]], [[0.2881569266319275]], [[0.2720351219177246]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.17077898979187012]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.03359323740005493]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.1973302960395813]], [[0.0]], [[0.027334213256835938]], [[0.1089557409286499]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.39060115814208984]], [[0.20470088720321655]], [[0.3241773843765259]], [[0.0]], [[0.03818768262863159]], [[0.010790586471557617]], [[0.0]], [[0.0]], [[0.08270227909088135]], [[0.1549307107925415]], [[0.11284643411636353]], [[0.22594016790390015]], [[2.443530559539795]], [[1.0439453125]], [[0.6924594640731812]], [[1.3123409748077393]], [[0.06080728769302368]], [[1.4182922840118408]], [[2.0870466232299805]], [[0.49309539794921875]], [[1.1596356630325317]], [[1.559216022491455]], [[0.8559755086898804]], [[0.9577215909957886]], [[1.2363677024841309]], [[2.6973049640655518]], [[2.880769729614258]], [[2.762855291366577]], [[1.4470441341400146]], [[0.8177244663238525]], [[0.7882870435714722]], [[1.922593593597412]], [[2.8359341621398926]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.06347006559371948]], [[0.0]], [[1.802854299545288]], [[1.8749361038208008]], [[0.4218263626098633]], [[1.3782384395599365]], [[1.7601149082183838]], [[1.2460565567016602]], [[1.0286014080047607]], [[2.0453052520751953]], [[0.2444269061088562]], [[0.0]], [[0.5851564407348633]], [[0.9765392541885376]], [[1.5294780731201172]], [[1.2176717519760132]], [[0.5375726222991943]], [[0.9212256669998169]], [[1.4400856494903564]], [[0.9198911190032959]], [[2.0164506435394287]], [[0.6767697334289551]], [[1.611403465270996]], [[0.689254641532898]], [[0.9989840984344482]], [[0.7477866411209106]], [[1.4163498878479004]], [[2.2330617904663086]], [[3.3266074657440186]], [[3.1683294773101807]], [[1.0645406246185303]], [[2.205601930618286]], [[1.479196548461914]], [[0.8417621850967407]], [[0.37722790241241455]], [[0.5752818584442139]], [[1.6664834022521973]], [[0.044951021671295166]], [[0.6719822883605957]], [[0.5143978595733643]], [[0.5707625150680542]], [[2.452650308609009]], [[3.3323323726654053]], [[1.6675353050231934]], [[1.6783554553985596]], [[0.7006189823150635]], [[0.0]], [[0.3579052686691284]], [[1.0309221744537354]], [[2.766800880432129]]], [[[0.0]], [[0.15370863676071167]], [[0.4303387403488159]], [[0.7042974233627319]], [[0.0]], [[0.2045365571975708]], [[0.0]], [[0.15135014057159424]], [[0.047507286071777344]], [[0.3375891447067261]], [[0.3439897298812866]], [[0.477217435836792]], [[0.2047526240348816]], [[0.8729926347732544]], [[0.0]], [[0.4485386610031128]], [[0.07710707187652588]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.23235493898391724]], [[0.0]], [[0.0]], [[0.4675610065460205]], [[0.02217763662338257]], [[0.2881569266319275]], [[0.2720351219177246]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.17077898979187012]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.03359323740005493]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.1973302960395813]], [[0.0]], [[0.027334213256835938]], [[0.1089557409286499]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.39060115814208984]], [[0.20470088720321655]], [[0.3241773843765259]], [[0.0]], [[0.03818768262863159]], [[0.010790586471557617]], [[0.0]], [[0.0]], [[0.08270227909088135]], [[0.1549307107925415]], [[0.11284643411636353]], [[0.22594016790390015]], [[2.443530559539795]], [[1.0439453125]], [[0.6924594640731812]], [[1.3123409748077393]], [[0.06080728769302368]], [[1.4182922840118408]], [[2.0870466232299805]], [[0.49309539794921875]], [[1.1596356630325317]], [[1.559216022491455]], [[0.8559755086898804]], [[0.9577215909957886]], [[1.2363677024841309]], [[2.6973049640655518]], [[2.880769729614258]], [[2.762855291366577]]], [[[0.0]], [[0.15370863676071167]], [[0.4303387403488159]], [[0.7042974233627319]], [[0.0]], [[0.2045365571975708]], [[0.0]], [[0.15135014057159424]], [[0.047507286071777344]], [[0.3375891447067261]], [[0.3439897298812866]], [[0.477217435836792]], [[0.2047526240348816]], [[0.8729926347732544]], [[0.0]], [[0.4485386610031128]], [[0.07710707187652588]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.23235493898391724]], [[0.0]], [[0.0]], [[0.4675610065460205]], [[0.02217763662338257]], [[0.2881569266319275]], [[0.2720351219177246]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.17077898979187012]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.03359323740005493]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.1973302960395813]], [[0.0]], [[0.027334213256835938]], [[0.1089557409286499]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.39060115814208984]], [[0.20470088720321655]], [[0.3241773843765259]], [[0.0]], [[0.03818768262863159]], [[0.010790586471557617]], [[0.0]], [[0.0]], [[0.08270227909088135]], [[0.1549307107925415]], [[0.11284643411636353]], [[0.22594016790390015]], [[2.443530559539795]], [[1.0439453125]], [[0.6924594640731812]], [[1.3123409748077393]], [[0.06080728769302368]], [[1.4182922840118408]], [[2.0870466232299805]], [[0.49309539794921875]], [[1.1596356630325317]], [[1.559216022491455]], [[0.8559755086898804]], [[0.9577215909957886]], [[1.2363677024841309]], [[2.6973049640655518]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.06347006559371948]], [[0.0]], [[1.802854299545288]], [[1.8749361038208008]], [[0.4218263626098633]], [[1.3782384395599365]], [[1.7601149082183838]], [[1.2460565567016602]], [[1.0286014080047607]], [[2.0453052520751953]], [[0.2444269061088562]], [[0.0]], [[0.5851564407348633]], [[0.9765392541885376]], [[1.5294780731201172]], [[1.2176717519760132]], [[0.5375726222991943]], [[0.9212256669998169]], [[1.4400856494903564]], [[0.9198911190032959]], [[2.0164506435394287]], [[0.6767697334289551]], [[1.611403465270996]], [[0.689254641532898]], [[0.9989840984344482]], [[0.7477866411209106]], [[1.4163498878479004]], [[2.2330617904663086]], [[3.3266074657440186]], [[3.1683294773101807]], [[1.0645406246185303]], [[2.205601930618286]], [[1.479196548461914]], [[0.8417621850967407]], [[0.37722790241241455]], [[0.5752818584442139]], [[1.6664834022521973]], [[0.044951021671295166]], [[0.6719822883605957]], [[0.5143978595733643]], [[0.5707625150680542]], [[2.452650308609009]], [[3.3323323726654053]], [[1.6675353050231934]], [[1.6783554553985596]], [[0.7006189823150635]], [[0.0]], [[0.3579052686691284]], [[1.0309221744537354]], [[2.766800880432129]], [[2.347296714782715]], [[2.607029914855957]], [[3.462661027908325]], [[2.6269211769104004]], [[1.1981770992279053]], [[0.0]], [[1.785571813583374]], [[1.6312081813812256]], [[1.7764215469360352]], [[2.966609239578247]], [[1.575878620147705]], [[0.5335711240768433]], [[0.6071090698242188]], [[1.0149058103561401]], [[2.1140263080596924]], [[3.190859794616699]], [[0.3823971748352051]], [[1.530935525894165]], [[2.917895793914795]], [[2.081772804260254]], [[1.1767274141311646]], [[1.7527835369110107]], [[3.1707708835601807]], [[2.051011562347412]], [[2.6565797328948975]], [[1.9720652103424072]], [[0.9408859014511108]], [[0.3527991771697998]], [[2.439871311187744]], [[2.6888978481292725]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.06347006559371948]], [[0.0]], [[1.802854299545288]], [[1.8749361038208008]], [[0.4218263626098633]], [[1.3782384395599365]], [[1.7601149082183838]], [[1.2460565567016602]], [[1.0286014080047607]], [[2.0453052520751953]], [[0.2444269061088562]], [[0.0]], [[0.5851564407348633]], [[0.9765392541885376]], [[1.5294780731201172]], [[1.2176717519760132]], [[0.5375726222991943]], [[0.9212256669998169]], [[1.4400856494903564]], [[0.9198911190032959]], [[2.0164506435394287]], [[0.6767697334289551]], [[1.611403465270996]], [[0.689254641532898]], [[0.9989840984344482]], [[0.7477866411209106]], [[1.4163498878479004]], [[2.2330617904663086]], [[3.3266074657440186]], [[3.1683294773101807]], [[1.0645406246185303]], [[2.205601930618286]], [[1.479196548461914]], [[0.8417621850967407]], [[0.37722790241241455]], [[0.5752818584442139]], [[1.6664834022521973]], [[0.044951021671295166]], [[0.6719822883605957]], [[0.5143978595733643]], [[0.5707625150680542]], [[2.452650308609009]], [[3.3323323726654053]], [[1.6675353050231934]], [[1.6783554553985596]], [[0.7006189823150635]], [[0.0]], [[0.3579052686691284]], [[1.0309221744537354]], [[2.766800880432129]], [[2.347296714782715]], [[2.607029914855957]], [[3.462661027908325]], [[2.6269211769104004]], [[1.1981770992279053]], [[0.0]], [[1.785571813583374]], [[1.6312081813812256]], [[1.7764215469360352]], [[2.966609239578247]], [[1.575878620147705]], [[0.5335711240768433]], [[0.6071090698242188]], [[1.0149058103561401]], [[2.1140263080596924]], [[3.190859794616699]], [[0.3823971748352051]], [[1.530935525894165]], [[2.917895793914795]], [[2.081772804260254]], [[1.1767274141311646]], [[1.7527835369110107]], [[3.1707708835601807]], [[2.051011562347412]], [[2.6565797328948975]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.06347006559371948]], [[0.0]], [[1.802854299545288]], [[1.8749361038208008]], [[0.4218263626098633]], [[1.3782384395599365]], [[1.7601149082183838]], [[1.2460565567016602]], [[1.0286014080047607]], [[2.0453052520751953]], [[0.2444269061088562]], [[0.0]], [[0.5851564407348633]], [[0.9765392541885376]], [[1.5294780731201172]], [[1.2176717519760132]], [[0.5375726222991943]], [[0.9212256669998169]], [[1.4400856494903564]], [[0.9198911190032959]], [[2.0164506435394287]], [[0.6767697334289551]], [[1.611403465270996]], [[0.689254641532898]], [[0.9989840984344482]], [[0.7477866411209106]], [[1.4163498878479004]], [[2.2330617904663086]], [[3.3266074657440186]], [[3.1683294773101807]], [[1.0645406246185303]], [[2.205601930618286]], [[1.479196548461914]], [[0.8417621850967407]], [[0.37722790241241455]], [[0.5752818584442139]], [[1.6664834022521973]], [[0.044951021671295166]], [[0.6719822883605957]], [[0.5143978595733643]], [[0.5707625150680542]], [[2.452650308609009]], [[3.3323323726654053]], [[1.6675353050231934]], [[1.6783554553985596]], [[0.7006189823150635]], [[0.0]], [[0.3579052686691284]], [[1.0309221744537354]], [[2.766800880432129]], [[2.347296714782715]], [[2.607029914855957]], [[3.462661027908325]], [[2.6269211769104004]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.06347006559371948]], [[0.0]], [[1.802854299545288]], [[1.8749361038208008]], [[0.4218263626098633]], [[1.3782384395599365]], [[1.7601149082183838]], [[1.2460565567016602]], [[1.0286014080047607]], [[2.0453052520751953]], [[0.2444269061088562]], [[0.0]], [[0.5851564407348633]], [[0.9765392541885376]], [[1.5294780731201172]], [[1.2176717519760132]], [[0.5375726222991943]], [[0.9212256669998169]], [[1.4400856494903564]], [[0.9198911190032959]], [[2.0164506435394287]], [[0.6767697334289551]], [[1.611403465270996]], [[0.689254641532898]], [[0.9989840984344482]], [[0.7477866411209106]], [[1.4163498878479004]], [[2.2330617904663086]], [[3.3266074657440186]], [[3.1683294773101807]], [[1.0645406246185303]], [[2.205601930618286]], [[1.479196548461914]], [[0.8417621850967407]], [[0.37722790241241455]], [[0.5752818584442139]], [[1.6664834022521973]], [[0.044951021671295166]], [[0.6719822883605957]], [[0.5143978595733643]], [[0.5707625150680542]], [[2.452650308609009]], [[3.3323323726654053]], [[1.6675353050231934]], [[1.6783554553985596]], [[0.7006189823150635]], [[0.0]], [[0.3579052686691284]], [[1.0309221744537354]], [[2.766800880432129]], [[2.347296714782715]], [[2.607029914855957]], [[3.462661027908325]], [[2.6269211769104004]], [[1.1981770992279053]], [[0.0]], [[1.785571813583374]], [[1.6312081813812256]], [[1.7764215469360352]], [[2.966609239578247]], [[1.575878620147705]], [[0.5335711240768433]], [[0.6071090698242188]], [[1.0149058103561401]], [[2.1140263080596924]], [[3.190859794616699]], [[0.3823971748352051]], [[1.530935525894165]], [[2.917895793914795]], [[2.081772804260254]], [[1.1767274141311646]], [[1.7527835369110107]], [[3.1707708835601807]], [[2.051011562347412]], [[2.6565797328948975]], [[1.9720652103424072]], [[0.9408859014511108]], [[0.3527991771697998]], [[2.439871311187744]], [[2.6888978481292725]], [[1.4930927753448486]], [[1.616208553314209]], [[0.0]], [[1.2653350830078125]], [[2.6206161975860596]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.06347006559371948]], [[0.0]], [[1.802854299545288]], [[1.8749361038208008]], [[0.4218263626098633]], [[1.3782384395599365]], [[1.7601149082183838]], [[1.2460565567016602]], [[1.0286014080047607]], [[2.0453052520751953]], [[0.2444269061088562]], [[0.0]], [[0.5851564407348633]], [[0.9765392541885376]], [[1.5294780731201172]], [[1.2176717519760132]], [[0.5375726222991943]], [[0.9212256669998169]], [[1.4400856494903564]], [[0.9198911190032959]], [[2.0164506435394287]], [[0.6767697334289551]], [[1.611403465270996]], [[0.689254641532898]], [[0.9989840984344482]], [[0.7477866411209106]], [[1.4163498878479004]], [[2.2330617904663086]], [[3.3266074657440186]], [[3.1683294773101807]], [[1.0645406246185303]], [[2.205601930618286]], [[1.479196548461914]], [[0.8417621850967407]], [[0.37722790241241455]], [[0.5752818584442139]], [[1.6664834022521973]], [[0.044951021671295166]], [[0.6719822883605957]], [[0.5143978595733643]], [[0.5707625150680542]], [[2.452650308609009]], [[3.3323323726654053]], [[1.6675353050231934]], [[1.6783554553985596]], [[0.7006189823150635]], [[0.0]], [[0.3579052686691284]], [[1.0309221744537354]], [[2.766800880432129]], [[2.347296714782715]], [[2.607029914855957]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.06347006559371948]], [[0.0]], [[1.802854299545288]], [[1.8749361038208008]], [[0.4218263626098633]], [[1.3782384395599365]], [[1.7601149082183838]], [[1.2460565567016602]], [[1.0286014080047607]], [[2.0453052520751953]], [[0.2444269061088562]], [[0.0]], [[0.5851564407348633]], [[0.9765392541885376]], [[1.5294780731201172]], [[1.2176717519760132]], [[0.5375726222991943]], [[0.9212256669998169]], [[1.4400856494903564]], [[0.9198911190032959]], [[2.0164506435394287]], [[0.6767697334289551]], [[1.611403465270996]], [[0.689254641532898]], [[0.9989840984344482]], [[0.7477866411209106]], [[1.4163498878479004]], [[2.2330617904663086]], [[3.3266074657440186]], [[3.1683294773101807]], [[1.0645406246185303]], [[2.205601930618286]], [[1.479196548461914]], [[0.8417621850967407]], [[0.37722790241241455]], [[0.5752818584442139]], [[1.6664834022521973]], [[0.044951021671295166]], [[0.6719822883605957]], [[0.5143978595733643]], [[0.5707625150680542]], [[2.452650308609009]]], [[[0.0]], [[0.15370863676071167]], [[0.4303387403488159]], [[0.7042974233627319]], [[0.0]], [[0.2045365571975708]], [[0.0]], [[0.15135014057159424]], [[0.047507286071777344]], [[0.3375891447067261]], [[0.3439897298812866]], [[0.477217435836792]], [[0.2047526240348816]], [[0.8729926347732544]], [[0.0]], [[0.4485386610031128]], [[0.07710707187652588]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.23235493898391724]], [[0.0]], [[0.0]], [[0.4675610065460205]], [[0.02217763662338257]], [[0.2881569266319275]], [[0.2720351219177246]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.17077898979187012]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.03359323740005493]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.1973302960395813]], [[0.0]], [[0.027334213256835938]], [[0.1089557409286499]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.39060115814208984]], [[0.20470088720321655]], [[0.3241773843765259]], [[0.0]], [[0.03818768262863159]], [[0.010790586471557617]], [[0.0]], [[0.0]], [[0.08270227909088135]], [[0.1549307107925415]], [[0.11284643411636353]], [[0.22594016790390015]], [[2.443530559539795]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.06347006559371948]], [[0.0]], [[1.802854299545288]], [[1.8749361038208008]], [[0.4218263626098633]], [[1.3782384395599365]], [[1.7601149082183838]], [[1.2460565567016602]], [[1.0286014080047607]], [[2.0453052520751953]], [[0.2444269061088562]], [[0.0]], [[0.5851564407348633]], [[0.9765392541885376]], [[1.5294780731201172]], [[1.2176717519760132]], [[0.5375726222991943]], [[0.9212256669998169]], [[1.4400856494903564]], [[0.9198911190032959]], [[2.0164506435394287]], [[0.6767697334289551]], [[1.611403465270996]], [[0.689254641532898]], [[0.9989840984344482]], [[0.7477866411209106]], [[1.4163498878479004]], [[2.2330617904663086]], [[3.3266074657440186]], [[3.1683294773101807]], [[1.0645406246185303]], [[2.205601930618286]], [[1.479196548461914]], [[0.8417621850967407]], [[0.37722790241241455]], [[0.5752818584442139]], [[1.6664834022521973]], [[0.044951021671295166]], [[0.6719822883605957]], [[0.5143978595733643]], [[0.5707625150680542]], [[2.452650308609009]], [[3.3323323726654053]], [[1.6675353050231934]], [[1.6783554553985596]], [[0.7006189823150635]], [[0.0]], [[0.3579052686691284]], [[1.0309221744537354]], [[2.766800880432129]], [[2.347296714782715]], [[2.607029914855957]], [[3.462661027908325]], [[2.6269211769104004]], [[1.1981770992279053]], [[0.0]], [[1.785571813583374]], [[1.6312081813812256]], [[1.7764215469360352]], [[2.966609239578247]], [[1.575878620147705]], [[0.5335711240768433]], [[0.6071090698242188]], [[1.0149058103561401]], [[2.1140263080596924]], [[3.190859794616699]], [[0.3823971748352051]], [[1.530935525894165]], [[2.917895793914795]], [[2.081772804260254]], [[1.1767274141311646]], [[1.7527835369110107]], [[3.1707708835601807]], [[2.051011562347412]], [[2.6565797328948975]], [[1.9720652103424072]], [[0.9408859014511108]], [[0.3527991771697998]], [[2.439871311187744]]], [[[0.0]], [[0.15370863676071167]], [[0.4303387403488159]], [[0.7042974233627319]], [[0.0]], [[0.2045365571975708]], [[0.0]], [[0.15135014057159424]], [[0.047507286071777344]], [[0.3375891447067261]], [[0.3439897298812866]], [[0.477217435836792]], [[0.2047526240348816]], [[0.8729926347732544]], [[0.0]], [[0.4485386610031128]], [[0.07710707187652588]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.23235493898391724]], [[0.0]], [[0.0]], [[0.4675610065460205]], [[0.02217763662338257]], [[0.2881569266319275]], [[0.2720351219177246]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.17077898979187012]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.03359323740005493]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.1973302960395813]], [[0.0]], [[0.027334213256835938]], [[0.1089557409286499]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.39060115814208984]], [[0.20470088720321655]], [[0.3241773843765259]], [[0.0]], [[0.03818768262863159]], [[0.010790586471557617]], [[0.0]], [[0.0]], [[0.08270227909088135]], [[0.1549307107925415]], [[0.11284643411636353]], [[0.22594016790390015]], [[2.443530559539795]], [[1.0439453125]], [[0.6924594640731812]], [[1.3123409748077393]], [[0.06080728769302368]], [[1.4182922840118408]], [[2.0870466232299805]], [[0.49309539794921875]], [[1.1596356630325317]], [[1.559216022491455]], [[0.8559755086898804]], [[0.9577215909957886]], [[1.2363677024841309]], [[2.6973049640655518]], [[2.880769729614258]], [[2.762855291366577]], [[1.4470441341400146]], [[0.8177244663238525]], [[0.7882870435714722]], [[1.922593593597412]], [[2.8359341621398926]], [[1.509955883026123]], [[2.0163373947143555]], [[2.8998165130615234]], [[1.8599400520324707]], [[2.381211996078491]]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7fe9e2700290>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "component_idx = 4\n",
    "feat_idx = 12420\n",
    "\n",
    "submodule = submodules[component_idx]\n",
    "dictionary = dictionaries[submodule]\n",
    "\n",
    "# interpret some features\n",
    "data = zst_to_generator('/share/data/datasets/pile/the-eye.eu/public/AI/pile/train/00.jsonl.zst')\n",
    "buffer = ActivationBuffer(\n",
    "    data,\n",
    "    model,\n",
    "    submodule,\n",
    "    out_feats=512,\n",
    "    in_batch_size=128,\n",
    "    n_ctxs=512,\n",
    ")\n",
    "\n",
    "out = examine_dimension(\n",
    "    model,\n",
    "    submodule,\n",
    "    buffer,\n",
    "    dictionary,\n",
    "    dim_idx=feat_idx,\n",
    "    n_inputs=256\n",
    ")\n",
    "print(out.top_tokens)\n",
    "print(out.top_affected)\n",
    "out.top_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_ablate = {\n",
    "    submodules[0] : [\n",
    "        # 4074, # predicts code\n",
    "        # 18775, # certain periods, unclear\n",
    "        18967, # 'He'\n",
    "        22084, # 'he'\n",
    "        29626, # 'his'\n",
    "        # 951, # 'with'\n",
    "        1022, # 'she'\n",
    "        # 1692, # 'or'\n",
    "        2079, # 'Woman' or 'Ladies'\n",
    "        # 2493, # 'nurse(s)'\n",
    "        3122, # 'Her'\n",
    "        # 5648, # 'care' in medical context\n",
    "        # 5950, # 'to'\n",
    "        # 9610, # certain periods, unclear\n",
    "        9651, # female names\n",
    "        10060, # 'She'\n",
    "        # 11778, # 'phone(s)'\n",
    "        # 13675, # certain verbs\n",
    "        # 15032, # unclear\n",
    "        # 23666, # 'and'\n",
    "        # 24418, # unclear,\n",
    "        26504, # 'her'\n",
    "        # 26586, # unclear\n",
    "        # 31201, # 'nursing'\n",
    "    ],\n",
    "    submodules[1] : [\n",
    "        2995, # 'He'\n",
    "        8920, # 'he'\n",
    "        12128, # 'his'\n",
    "        # 12436, # gendered pronouns?\n",
    "        4592, # 'her'\n",
    "        9877, # female names\n",
    "        # 12882, # 'share'\n",
    "        # 14918, # certain periods, unclear\n",
    "        15017, # 'she'\n",
    "        # 17369, # predicts phone numbers\n",
    "        26204, # 'Her'\n",
    "        # 26476, # certain periods, unclear\n",
    "        # 26969, # related to nursing\n",
    "        # 28145, # 'medical' \n",
    "        30248, # female names\n",
    "    ],\n",
    "    submodules[2] : [\n",
    "        4433, # promotes male-associated words\n",
    "        4539, # gendered pronouns\n",
    "        8944, # capitalized gendered pronouns\n",
    "        # 10700, # unclear\n",
    "        11656, # promotes male-associated words\n",
    "        # 14559, # periods in biographies, might promote male words a bit\n",
    "        # 15225, # something about dates?\n",
    "        # 15938, # unclear, might promote male words a bit\n",
    "        # 19014, # certain periods, unclear\n",
    "        # 27803, # unclear\n",
    "        29206, # gendered pronouns\n",
    "        30263, # gendered pronouns\n",
    "        # 277, # spammy advertisements\n",
    "        1995, # promotes female-associated words\n",
    "        # 4770, # unclear\n",
    "        9128, # female pronouns\n",
    "        10635, # female-associated word\n",
    "        # 10757, # unclear\n",
    "        12440, # promotes female-associated words\n",
    "        # 14638, # related to contact information?\n",
    "        # 17774, # related to terms and conditions?\n",
    "        # 21331, # certain periods, unclear\n",
    "        # 26413, # fires on periods in ?blog posts/first-person writing?\n",
    "        # 27838, # promotes parts of urls\n",
    "        29295, # female names\n",
    "        # 29371, # certain periods, unclear\n",
    "        # 31098, # nursing-related words\n",
    "    ],\n",
    "    submodules[3] : [\n",
    "        # 93, # periods in medical contexts\n",
    "        # 2751, # words related to research\n",
    "        # 13474, # active in bios of medical professionals\n",
    "        # 15246, # promotes words about art\n",
    "        # 24661, # periods in bios\n",
    "        27334, # promotes male-associated words\n",
    "        # 27867, # periods in bios, might promote male words a bit\n",
    "        # 7539, # unclear \n",
    "        # 10295, # certain periods\n",
    "        # 13542, # promotes advertising words\n",
    "        # 14401, # unclear\n",
    "        19558, # promotes female-associated words\n",
    "        # 20526, # promotes medical words\n",
    "        22152, # promotes female-associated words\n",
    "        # 23375, # addresses\n",
    "        23545, # 'she'\n",
    "        # 24484, # promotes ?names of diseases?\n",
    "        24806, # 'her'\n",
    "        # 27051, # promotes capitalized words\n",
    "        30802, # 'woman'/'women'\n",
    "        # 31182, # contact information\n",
    "        # 31751, # accommodation words\n",
    "    ],\n",
    "    submodules[4] : [\n",
    "        # 4125, # periods in bios\n",
    "        # 11987, # promotes verbs in bios\n",
    "        # 12332, # promotes words related to academia\n",
    "        # 14658, # periods\n",
    "        30220, # promotes male pronouns\n",
    "        # 1804, # words related to RSVPing\n",
    "        # 4926, # certain periods\n",
    "        # 5731, # promotes medicinal words\n",
    "        # 6869, # promotes verbs about phone calls\n",
    "        9766, # promotes female-associated words\n",
    "        12420, # promotes female pronouns\n",
    "        # 20708, # contact info\n",
    "        # 21979, # promotes capitalized words\n",
    "        23207, # promotes gendered words, especial female\n",
    "        # 30612, # fundraising\n",
    "        # 31282, # capitalized words about contacting\n",
    "    ]\n",
    "        \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_ablations(\n",
    "        model,\n",
    "        inputs,\n",
    "        submodules,\n",
    "        dictionaries,\n",
    "        to_ablate,\n",
    "        inference=True,\n",
    "):\n",
    "    with model.invoke(inputs, fwd_args={'inference': inference}):\n",
    "        for submodule in submodules:\n",
    "            dictionary = dictionaries[submodule]\n",
    "            x = submodule.output\n",
    "            is_resid = (type(x.shape) == tuple)\n",
    "            if is_resid:\n",
    "                x = x[0]\n",
    "            x_hat = dictionary(x)\n",
    "            residual = x - x_hat\n",
    "\n",
    "            f = dictionary.encode(x)\n",
    "            ablation_idxs = t.Tensor(to_ablate[submodule]).long()\n",
    "            f[:, :, ablation_idxs] = 0.\n",
    "            x_hat = dictionary.decode(f)\n",
    "            if is_resid:\n",
    "                submodule.output[0][:] = x_hat + residual\n",
    "            else:\n",
    "                submodule.output = x_hat + residual\n",
    "            \n",
    "        acts = model.gpt_neox.layers[layer].output[0][:,-1,:].save()\n",
    "    return acts.value.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acts_abl(text):\n",
    "    return run_with_ablations(\n",
    "        model,\n",
    "        text,\n",
    "        submodules,\n",
    "        dictionaries,\n",
    "        to_ablate,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous test accuracy: 0.8661710023880005\n",
      "Ground truth accuracy: 0.8254608511924744\n",
      "Spurious accuracy: 0.5547235012054443\n"
     ]
    }
   ],
   "source": [
    "print('Ambiguous test accuracy:', test_probe(probe, get_acts_abl, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(probe, get_acts_abl, batches=batches, label_idx=0))\n",
    "print('Spurious accuracy:', test_probe(probe, get_acts_abl, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.9879666566848755\n",
      "Accuracy for (0, 1): 0.9644010066986084\n",
      "Accuracy for (1, 0): 0.559907853603363\n",
      "Accuracy for (1, 1): 0.7453531622886658\n"
     ]
    }
   ],
   "source": [
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(probe, get_acts_abl, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous test accuracy: 0.9350603818893433\n",
      "Ground truth accuracy: 0.89573734998703\n",
      "Spurious accuracy: 0.5339861512184143\n"
     ]
    }
   ],
   "source": [
    "new_probe, _ = train_probe(get_acts_abl, label_idx=0, lr=lr, epochs=epochs)\n",
    "print('Ambiguous test accuracy:', test_probe(new_probe, get_acts_abl, label_idx=0))\n",
    "batches = get_data(train=False, ambiguous=False)\n",
    "print('Ground truth accuracy:', test_probe(new_probe, get_acts_abl, batches=batches, label_idx=0))\n",
    "print('Spurious accuracy:', test_probe(new_probe, get_acts_abl, batches=batches, label_idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for (0, 0): 0.9301449656486511\n",
      "Accuracy for (0, 1): 0.8476905226707458\n",
      "Accuracy for (1, 0): 0.8709677457809448\n",
      "Accuracy for (1, 1): 0.9442378878593445\n"
     ]
    }
   ],
   "source": [
    "subgroups = get_subgroups(train=False, ambiguous=False)\n",
    "for label_profile, batches in subgroups.items():\n",
    "    print(f'Accuracy for {label_profile}:', test_probe(new_probe, get_acts_abl, batches=batches, label_idx=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on ambiguous data: 0.4993029832839966\n",
      "Ground truth accuracy 0.4976958632469177\n",
      "Spurious accuracy 0.5011520981788635\n"
     ]
    }
   ],
   "source": [
    "def out_fn(model):\n",
    "    return probe(model.gpt_neox.layers[layer].output[0][:,-1,:])\n",
    "\n",
    "# get accuracy after ablating above features\n",
    "batches = get_data(train=False, ambiguous=True, batch_size=128, seed=SEED)\n",
    "\n",
    "with t.no_grad():\n",
    "    corrects = []\n",
    "    for batch in batches:\n",
    "        text = batch[0]\n",
    "        labels = batch[1]\n",
    "        probs = run_with_ablations(\n",
    "            model,\n",
    "            text,\n",
    "            submodules,\n",
    "            dictionaries,\n",
    "            to_ablate,\n",
    "            out_fn\n",
    "        )\n",
    "        preds = (probs > 0.5).long()\n",
    "        corrects.append((preds == labels).float())\n",
    "    print(\"Accuracy on ambiguous data:\", t.cat(corrects).mean().item())\n",
    "\n",
    "batches = get_data(train=False, ambiguous=False, batch_size=128, seed=SEED)\n",
    "\n",
    "with t.no_grad():\n",
    "    truth_corrects, spurious_corrects = [], []\n",
    "    for batch in batches:\n",
    "        text = batch[0]\n",
    "        true_labels = batch[1]\n",
    "        spurious_labels = batch[2]\n",
    "        probs = run_with_ablations(\n",
    "            model,\n",
    "            text,\n",
    "            submodules,\n",
    "            dictionaries,\n",
    "            to_ablate,\n",
    "            out_fn\n",
    "        )\n",
    "        preds = (probs > 0.5).long()\n",
    "        truth_corrects.append((preds == true_labels).float())\n",
    "        spurious_corrects.append((preds == spurious_labels).float())\n",
    "    print(\"Ground truth accuracy\", t.cat(truth_corrects).mean().item())\n",
    "    print(\"Spurious accuracy\", t.cat(spurious_corrects).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrain probe with ablated model\n",
    "\n",
    "def out_fn(model):\n",
    "    return model.gpt_neox.layers[layer].output[0][:,-1,:]\n",
    "\n",
    "t.manual_seed(SEED)\n",
    "new_probe = Probe(512).to('cuda:0')\n",
    "optimizer = t.optim.AdamW(new_probe.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "batches = get_data(train=True, ambiguous=True, batch_size=64, seed=SEED)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    for batch in batches:\n",
    "        text = batch[0]\n",
    "        labels = batch[1]\n",
    "        acts = run_with_ablations(\n",
    "            model,\n",
    "            text,\n",
    "            submodules,\n",
    "            dictionaries,\n",
    "            to_ablate,\n",
    "            out_fn\n",
    "        ).clone()\n",
    "        probs = new_probe(acts)\n",
    "        loss = criterion(probs, labels.float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
