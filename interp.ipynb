{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dictionary_learning import AutoEncoder, ActivationBuffer\n",
    "from nnsight import LanguageModel\n",
    "from dictionary_learning.interp import examine_dimension\n",
    "from dictionary_learning.utils import zst_to_generator\n",
    "import torch as t\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = LanguageModel('EleutherAI/pythia-70m-deduped', device_map='cuda:0', dispatch=True)\n",
    "layer = 0\n",
    "component = 'embed'\n",
    "\n",
    "if component == 'resid':\n",
    "    submodule = model.gpt_neox.layers[layer]\n",
    "elif component == 'attn':\n",
    "    submodule = model.gpt_neox.layers[layer].attention\n",
    "elif component == 'mlp':\n",
    "    submodule = model.gpt_neox.layers[layer].mlp\n",
    "elif component == 'embed':\n",
    "    submodule = model.gpt_neox.embed_in\n",
    "\n",
    "activation_dim=512\n",
    "\n",
    "buffer = ActivationBuffer(\n",
    "    zst_to_generator('/share/data/datasets/pile/the-eye.eu/public/AI/pile/train/00.jsonl.zst'),\n",
    "    model,\n",
    "    submodule,\n",
    "    io='out',\n",
    "    in_feats=activation_dim,\n",
    "    out_feats=activation_dim,\n",
    "    in_batch_size=128,\n",
    "    out_batch_size=2 ** 13,\n",
    "    n_ctxs=1e4,\n",
    "    device='cuda:0',\n",
    ")\n",
    "\n",
    "ae = AutoEncoder(activation_dim, 64 * activation_dim).cuda()\n",
    "if component != 'embed':\n",
    "    ae.load_state_dict(t.load(f'/share/projects/dictionary_circuits/autoencoders/pythia-70m-deduped/{component}_out_layer{layer}/10_32768/ae.pt'))\n",
    "else:\n",
    "    ae.load_state_dict(t.load(f'/share/projects/dictionary_circuits/autoencoders/pythia-70m-deduped/embed/10_32768/ae.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LanguageModelProxy (argument_2): FakeTensor(..., device='cuda:0', size=(256, 128, 512),\n",
      "           grad_fn=<EmbeddingBackward0>)\n",
      "[(' driver', 0.7144788503646851), (' hood', 0.0361337810754776), ('DW', 0.030997931957244873), ('rette', 0.029590070247650146), (' dwarf', 0.013755664229393005), ('Hol', 0.0), ('iday', 0.0), (' P', 0.0), ('unch', 0.0), (' —', 0.0), (' Plus', 0.0), (' a', 0.0), (' Co', 0.0), ('zy', 0.0), (' Fire', 0.0), ('\\n', 0.0), ('Charles', 0.0), (' Dick', 0.0), ('ens', 0.0), (' gave', 0.0), (' us', 0.0), (' so', 0.0), (' much', 0.0), ('.', 0.0), (' In', 0.0), ('cluding', 0.0), (' this', 0.0), ('In', 0.0), (' A', 0.0), (' Christmas', 0.0)]\n",
      "[('lessly', 0.26721781492233276), ('lessness', 0.25997886061668396), (',[@', 0.24340176582336426), ('.[@', 0.23646807670593262), (',^[@', 0.21933341026306152), ('less', 0.20868515968322754), ('doms', 0.20835530757904053), (' since', 0.2060118019580841), ('.^[@', 0.20522615313529968), (';', 0.20469340682029724), (' of', 0.1955747753381729), (' alike', 0.19553811848163605), (',', 0.19463075697422028), ('.\\\\[[@', 0.19186758995056152), (' throughout', 0.1836525797843933), (',...', 0.18250107765197754), (' during', 0.17124208807945251), (',—', 0.16633479297161102), (' aboard', 0.16213564574718475), ('.</', 0.16124455630779266), (',[', 0.15604422986507416), (' across', 0.1536192148923874), ('.]{}', 0.15348871052265167), (' (@', 0.1526058316230774), (' --', 0.15221132338047028), ('.[^', 0.14823561906814575), ('.[', 0.1460874378681183), (' amongst', 0.1451352834701538), (' for', 0.1450173556804657), (\".'\", 0.14299099147319794)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-f49c9646-77c9\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.41.0/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-f49c9646-77c9\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [[\"The\", \" verb\", \"als\", \":\", \" sports\", \" quotes\", \" of\", \" 1994\", \"\\n\", \"\\n\", \"There\", \" are\", \" no\", \" small\", \" accidents\", \" on\", \" this\", \" circuit\", \".\", \" A\", \"yr\", \"ton\", \" Sen\", \"na\", \",\", \" before\", \" the\", \" San\", \" Mar\", \"ino\", \" Grand\", \" Prix\", \",\", \" during\", \" which\", \" he\", \" suffered\", \" a\", \" fatal\", \" crash\", \".\", \"\\n\", \"\\n\", \"One\", \" of\", \" my\", \" best\", \" friends\", \" has\", \" been\", \" killed\", \" on\", \" the\", \" curve\", \" where\", \" I\", \" escaped\", \" death\", \".\", \" I\", \" was\", \" lucky\", \";\", \" he\", \" wasn\", \"'t\", \".\", \" It\", \"'s\", \" like\", \" having\", \" a\", \" che\", \"que\", \" book\", \".\", \" You\", \" start\", \" pulling\", \" out\", \" the\", \" pages\", \" until\", \" one\", \" day\", \" no\", \" pages\", \" are\", \" left\", \".\", \" He\", \" was\", \" the\", \" one\", \" driver\"], [\"The\", \" verb\", \"als\", \":\", \" sports\", \" quotes\", \" of\", \" 1994\", \"\\n\", \"\\n\", \"There\", \" are\", \" no\", \" small\", \" accidents\", \" on\", \" this\", \" circuit\", \".\", \" A\", \"yr\", \"ton\", \" Sen\", \"na\", \",\", \" before\", \" the\", \" San\", \" Mar\", \"ino\", \" Grand\", \" Prix\", \",\", \" during\", \" which\", \" he\", \" suffered\", \" a\", \" fatal\", \" crash\", \".\", \"\\n\", \"\\n\", \"One\", \" of\", \" my\", \" best\", \" friends\", \" has\", \" been\", \" killed\", \" on\", \" the\", \" curve\", \" where\", \" I\", \" escaped\", \" death\", \".\", \" I\", \" was\", \" lucky\", \";\", \" he\", \" wasn\", \"'t\", \".\", \" It\", \"'s\", \" like\", \" having\", \" a\", \" che\", \"que\", \" book\", \".\", \" You\", \" start\", \" pulling\", \" out\", \" the\", \" pages\", \" until\", \" one\", \" day\", \" no\", \" pages\", \" are\", \" left\", \".\", \" He\", \" was\", \" the\", \" one\", \" driver\", \" so\", \" perfect\", \" nobody\", \" thought\", \" anything\", \" could\", \" happen\", \" to\", \" him\", \".\", \" Ger\", \"hard\", \" Berger\", \",\", \" Formula\", \" 1\", \" driver\"], [\"\\n\", \"\\n\", \"Is\", \" there\", \" a\", \" Product\", \"H\", \"unt\", \" without\", \" the\", \" \\u201c\", \"selection\", \" process\", \"\\u201d?\", \" -\", \" hood\"], [\"The\", \" VC\", \"-\", \"2\", \" video\", \" compression\", \" standard\", \" is\", \" an\", \" open\", \" free\", \"-\", \"use\", \" video\", \"-\", \"dec\", \"oding\", \" standard\", \" contributed\", \" by\", \" British\", \" Broadcasting\", \" Corporation\", \" (\", \"BBC\", \")\", \" to\", \" the\", \" Society\", \" of\", \" Motion\", \" Picture\", \" and\", \" Television\", \" Engineers\", \" (\", \"S\", \"MP\", \"TE\", \")\", \" standard\", \".\", \" The\", \" VC\", \"-\", \"2\", \" standard\", \" uses\", \" discrete\", \"-\", \"wave\", \"let\", \"-\", \"transform\", \" (\", \"DW\"], [\"\\u2018\", \"Well\", \",\", \" this\", \" is\", \" weird\", \",\", \" isn\", \"\\u2019\", \"t\", \" it\", \"?\\u201d\", \" says\", \" Dam\", \"on\", \" Alb\", \"arn\", \".\", \" Six\", \" days\", \" ago\", \",\", \" he\", \" was\", \" in\", \" Mexico\", \" City\", \",\", \" playing\", \" with\", \" Gor\", \"ill\", \"az\", \".\", \" Now\", \",\", \" he\", \" and\", \" his\", \" band\", \"mates\", \" in\", \" the\", \" Good\", \",\", \" the\", \" Bad\", \" &\", \" the\", \" Queen\", \" are\", \" in\", \" Kent\", \",\", \" taking\", \" turns\", \" to\", \" explain\", \" their\", \" second\", \" album\", \" in\", \" a\", \" fake\", \" American\", \" d\", \"iner\", \" adjacent\", \" to\", \" the\", \" Ma\", \"id\", \"stone\", \" studio\", \" where\", \" they\", \" will\", \" be\", \" performing\", \" on\", \" Later\", \" \\u2026\", \" with\", \" J\", \"ools\", \" Holland\", \".\", \"\\n\", \"\\n\", \"The\", \" seats\", \" are\", \" regulation\", \" red\", \" le\", \"athe\", \"rette\"], [\"Brown\", \" Man\", \" of\", \" the\", \" Mu\", \"irs\", \"\\n\", \"\\n\", \"In\", \" the\", \" fol\", \"kl\", \"ore\", \" of\", \" the\", \" Anglo\", \"-\", \"Scott\", \"ish\", \" border\", \" the\", \" Brown\", \" Man\", \" of\", \" the\", \" Mu\", \"irs\", \" is\", \" a\", \" dwarf\"], [\"Hol\"], [\"Hol\", \"iday\"], [\"Hol\", \"iday\", \" P\"], [\"Hol\", \"iday\", \" P\", \"unch\"], [\"Hol\", \"iday\", \" P\", \"unch\", \" \\u2014\"], [\"Hol\", \"iday\", \" P\", \"unch\", \" \\u2014\", \" Plus\"], [\"Hol\", \"iday\", \" P\", \"unch\", \" \\u2014\", \" Plus\", \" a\"], [\"Hol\", \"iday\", \" P\", \"unch\", \" \\u2014\", \" Plus\", \" a\", \" Co\"], [\"Hol\", \"iday\", \" P\", \"unch\", \" \\u2014\", \" Plus\", \" a\", \" Co\", \"zy\"], [\"Hol\", \"iday\", \" P\", \"unch\", \" \\u2014\", \" Plus\", \" a\", \" Co\", \"zy\", \" Fire\"], [\"Hol\", \"iday\", \" P\", \"unch\", \" \\u2014\", \" Plus\", \" a\", \" Co\", \"zy\", \" Fire\", \"\\n\"], [\"Hol\", \"iday\", \" P\", \"unch\", \" \\u2014\", \" Plus\", \" a\", \" Co\", \"zy\", \" Fire\", \"\\n\", \"\\n\"], [\"Hol\", \"iday\", \" P\", \"unch\", \" \\u2014\", \" Plus\", \" a\", \" Co\", \"zy\", \" Fire\", \"\\n\", \"\\n\", \"Charles\"], [\"Hol\", \"iday\", \" P\", \"unch\", \" \\u2014\", \" Plus\", \" a\", \" Co\", \"zy\", \" Fire\", \"\\n\", \"\\n\", \"Charles\", \" Dick\"], [\"Hol\", \"iday\", \" P\", \"unch\", \" \\u2014\", \" Plus\", \" a\", \" Co\", \"zy\", \" Fire\", \"\\n\", \"\\n\", \"Charles\", \" Dick\", \"ens\"], [\"Hol\", \"iday\", \" P\", \"unch\", \" \\u2014\", \" Plus\", \" a\", \" Co\", \"zy\", \" Fire\", \"\\n\", \"\\n\", \"Charles\", \" Dick\", \"ens\", \" gave\"], [\"Hol\", \"iday\", \" P\", \"unch\", \" \\u2014\", \" Plus\", \" a\", \" Co\", \"zy\", \" Fire\", \"\\n\", \"\\n\", \"Charles\", \" Dick\", \"ens\", \" gave\", \" us\"], [\"Hol\", \"iday\", \" P\", \"unch\", \" \\u2014\", \" Plus\", \" a\", \" Co\", \"zy\", \" Fire\", \"\\n\", \"\\n\", \"Charles\", \" Dick\", \"ens\", \" gave\", \" us\", \" so\"], [\"Hol\", \"iday\", \" P\", \"unch\", \" \\u2014\", \" Plus\", \" a\", \" Co\", \"zy\", \" Fire\", \"\\n\", \"\\n\", \"Charles\", \" Dick\", \"ens\", \" gave\", \" us\", \" so\", \" much\"], [\"Hol\", \"iday\", \" P\", \"unch\", \" \\u2014\", \" Plus\", \" a\", \" Co\", \"zy\", \" Fire\", \"\\n\", \"\\n\", \"Charles\", \" Dick\", \"ens\", \" gave\", \" us\", \" so\", \" much\", \".\"], [\"Hol\", \"iday\", \" P\", \"unch\", \" \\u2014\", \" Plus\", \" a\", \" Co\", \"zy\", \" Fire\", \"\\n\", \"\\n\", \"Charles\", \" Dick\", \"ens\", \" gave\", \" us\", \" so\", \" much\", \".\", \" In\"], [\"Hol\", \"iday\", \" P\", \"unch\", \" \\u2014\", \" Plus\", \" a\", \" Co\", \"zy\", \" Fire\", \"\\n\", \"\\n\", \"Charles\", \" Dick\", \"ens\", \" gave\", \" us\", \" so\", \" much\", \".\", \" In\", \"cluding\"], [\"Hol\", \"iday\", \" P\", \"unch\", \" \\u2014\", \" Plus\", \" a\", \" Co\", \"zy\", \" Fire\", \"\\n\", \"\\n\", \"Charles\", \" Dick\", \"ens\", \" gave\", \" us\", \" so\", \" much\", \".\", \" In\", \"cluding\", \" this\"], [\"Hol\", \"iday\", \" P\", \"unch\", \" \\u2014\", \" Plus\", \" a\", \" Co\", \"zy\", \" Fire\", \"\\n\", \"\\n\", \"Charles\", \" Dick\", \"ens\", \" gave\", \" us\", \" so\", \" much\", \".\", \" In\", \"cluding\", \" this\", \".\"]], \"activations\": [[[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7144788503646851]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7144788503646851]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7144788503646851]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0361337810754776]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.030997931957244873]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.029590070247650146]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.013755664229393005]]], [[[0.0]]], [[[0.0]], [[0.0]]], [[[0.0]], [[0.0]], [[0.0]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]]], [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f6f841b1a80>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    del fp\n",
    "    gc.collect()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "feature = 1565\n",
    "\n",
    "fp = examine_dimension(\n",
    "    model,\n",
    "    submodule,\n",
    "    buffer,\n",
    "    ae,\n",
    "    dim_idx = feature,\n",
    "    n_inputs=256\n",
    ")\n",
    "\n",
    "print(fp.top_tokens)\n",
    "print(fp.top_affected)\n",
    "fp.top_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed\t\t293\n",
      "embed\t\t11649\n",
      "embed\t\t17003\n",
      "embed\t\t27441\n",
      "mlp\t0\t1901\n",
      "mlp\t0\t10355\n",
      "mlp\t0\t17834\n",
      "mlp\t0\t18502\n",
      "resid\t0\t2494\n",
      "resid\t0\t10630\n",
      "resid\t0\t23760\n",
      "resid\t1\t4678\n",
      "resid\t1\t15769\n",
      "resid\t1\t17151\n",
      "resid\t1\t18001\n",
      "resid\t1\t32616\n",
      "resid\t2\t10995\n",
      "resid\t2\t14779\n",
      "resid\t3\t8437\n",
      "resid\t3\t11981\n",
      "resid\t3\t20009\n",
      "mlp\t4\t4523\n",
      "mlp\t4\t15560\n",
      "resid\t4\t8913\n",
      "resid\t4\t11586\n",
      "resid\t4\t14719\n",
      "resid\t4\t16089\n",
      "attn\t5\t25516\n",
      "resid\t5\t295\n",
      "resid\t5\t9340\n",
      "resid\t5\t11839\n",
      "resid\t5\t18629\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "\n",
    "threshold = 0.2\n",
    "e_threshold = threshold / 10.\n",
    "\n",
    "c = t.load(f'circuits/within_rc_train_dict10_node{threshold}_edge{e_threshold}_n100_aggnone.pt')\n",
    "nodes = c['nodes']\n",
    "\n",
    "for component, x in nodes.items():\n",
    "    if component == 'embed': comp, layer = 'embed', \"\"\n",
    "    else:\n",
    "        comp, layer = component.split('_')\n",
    "\n",
    "    for idx in (x.act.abs() > threshold).nonzero():\n",
    "        print(f\"{comp}\\t{layer}\\t{idx[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sing do, re, mi,\n",
      "Original:\n",
      "     mi: 0.08595732599496841\n",
      "     m: 0.016683943569660187\n",
      "     no: 0.01310817152261734\n",
      "     o: 0.012504851445555687\n",
      "     s: 0.011511574499309063\n",
      "Ablating attn_3/14579:\n",
      "     mi: 0.085909903049469\n",
      "     m: 0.01670733653008938\n",
      "     no: 0.0130849564447999\n",
      "     o: 0.012484229169785976\n",
      "     s: 0.011529122479259968\n"
     ]
    }
   ],
   "source": [
    "def experiment(text, feature):\n",
    "    print(text)\n",
    "    component, feat_idx = feature.split('/')\n",
    "    component, layer = component.split('_')\n",
    "    layer, feat_idx = int(layer), int(feat_idx)\n",
    "    ae = AutoEncoder(512, 32768).to('cuda:0')\n",
    "    if component == 'resid':\n",
    "        submodule = model.gpt_neox.layers[layer]\n",
    "        ae.load_state_dict(t.load(f'/share/projects/dictionary_circuits/autoencoders/pythia-70m-deduped/resid_out_layer{layer}/10_32768/ae.pt'))\n",
    "    elif component == 'attn':\n",
    "        submodule = model.gpt_neox.layers[layer].attention\n",
    "        ae.load_state_dict(t.load(f'/share/projects/dictionary_circuits/autoencoders/pythia-70m-deduped/attn_out_layer{layer}/10_32768/ae.pt'))\n",
    "    elif component == 'mlp':\n",
    "        submodule = model.gpt_neox.layers[layer].mlp\n",
    "        ae.load_state_dict(t.load(f'/share/projects/dictionary_circuits/autoencoders/pythia-70m-deduped/mlp_out_layer{layer}/10_32768/ae.pt'))\n",
    "    elif component == 'embed':\n",
    "        submodule = model.gpt_neox.embed_in\n",
    "        ae.load_state_dict(t.load(f'/share/projects/dictionary_circuits/autoencoders/pythia-70m-deduped/embed/10_32768/ae.pt'))\n",
    "    \n",
    "    print('Original:')\n",
    "    with model.trace(text):\n",
    "        probs = t.softmax(model.output.logits[0, -1, :], dim=-1).save()\n",
    "    for prob, idx in zip(*t.topk(probs, 5)):\n",
    "        print(f'    {model.tokenizer.decode(idx)}: {prob.item()}')\n",
    "    \n",
    "    print(f'Ablating {feature}:')\n",
    "    with model.trace(text):\n",
    "        x = submodule.output[0]\n",
    "        x_hat, f = ae(x, output_features=True)\n",
    "        f[..., feat_idx] = 0\n",
    "        submodule.output[0][:] = ae.decode(f) + (x - x_hat)\n",
    "        probs = t.softmax(model.output.logits[0, -1, :], dim=-1).save()\n",
    "    for prob, idx in zip(*t.topk(probs, 5)):\n",
    "        print(f'    {model.tokenizer.decode(idx)}: {prob.item()}')\n",
    "\n",
    "experiment(\"\", 'attn_3/14579')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "('#0000FF', '#ffffff')\n",
      "8\n",
      "('#D3D3FF', '#000000')\n",
      "5\n",
      "('#EAEAFF', '#000000')\n",
      "4\n",
      "('#DCDCFF', '#000000')\n",
      "16\n",
      "('#A8A8FF', '#000000')\n",
      "5\n",
      "('#DBDBFF', '#000000')\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# sums = [[\"embed/2886\", \"embed/9394\", \"embed/9491\", \"embed/13979\",\n",
    "#          \"embed/24081\", \"embed/16130\", \"embed/23101\", \"mlp_0/12107\",\n",
    "#          \"mlp_0/12228\", \"mlp_0/4995\", \"mlp_0/28747\", \"mlp_0/28953\",\n",
    "#          \"mlp_0/12282\", \"mlp_0/16595\", \"mlp_0/23395\", \"resid_0/8729\",\n",
    "#          \"resid_0/11657\", \"resid_0/30086\", \"resid_0/15527\", \"resid_0/12983\",\n",
    "#          \"resid_0/4893\"],\n",
    "#         [\"resid_1/18376\", \"resid_1/7542\", \"resid_2/25123\", \"resid_1/20098\", \"resid_1/19708\",\n",
    "#          \"resid_2/26688\", \"resid_2/31615\", \"resid_2/21572\", \"resid_2/6749\"], [\"resid_1/ε\", \"attn_2/ε\", \"resid_2/ε\"],\n",
    "#         [\"attn_3/14579\"],\n",
    "#         [\"attn_3/26353\", \"resid_3/11408\", \"resid_3/5357\", \"resid_3/25644\", \n",
    "#          \"resid_3/27675\", \"resid_3/26633\", \"resid_4/26222\", \"resid_4/23831\",\n",
    "#          \"resid_4/19577\", \"resid_4/4340\", \"resid_4/8908\", \"resid_4/12046\",\n",
    "#          \"resid_5/899\", \"resid_5/26851\", \"resid_5/27843\", \"resid_5/21625\"], [\"resid_3/ε\", \"resid_5/ε\"]\n",
    "# ]\n",
    "\n",
    "# sums = [[\"embed/10859\", \"embed/13807\", \"embed/16937\", \"embed/31875\", \"embed/8233\",\n",
    "#          \"mlp_0/10665\", \"mlp_0/29209\", \"mlp_0/15008\", \"mlp_0/20713\", \"mlp_0/26732\"], [\"embed/ε\"],\n",
    "#         [\"resid_0/12292\", \"resid_0/30147\", \"resid_1/32358\", \"resid_2/22653\",\n",
    "#          \"resid_2/20421\", \"resid_3/10364\", \"resid_3/31728\", \"resid_3/11794\",\n",
    "#          \"resid_4/25089\", \"resid_4/6798\", \"resid_4/1620\", \"resid_4/10570\"], [\"resid_1/ε\", \"resid_2/ε\", \"resid_3/ε\", \"resid_4/ε\"],\n",
    "#         [\"resid_5/7552\", \"resid_5/308\"]\n",
    "#         ]\n",
    "# sums = [[\"embed/293\", \"embed/15970\", \"embed/22499\", \"embed/26494\", \"embed/29610\" \"embed/31565\",\n",
    "#          \"mlp_0/5536\", \"mlp_0/16150\", \"mlp_0/17834\", \"mlp_0/19216\", \"mlp_0/24139\", \"mlp_0/30990\",\n",
    "#          \"resid_0/1807\", \"resid_0/2494\", \"resid_0/10590\", \"resid_0/15075\", \"resid_0/21085\", \"resid_0/23953\",\n",
    "#          \"mlp_1/22854\", \"resid_1/7749\", \"resid_1/10524\", \"resid_1/12037\", \"resid_1/15769\", \"resid_1/18001\",\n",
    "#          \"resid_2/10995\", \"resid_2/14779\", \"resid_3/8437\", \"resid_3/11981\", \"mlp_4/15560\", \"resid_4/8913\",\n",
    "#          \"resid_4/11586\"], [\"\"]\n",
    "#         [], []\n",
    "#         [], []]\n",
    "sums = [['1, embed/293', '1, embed/1319', '1, embed/4285', '1, embed/10189', '1, embed/10941', '1, embed/15970', '1, embed/19485', '1, embed/22499', '1, embed/26494', '1, embed/28282', '1, embed/29610', '1, embed/31565', '1, mlp_0/5536', '1, mlp_0/16150', '1, mlp_0/16934', '1, mlp_0/17834', '1, mlp_0/19216', '1, mlp_0/23899', '1, mlp_0/24139', '1, mlp_0/28637', '1, mlp_0/30990', '1, resid_0/1807', '1, resid_0/2494', '1, resid_0/9765', '1, resid_0/10590', '1, resid_0/15075', '1, resid_0/21085', '1, resid_0/23953', '1, resid_0/24693', '1, resid_0/25899', '1, mlp_1/10410', '1, mlp_1/22854', '1, resid_1/7749', '1, resid_1/10524', '1, resid_1/12037', '1, resid_1/15769', '1, resid_1/18001', '1, resid_2/10995', '1, resid_2/14779', '1, resid_3/8437', '1, resid_3/11981', '1, resid_3/20009', '1, mlp_4/15560', '1, resid_4/8913', '1, resid_4/11586', '1, resid_4/16089', '1, resid_4/22614', '1, resid_4/26914'], ['1, embed/ε', '1, mlp_0/ε', '1, mlp_1/ε', '1, resid_1/ε', '1, attn_2/ε', '1, resid_2/ε', '1, resid_3/ε', '1, mlp_4/ε'], ['2, attn_2/32044', '2, resid_2/30677', '2, mlp_3/6782', '2, mlp_3/17671', '2, resid_3/18529'], ['2, attn_2/ε', '2, resid_2/ε', '2, mlp_3/ε', '2, resid_3/ε'], ['5, attn_4/3982', '5, attn_4/31148', '5, resid_4/14719', '5, attn_5/974', '5, attn_5/7447', '5, attn_5/9376', '5, attn_5/19313', '5, attn_5/25516', '5, resid_5/295', '5, resid_5/3036', '5, resid_5/4264', '5, resid_5/7468', '5, resid_5/8919', '5, resid_5/19066', '5, resid_5/28507', '5, resid_5/30107'], ['5, attn_4/ε', '5, resid_4/ε', '5, attn_5/ε', '5, mlp_5/ε', '5, resid_5/ε']]\n",
    "\n",
    "\n",
    "circuit_path = \"circuits/rc_train_dict10_node0.1_edge0.01_n100_aggnone.pt\"\n",
    "with open(circuit_path, \"rb\") as circuit_data:\n",
    "    circuit = t.load(circuit_data)\n",
    "\n",
    "sum_effects = defaultdict(int)\n",
    "\n",
    "nodes = circuit[\"nodes\"]\n",
    "min_effect = min([v.to_tensor().min() for n, v in nodes.items() if n != 'y'])\n",
    "max_effect = max([v.to_tensor().sum() for n, v in nodes.items() if n != 'y'])\n",
    "\n",
    "for idx, cluster in enumerate(sums):\n",
    "    for feature in cluster:\n",
    "        pos_submod, feat_idx = feature.split(\"/\")\n",
    "        pos, submod = pos_submod.split(\", \")\n",
    "        pos = int(pos)\n",
    "        if feat_idx == \"ε\":\n",
    "            effect = nodes[submod].resc[pos]\n",
    "        else:\n",
    "            effect = nodes[submod].act[pos, int(feat_idx)]\n",
    "        sum_effects[idx] += effect\n",
    "\n",
    "for cluster in sum_effects:\n",
    "    max_effect = max(sum_effects[cluster], max_effect)\n",
    "    min_effect = min(sum_effects[cluster], min_effect)\n",
    "\n",
    "scale = max(abs(min_effect), abs(max_effect))\n",
    "\n",
    "def to_hex(number):\n",
    "    number = number / scale\n",
    "    \n",
    "    # Define how the intensity changes based on the number\n",
    "    # - Negative numbers increase red component to max\n",
    "    # - Positive numbers increase blue component to max\n",
    "    # - 0 results in white\n",
    "    if number < 0:\n",
    "        # Increase towards red, full intensity at -1.0\n",
    "        red = 255\n",
    "        green = blue = int((1 + number) * 255)  # Increase other components less as it gets more negative\n",
    "    elif number > 0:\n",
    "        # Increase towards blue, full intensity at 1.0\n",
    "        blue = 255\n",
    "        red = green = int((1 - number) * 255)  # Increase other components less as it gets more positive\n",
    "    else:\n",
    "        # Exact 0, resulting in white\n",
    "        red = green = blue = 255 \n",
    "    \n",
    "    # decide whether text is black or white depending on darkness of color\n",
    "    text_hex = \"#000000\" if (red*0.299 + green*0.587 + blue*0.114) > 170 else \"#ffffff\"\n",
    "\n",
    "    # Convert to hex, ensuring each component is 2 digits\n",
    "    hex_code = f'#{red:02X}{green:02X}{blue:02X}'\n",
    "    \n",
    "    return hex_code, text_hex\n",
    "\n",
    "for cluster in sum_effects:\n",
    "    hex = to_hex(sum_effects[cluster])\n",
    "    print(len(sums[cluster]))\n",
    "    print(hex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "48+8+5+4+16+5 - 69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
